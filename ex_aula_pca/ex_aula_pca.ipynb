{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26fxjoVADscQ"
      },
      "source": [
        "Para abrir o notebook no Google Colab, altere o domínio `github.com` para `githubtocolab.com`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sf-Xuxsf3K_T"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\">\n",
        "Para praticar programação, é importante que você erre, leia as mensagens de erro e tente corrigí-los.\n",
        "    \n",
        "Dessa forma, no Google Colab, é importante que você DESATIVE OS RECURSOS DE AUTOCOMPLETAR:\n",
        "\n",
        "- Menu Ferramentas -> Configurações\n",
        "- Na janela que é aberta:\n",
        "  - Seção Editor -> Desativar \"Mostrar sugestões de preenchimento de código com base no contexto\"\n",
        "  - Seção Assistência de IA -> Desabilitar itens\n",
        "\n",
        "Na versão em inglês:\n",
        "\n",
        "- Menu Tools -> Settings\n",
        "- Na janela que é aberta:\n",
        "  - Seção Editor -> Desativar \"Show context-powered code completions\"\n",
        "  - Seção AI Assistance -> Desabilitar itens\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c04thLl8Dzh4"
      },
      "source": [
        "# PSI5892 - Aula de Exercícios\n",
        "\n",
        "# Análise de componentes principais (PCA) e análise de discriminantes lineares (LDA)\n",
        "\n",
        "Neste exercício, você vai trabalhar com uma aplicação de redes neurais para  para a área de saúde. O objetivo é obter um modelo de predição de uma doença cardíaca baseado em dados com características extraídas de exames clínicos laboratoriais.\n",
        "\n",
        "## Dados disponibilizados\n",
        "\n",
        "Os dados para treinamento e teste do modelo estão disponíveis no formato CSV, em um arquivo zip disponível [neste link](./data.zip).\n",
        "\n",
        "Após extrair os arquivos, utiliza a biblioteca Pandas para carregar os `DataFrames` `data_train` e `data_test`, como mostrado a seguir:\n",
        "\n",
        "``` python\n",
        "import pandas as pd\n",
        "\n",
        "data_train = pd.read_csv(\"data_train.csv\").drop(columns=[\"Unnamed: 0\"])\n",
        "\n",
        "data_test = pd.read_csv(\"data_test.csv\").drop(columns=[\"Unnamed: 0\"])\n",
        "```\n",
        "\n",
        "Os dados consistem de 800 exemplos de treinamento e 225 para teste, cada um contendo 13 características de entrada, representadas pelas colunas de 0 a 12 e a saída desejada binária, indicando se o paciente é portador ou não da doença, representada pela coluna 13.\n",
        "\n",
        "O objetivo é treinar uma rede neural com estes dados, avaliar o desempenho e depois comparar com o desempenho obtido usando o PCA para realizar redução de dimensionalidade. Use como referência o exemplo mostrado [neste Jupyter Notebook](./PCA_IRIS.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thFmnLOj3K_Y"
      },
      "source": [
        "# Exercício 1\n",
        "\n",
        "Implemente uma rede neural para classificar se o indivíduo é portador ou não da doença cardíaca (coluna 13) usando como entrada os dados dos exames laboratoriais (colunas 1 a 12). Calcule a acurácia obtida nos dados de teste.\n",
        "\n",
        "## Resolução"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "x8pe0-5-3K_b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregamento dos dados\n",
        "\n",
        "data_train = pd.read_csv(\"data_train.csv\").drop(columns=[\"Unnamed: 0\"])\n",
        "data_test = pd.read_csv(\"data_test.csv\").drop(columns=[\"Unnamed: 0\"])"
      ],
      "metadata": {
        "id": "qehWt4DP3oS9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "659kqytE51Za",
        "outputId": "e6325178-f510-4abc-f682-8137e650e799"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        0    1    2      3      4    5    6      7    8    9   10   11   12  \\\n",
              "0    52.0  1.0  0.0  125.0  212.0  0.0  1.0  168.0  0.0  1.0  2.0  2.0  3.0   \n",
              "1    53.0  1.0  0.0  140.0  203.0  1.0  0.0  155.0  1.0  3.1  0.0  0.0  3.0   \n",
              "2    70.0  1.0  0.0  145.0  174.0  0.0  1.0  125.0  1.0  2.6  0.0  0.0  3.0   \n",
              "3    61.0  1.0  0.0  148.0  203.0  0.0  1.0  161.0  0.0  0.0  2.0  1.0  3.0   \n",
              "4    62.0  0.0  0.0  138.0  294.0  1.0  1.0  106.0  0.0  1.9  1.0  3.0  2.0   \n",
              "..    ...  ...  ...    ...    ...  ...  ...    ...  ...  ...  ...  ...  ...   \n",
              "795  62.0  1.0  1.0  128.0  208.0  1.0  0.0  140.0  0.0  0.0  2.0  0.0  2.0   \n",
              "796  41.0  1.0  1.0  135.0  203.0  0.0  1.0  132.0  0.0  0.0  1.0  0.0  1.0   \n",
              "797  65.0  0.0  0.0  150.0  225.0  0.0  0.0  114.0  0.0  1.0  1.0  3.0  3.0   \n",
              "798  59.0  1.0  3.0  170.0  288.0  0.0  0.0  159.0  0.0  0.2  1.0  0.0  3.0   \n",
              "799  43.0  1.0  0.0  115.0  303.0  0.0  1.0  181.0  0.0  1.2  1.0  0.0  2.0   \n",
              "\n",
              "      13  \n",
              "0    0.0  \n",
              "1    0.0  \n",
              "2    0.0  \n",
              "3    0.0  \n",
              "4    0.0  \n",
              "..   ...  \n",
              "795  1.0  \n",
              "796  1.0  \n",
              "797  0.0  \n",
              "798  0.0  \n",
              "799  1.0  \n",
              "\n",
              "[800 rows x 14 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-66185ab9-42d4-4052-9e06-d7db472537e1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>52.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>125.0</td>\n",
              "      <td>212.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>168.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>53.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>140.0</td>\n",
              "      <td>203.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>155.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>70.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>145.0</td>\n",
              "      <td>174.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>125.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>61.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>148.0</td>\n",
              "      <td>203.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>161.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>62.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>138.0</td>\n",
              "      <td>294.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>106.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.9</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>795</th>\n",
              "      <td>62.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>128.0</td>\n",
              "      <td>208.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>140.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>796</th>\n",
              "      <td>41.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>135.0</td>\n",
              "      <td>203.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>797</th>\n",
              "      <td>65.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>225.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>798</th>\n",
              "      <td>59.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>288.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>159.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>799</th>\n",
              "      <td>43.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>115.0</td>\n",
              "      <td>303.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>181.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>800 rows × 14 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-66185ab9-42d4-4052-9e06-d7db472537e1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-66185ab9-42d4-4052-9e06-d7db472537e1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-66185ab9-42d4-4052-9e06-d7db472537e1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-83333c1f-6cd7-411e-9465-3c30e13fafc6\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-83333c1f-6cd7-411e-9465-3c30e13fafc6')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-83333c1f-6cd7-411e-9465-3c30e13fafc6 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_bf616459-ff4b-480c-b8d4-99a48461cdba\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('data_train')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_bf616459-ff4b-480c-b8d4-99a48461cdba button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('data_train');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data_train",
              "summary": "{\n  \"name\": \"data_train\",\n  \"rows\": 800,\n  \"fields\": [\n    {\n      \"column\": \"0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9.232059014220622,\n        \"min\": 29.0,\n        \"max\": 77.0,\n        \"num_unique_values\": 41,\n        \"samples\": [\n          65.0,\n          50.0,\n          54.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4657949722734747,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0156588458305733,\n        \"min\": 0.0,\n        \"max\": 3.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1.0,\n          3.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 17.857721656988417,\n        \"min\": 94.0,\n        \"max\": 200.0,\n        \"num_unique_values\": 49,\n        \"samples\": [\n          128.0,\n          172.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"4\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 53.06672764469413,\n        \"min\": 126.0,\n        \"max\": 564.0,\n        \"num_unique_values\": 152,\n        \"samples\": [\n          267.0,\n          262.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"5\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.36331933429607305,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"6\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5245833915292517,\n        \"min\": 0.0,\n        \"max\": 2.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"7\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 23.076981688574847,\n        \"min\": 71.0,\n        \"max\": 202.0,\n        \"num_unique_values\": 90,\n        \"samples\": [\n          180.0,\n          152.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"8\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4718466239634246,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"9\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.1848183376013328,\n        \"min\": 0.0,\n        \"max\": 6.2,\n        \"num_unique_values\": 40,\n        \"samples\": [\n          2.8,\n          0.4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"10\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6151202859667018,\n        \"min\": 0.0,\n        \"max\": 2.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"11\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0495953985349717,\n        \"min\": 0.0,\n        \"max\": 4.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.0,\n          4.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"12\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6212295531420899,\n        \"min\": 0.0,\n        \"max\": 3.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"13\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.49991238281133826,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_total = len(data_train)\n",
        "n_treino = int(n_total * 0.8) # Divisão 80/20 (treino/validação)\n",
        "\n",
        "indices_aleatorios = np.random.permutation(n_total)\n",
        "\n",
        "indices_treino = indices_aleatorios[:n_treino]\n",
        "indices_val = indices_aleatorios[n_treino:]\n",
        "\n",
        "# DataFrames de treino e validação\n",
        "df_treino = data_train.iloc[indices_treino]\n",
        "df_val = data_train.iloc[indices_val]\n",
        "\n",
        "print(len(df_treino))\n",
        "print(len(df_val))\n",
        "\n",
        "# Dados de treino\n",
        "x_treino_np = df_treino.iloc[:, :-1].to_numpy()\n",
        "d_treino_np = df_treino.iloc[:, -1].to_numpy()\n",
        "\n",
        "# Dados de validação\n",
        "x_val_np = df_val.iloc[:, :-1].to_numpy()\n",
        "d_val_np = df_val.iloc[:, -1].to_numpy()\n",
        "\n",
        "# ============================================================== #\n",
        "# Normalizar os dados de treino e val\n",
        "\n",
        "# Calcula a média das características\n",
        "media_treino = np.mean(x_treino_np, axis=0)\n",
        "\n",
        "# Calcula o desvio padrao das características\n",
        "desvio_padrao_treino = np.std(x_treino_np, axis=0, ddof=1)\n",
        "\n",
        "# Aplica a normalização no conjunto\n",
        "x_treino_np_norm = (x_treino_np - media_treino) / desvio_padrao_treino\n",
        "x_val_np_norm = (x_val_np - media_treino) / desvio_padrao_treino\n",
        "\n",
        "# ============================================================== #\n",
        "# Converter para tensores PyTorch\n",
        "x_treino_tensor = torch.tensor(x_treino_np_norm, dtype=torch.float32)\n",
        "d_treino_tensor = torch.tensor(d_treino_np, dtype=torch.long)\n",
        "\n",
        "x_val_tensor = torch.tensor(x_val_np_norm, dtype=torch.float32)\n",
        "d_val_tensor = torch.tensor(d_val_np, dtype=torch.long)\n",
        "\n",
        "# print(f\"x_treino: {x_treino_np.shape}\")\n",
        "# print(f\"d_treino: {d_treino_np.shape}\")\n",
        "# print(f\"x_val: {x_val_np.shape}\")\n",
        "# print(f\"d_val: {d_val_np.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkTDgfH7FVDQ",
        "outputId": "fd8d0ad0-e2e8-4453-a69c-d7d033189dd6"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "640\n",
            "160\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.model = nn.Sequential(\n",
        "        nn.Linear(13, 8),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.Linear(8, 4),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.Linear(4, 4),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.Linear(4, 2),\n",
        "    )\n",
        "\n",
        "    self._init_weights()\n",
        "\n",
        "  def _init_weights(self):\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Linear):\n",
        "        nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
        "\n",
        "        # Inicializa os bias com zero\n",
        "        if m.bias is not None:\n",
        "          nn.init.constant_(m.bias, 0)\n",
        "\n",
        "  def forward(self, x):\n",
        "    output = self.model(x)\n",
        "    return output"
      ],
      "metadata": {
        "id": "IAjXWGec9nSf"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Model().to(device=device)\n",
        "\n",
        "# Taxa de aprendizado\n",
        "eta = 0.001\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=eta)\n",
        "\n",
        "Nb = 64 # Tamanho do mini-batch\n",
        "Ne = 1000 # Número de épocas\n"
      ],
      "metadata": {
        "id": "0itvtGvd-HCM"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_set = TensorDataset(x_treino_tensor, d_treino_tensor)\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=Nb, shuffle=True)"
      ],
      "metadata": {
        "id": "Uhve4y04HqVW"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Treinamento\n",
        "losses = []\n",
        "val_losses = []\n",
        "\n",
        "x_val_tensor = x_val_tensor.to(device=device)\n",
        "d_val_tensor = d_val_tensor.to(device=device)\n",
        "\n",
        "for epoch in range(Ne):\n",
        "  for n, (X, d) in enumerate(train_loader):\n",
        "\n",
        "    X = X.to(device=device)\n",
        "    d = d.to(device=device)\n",
        "\n",
        "    # Treinamento\n",
        "    model.train()\n",
        "    model.zero_grad()\n",
        "    y = model(X)\n",
        "    loss = loss_function(y, d)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Validação\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      y_val = model(x_val_tensor)\n",
        "      val_loss = loss_function(y_val, d_val_tensor)\n",
        "\n",
        "    losses.append(loss.item())\n",
        "    val_losses.append(val_loss.item())\n",
        "\n",
        "    if epoch % 1 == 0 and n == x_treino_tensor.shape[0]//Nb - 1:\n",
        "      print(f\"Epoch: {epoch} | Loss: {loss} | Val. Loss: {val_loss}\")\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(losses)\n",
        "plt.plot(val_losses, alpha=0.8)\n",
        "plt.legend([\"Loss\", \"Val. Loss\"])\n",
        "plt.xlabel(\"Batch\")\n",
        "plt.ylabel(\"Loss\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CmkDGipS-qmR",
        "outputId": "b97689a3-f6ab-4c75-90cd-21dc94af066f"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | Loss: 0.6980275511741638 | Val. Loss: 0.715799868106842\n",
            "Epoch: 1 | Loss: 0.683483362197876 | Val. Loss: 0.7048778533935547\n",
            "Epoch: 2 | Loss: 0.6889467835426331 | Val. Loss: 0.696644127368927\n",
            "Epoch: 3 | Loss: 0.6863687634468079 | Val. Loss: 0.6895526647567749\n",
            "Epoch: 4 | Loss: 0.6898251175880432 | Val. Loss: 0.6833603382110596\n",
            "Epoch: 5 | Loss: 0.6869146823883057 | Val. Loss: 0.6757832765579224\n",
            "Epoch: 6 | Loss: 0.6711859703063965 | Val. Loss: 0.6688741445541382\n",
            "Epoch: 7 | Loss: 0.6631564497947693 | Val. Loss: 0.6598584055900574\n",
            "Epoch: 8 | Loss: 0.6534742116928101 | Val. Loss: 0.6495095491409302\n",
            "Epoch: 9 | Loss: 0.6341423392295837 | Val. Loss: 0.635840117931366\n",
            "Epoch: 10 | Loss: 0.6335485577583313 | Val. Loss: 0.620768666267395\n",
            "Epoch: 11 | Loss: 0.6037120819091797 | Val. Loss: 0.6051415205001831\n",
            "Epoch: 12 | Loss: 0.6003416180610657 | Val. Loss: 0.5886055827140808\n",
            "Epoch: 13 | Loss: 0.5939103960990906 | Val. Loss: 0.5728826522827148\n",
            "Epoch: 14 | Loss: 0.5811135768890381 | Val. Loss: 0.5587514638900757\n",
            "Epoch: 15 | Loss: 0.6083745360374451 | Val. Loss: 0.5463990569114685\n",
            "Epoch: 16 | Loss: 0.5118270516395569 | Val. Loss: 0.5345478057861328\n",
            "Epoch: 17 | Loss: 0.5202404260635376 | Val. Loss: 0.5236543416976929\n",
            "Epoch: 18 | Loss: 0.5842644572257996 | Val. Loss: 0.5126176476478577\n",
            "Epoch: 19 | Loss: 0.6105461120605469 | Val. Loss: 0.5030066967010498\n",
            "Epoch: 20 | Loss: 0.48800572752952576 | Val. Loss: 0.49397557973861694\n",
            "Epoch: 21 | Loss: 0.47202223539352417 | Val. Loss: 0.4851434826850891\n",
            "Epoch: 22 | Loss: 0.48952582478523254 | Val. Loss: 0.47719287872314453\n",
            "Epoch: 23 | Loss: 0.5365796089172363 | Val. Loss: 0.46910467743873596\n",
            "Epoch: 24 | Loss: 0.45699337124824524 | Val. Loss: 0.4625632166862488\n",
            "Epoch: 25 | Loss: 0.42131277918815613 | Val. Loss: 0.45537906885147095\n",
            "Epoch: 26 | Loss: 0.4741881191730499 | Val. Loss: 0.4487382769584656\n",
            "Epoch: 27 | Loss: 0.42444631457328796 | Val. Loss: 0.4427681565284729\n",
            "Epoch: 28 | Loss: 0.4543531835079193 | Val. Loss: 0.4368264079093933\n",
            "Epoch: 29 | Loss: 0.49566197395324707 | Val. Loss: 0.43274766206741333\n",
            "Epoch: 30 | Loss: 0.4998026490211487 | Val. Loss: 0.4268203377723694\n",
            "Epoch: 31 | Loss: 0.4417978823184967 | Val. Loss: 0.4229021966457367\n",
            "Epoch: 32 | Loss: 0.4044942855834961 | Val. Loss: 0.41803282499313354\n",
            "Epoch: 33 | Loss: 0.4105764329433441 | Val. Loss: 0.41297292709350586\n",
            "Epoch: 34 | Loss: 0.367907851934433 | Val. Loss: 0.4088359475135803\n",
            "Epoch: 35 | Loss: 0.41395580768585205 | Val. Loss: 0.4044245183467865\n",
            "Epoch: 36 | Loss: 0.391418993473053 | Val. Loss: 0.4004599452018738\n",
            "Epoch: 37 | Loss: 0.5521833300590515 | Val. Loss: 0.39726632833480835\n",
            "Epoch: 38 | Loss: 0.38203704357147217 | Val. Loss: 0.39406055212020874\n",
            "Epoch: 39 | Loss: 0.5074451565742493 | Val. Loss: 0.3912857174873352\n",
            "Epoch: 40 | Loss: 0.4029347896575928 | Val. Loss: 0.38837021589279175\n",
            "Epoch: 41 | Loss: 0.44536134600639343 | Val. Loss: 0.38705602288246155\n",
            "Epoch: 42 | Loss: 0.40571334958076477 | Val. Loss: 0.3867148756980896\n",
            "Epoch: 43 | Loss: 0.512184202671051 | Val. Loss: 0.3846486806869507\n",
            "Epoch: 44 | Loss: 0.36540254950523376 | Val. Loss: 0.38276225328445435\n",
            "Epoch: 45 | Loss: 0.3798275589942932 | Val. Loss: 0.38134557008743286\n",
            "Epoch: 46 | Loss: 0.3380463421344757 | Val. Loss: 0.37938326597213745\n",
            "Epoch: 47 | Loss: 0.3804076611995697 | Val. Loss: 0.376650869846344\n",
            "Epoch: 48 | Loss: 0.33010244369506836 | Val. Loss: 0.3746872544288635\n",
            "Epoch: 49 | Loss: 0.40890857577323914 | Val. Loss: 0.37185949087142944\n",
            "Epoch: 50 | Loss: 0.3181043267250061 | Val. Loss: 0.3701494038105011\n",
            "Epoch: 51 | Loss: 0.3745964765548706 | Val. Loss: 0.36930179595947266\n",
            "Epoch: 52 | Loss: 0.4517518877983093 | Val. Loss: 0.36715757846832275\n",
            "Epoch: 53 | Loss: 0.3235488831996918 | Val. Loss: 0.3652971684932709\n",
            "Epoch: 54 | Loss: 0.41375336050987244 | Val. Loss: 0.3633244037628174\n",
            "Epoch: 55 | Loss: 0.28972044587135315 | Val. Loss: 0.3615773916244507\n",
            "Epoch: 56 | Loss: 0.43393072485923767 | Val. Loss: 0.35914260149002075\n",
            "Epoch: 57 | Loss: 0.35591384768486023 | Val. Loss: 0.35689443349838257\n",
            "Epoch: 58 | Loss: 0.3138847053050995 | Val. Loss: 0.3556519150733948\n",
            "Epoch: 59 | Loss: 0.4347407817840576 | Val. Loss: 0.3527015745639801\n",
            "Epoch: 60 | Loss: 0.32774078845977783 | Val. Loss: 0.3501644730567932\n",
            "Epoch: 61 | Loss: 0.3276907503604889 | Val. Loss: 0.3476807475090027\n",
            "Epoch: 62 | Loss: 0.3257956802845001 | Val. Loss: 0.3446773886680603\n",
            "Epoch: 63 | Loss: 0.34343209862709045 | Val. Loss: 0.34156301617622375\n",
            "Epoch: 64 | Loss: 0.34923607110977173 | Val. Loss: 0.3401101231575012\n",
            "Epoch: 65 | Loss: 0.24367907643318176 | Val. Loss: 0.3385750353336334\n",
            "Epoch: 66 | Loss: 0.39552128314971924 | Val. Loss: 0.33652180433273315\n",
            "Epoch: 67 | Loss: 0.37937334179878235 | Val. Loss: 0.3348761200904846\n",
            "Epoch: 68 | Loss: 0.397009938955307 | Val. Loss: 0.3339514136314392\n",
            "Epoch: 69 | Loss: 0.35336601734161377 | Val. Loss: 0.33264389634132385\n",
            "Epoch: 70 | Loss: 0.275665819644928 | Val. Loss: 0.33052176237106323\n",
            "Epoch: 71 | Loss: 0.27042847871780396 | Val. Loss: 0.3308051824569702\n",
            "Epoch: 72 | Loss: 0.3407515287399292 | Val. Loss: 0.3283049166202545\n",
            "Epoch: 73 | Loss: 0.30240702629089355 | Val. Loss: 0.3278025984764099\n",
            "Epoch: 74 | Loss: 0.3182183802127838 | Val. Loss: 0.32547467947006226\n",
            "Epoch: 75 | Loss: 0.4031919538974762 | Val. Loss: 0.3246166408061981\n",
            "Epoch: 76 | Loss: 0.3011956214904785 | Val. Loss: 0.3232036232948303\n",
            "Epoch: 77 | Loss: 0.3381064236164093 | Val. Loss: 0.32170066237449646\n",
            "Epoch: 78 | Loss: 0.406253844499588 | Val. Loss: 0.3209232985973358\n",
            "Epoch: 79 | Loss: 0.32676205039024353 | Val. Loss: 0.319935142993927\n",
            "Epoch: 80 | Loss: 0.31774449348449707 | Val. Loss: 0.31850510835647583\n",
            "Epoch: 81 | Loss: 0.31437158584594727 | Val. Loss: 0.317277729511261\n",
            "Epoch: 82 | Loss: 0.31814414262771606 | Val. Loss: 0.31653156876564026\n",
            "Epoch: 83 | Loss: 0.2847592830657959 | Val. Loss: 0.3161509931087494\n",
            "Epoch: 84 | Loss: 0.3458637297153473 | Val. Loss: 0.31355834007263184\n",
            "Epoch: 85 | Loss: 0.3236615061759949 | Val. Loss: 0.31226831674575806\n",
            "Epoch: 86 | Loss: 0.3056607246398926 | Val. Loss: 0.31215816736221313\n",
            "Epoch: 87 | Loss: 0.3070840835571289 | Val. Loss: 0.31203094124794006\n",
            "Epoch: 88 | Loss: 0.30339962244033813 | Val. Loss: 0.31075939536094666\n",
            "Epoch: 89 | Loss: 0.32200857996940613 | Val. Loss: 0.30936169624328613\n",
            "Epoch: 90 | Loss: 0.28734415769577026 | Val. Loss: 0.3088780343532562\n",
            "Epoch: 91 | Loss: 0.2878847122192383 | Val. Loss: 0.30822548270225525\n",
            "Epoch: 92 | Loss: 0.26406747102737427 | Val. Loss: 0.30690258741378784\n",
            "Epoch: 93 | Loss: 0.308034747838974 | Val. Loss: 0.3061895966529846\n",
            "Epoch: 94 | Loss: 0.32541516423225403 | Val. Loss: 0.3055756986141205\n",
            "Epoch: 95 | Loss: 0.27135905623435974 | Val. Loss: 0.30402499437332153\n",
            "Epoch: 96 | Loss: 0.2642197608947754 | Val. Loss: 0.30422529578208923\n",
            "Epoch: 97 | Loss: 0.2716021239757538 | Val. Loss: 0.3029713034629822\n",
            "Epoch: 98 | Loss: 0.29726988077163696 | Val. Loss: 0.30160582065582275\n",
            "Epoch: 99 | Loss: 0.23253296315670013 | Val. Loss: 0.30188238620758057\n",
            "Epoch: 100 | Loss: 0.4297099709510803 | Val. Loss: 0.2993048131465912\n",
            "Epoch: 101 | Loss: 0.2602430284023285 | Val. Loss: 0.299294650554657\n",
            "Epoch: 102 | Loss: 0.20996452867984772 | Val. Loss: 0.2995086908340454\n",
            "Epoch: 103 | Loss: 0.3723874092102051 | Val. Loss: 0.2976263165473938\n",
            "Epoch: 104 | Loss: 0.24879229068756104 | Val. Loss: 0.2966373562812805\n",
            "Epoch: 105 | Loss: 0.3244883418083191 | Val. Loss: 0.2968316078186035\n",
            "Epoch: 106 | Loss: 0.2689218521118164 | Val. Loss: 0.295825332403183\n",
            "Epoch: 107 | Loss: 0.2595757842063904 | Val. Loss: 0.29356035590171814\n",
            "Epoch: 108 | Loss: 0.22694727778434753 | Val. Loss: 0.2932044565677643\n",
            "Epoch: 109 | Loss: 0.23726440966129303 | Val. Loss: 0.29310840368270874\n",
            "Epoch: 110 | Loss: 0.3030663728713989 | Val. Loss: 0.2917789816856384\n",
            "Epoch: 111 | Loss: 0.2174040824174881 | Val. Loss: 0.2902117967605591\n",
            "Epoch: 112 | Loss: 0.22435519099235535 | Val. Loss: 0.2903919816017151\n",
            "Epoch: 113 | Loss: 0.2660306394100189 | Val. Loss: 0.28873178362846375\n",
            "Epoch: 114 | Loss: 0.31129780411720276 | Val. Loss: 0.2871341407299042\n",
            "Epoch: 115 | Loss: 0.370518296957016 | Val. Loss: 0.28591644763946533\n",
            "Epoch: 116 | Loss: 0.23385125398635864 | Val. Loss: 0.28692835569381714\n",
            "Epoch: 117 | Loss: 0.2719931900501251 | Val. Loss: 0.2847926616668701\n",
            "Epoch: 118 | Loss: 0.2627871632575989 | Val. Loss: 0.2841857671737671\n",
            "Epoch: 119 | Loss: 0.22693344950675964 | Val. Loss: 0.2834926247596741\n",
            "Epoch: 120 | Loss: 0.39858099818229675 | Val. Loss: 0.281419575214386\n",
            "Epoch: 121 | Loss: 0.17128607630729675 | Val. Loss: 0.2813166379928589\n",
            "Epoch: 122 | Loss: 0.3053087890148163 | Val. Loss: 0.2803313434123993\n",
            "Epoch: 123 | Loss: 0.18337056040763855 | Val. Loss: 0.28026875853538513\n",
            "Epoch: 124 | Loss: 0.21343086659908295 | Val. Loss: 0.27898094058036804\n",
            "Epoch: 125 | Loss: 0.29695191979408264 | Val. Loss: 0.2771850526332855\n",
            "Epoch: 126 | Loss: 0.2296183705329895 | Val. Loss: 0.2765433192253113\n",
            "Epoch: 127 | Loss: 0.2270321100950241 | Val. Loss: 0.27542001008987427\n",
            "Epoch: 128 | Loss: 0.314881294965744 | Val. Loss: 0.27364668250083923\n",
            "Epoch: 129 | Loss: 0.29468104243278503 | Val. Loss: 0.2738681435585022\n",
            "Epoch: 130 | Loss: 0.3308989107608795 | Val. Loss: 0.272359162569046\n",
            "Epoch: 131 | Loss: 0.2171165943145752 | Val. Loss: 0.2719818949699402\n",
            "Epoch: 132 | Loss: 0.30925625562667847 | Val. Loss: 0.2719798684120178\n",
            "Epoch: 133 | Loss: 0.36455628275871277 | Val. Loss: 0.2699015736579895\n",
            "Epoch: 134 | Loss: 0.2790234088897705 | Val. Loss: 0.26818665862083435\n",
            "Epoch: 135 | Loss: 0.28399795293807983 | Val. Loss: 0.268111914396286\n",
            "Epoch: 136 | Loss: 0.21532811224460602 | Val. Loss: 0.26746034622192383\n",
            "Epoch: 137 | Loss: 0.2050975263118744 | Val. Loss: 0.2665335536003113\n",
            "Epoch: 138 | Loss: 0.27529293298721313 | Val. Loss: 0.26632413268089294\n",
            "Epoch: 139 | Loss: 0.23182731866836548 | Val. Loss: 0.2656460106372833\n",
            "Epoch: 140 | Loss: 0.18018685281276703 | Val. Loss: 0.2657286524772644\n",
            "Epoch: 141 | Loss: 0.1471235603094101 | Val. Loss: 0.2643236219882965\n",
            "Epoch: 142 | Loss: 0.2323346734046936 | Val. Loss: 0.2626523971557617\n",
            "Epoch: 143 | Loss: 0.2136373221874237 | Val. Loss: 0.2615762948989868\n",
            "Epoch: 144 | Loss: 0.2728561758995056 | Val. Loss: 0.26141124963760376\n",
            "Epoch: 145 | Loss: 0.18535642325878143 | Val. Loss: 0.2572907507419586\n",
            "Epoch: 146 | Loss: 0.17856267094612122 | Val. Loss: 0.25524064898490906\n",
            "Epoch: 147 | Loss: 0.21856366097927094 | Val. Loss: 0.25501105189323425\n",
            "Epoch: 148 | Loss: 0.15976151823997498 | Val. Loss: 0.2520008385181427\n",
            "Epoch: 149 | Loss: 0.20600521564483643 | Val. Loss: 0.25344234704971313\n",
            "Epoch: 150 | Loss: 0.25248727202415466 | Val. Loss: 0.25083857774734497\n",
            "Epoch: 151 | Loss: 0.2033325880765915 | Val. Loss: 0.2497026026248932\n",
            "Epoch: 152 | Loss: 0.17390532791614532 | Val. Loss: 0.2481895238161087\n",
            "Epoch: 153 | Loss: 0.20932380855083466 | Val. Loss: 0.24732859432697296\n",
            "Epoch: 154 | Loss: 0.14653608202934265 | Val. Loss: 0.24658659100532532\n",
            "Epoch: 155 | Loss: 0.20474731922149658 | Val. Loss: 0.2454882562160492\n",
            "Epoch: 156 | Loss: 0.17034031450748444 | Val. Loss: 0.24274389445781708\n",
            "Epoch: 157 | Loss: 0.15626664459705353 | Val. Loss: 0.24173462390899658\n",
            "Epoch: 158 | Loss: 0.24658642709255219 | Val. Loss: 0.24223116040229797\n",
            "Epoch: 159 | Loss: 0.25676214694976807 | Val. Loss: 0.23990662395954132\n",
            "Epoch: 160 | Loss: 0.1914806067943573 | Val. Loss: 0.23949530720710754\n",
            "Epoch: 161 | Loss: 0.19827309250831604 | Val. Loss: 0.23738662898540497\n",
            "Epoch: 162 | Loss: 0.15168613195419312 | Val. Loss: 0.23621709644794464\n",
            "Epoch: 163 | Loss: 0.1206061840057373 | Val. Loss: 0.23567600548267365\n",
            "Epoch: 164 | Loss: 0.32958510518074036 | Val. Loss: 0.2338068038225174\n",
            "Epoch: 165 | Loss: 0.20779184997081757 | Val. Loss: 0.23241570591926575\n",
            "Epoch: 166 | Loss: 0.21227562427520752 | Val. Loss: 0.2343093454837799\n",
            "Epoch: 167 | Loss: 0.2345457673072815 | Val. Loss: 0.2305745631456375\n",
            "Epoch: 168 | Loss: 0.21365487575531006 | Val. Loss: 0.22953447699546814\n",
            "Epoch: 169 | Loss: 0.13452300429344177 | Val. Loss: 0.22988101840019226\n",
            "Epoch: 170 | Loss: 0.1887470781803131 | Val. Loss: 0.2283288985490799\n",
            "Epoch: 171 | Loss: 0.1933574229478836 | Val. Loss: 0.2275986224412918\n",
            "Epoch: 172 | Loss: 0.19795413315296173 | Val. Loss: 0.2256028950214386\n",
            "Epoch: 173 | Loss: 0.1879325807094574 | Val. Loss: 0.22454893589019775\n",
            "Epoch: 174 | Loss: 0.19268348813056946 | Val. Loss: 0.22435691952705383\n",
            "Epoch: 175 | Loss: 0.2798115611076355 | Val. Loss: 0.2230038344860077\n",
            "Epoch: 176 | Loss: 0.17623917758464813 | Val. Loss: 0.2232259064912796\n",
            "Epoch: 177 | Loss: 0.14401552081108093 | Val. Loss: 0.221361443400383\n",
            "Epoch: 178 | Loss: 0.19031557440757751 | Val. Loss: 0.22108379006385803\n",
            "Epoch: 179 | Loss: 0.14918944239616394 | Val. Loss: 0.21810272336006165\n",
            "Epoch: 180 | Loss: 0.12048067152500153 | Val. Loss: 0.21723952889442444\n",
            "Epoch: 181 | Loss: 0.216061532497406 | Val. Loss: 0.21780641376972198\n",
            "Epoch: 182 | Loss: 0.14178186655044556 | Val. Loss: 0.2137862890958786\n",
            "Epoch: 183 | Loss: 0.21823742985725403 | Val. Loss: 0.2132430374622345\n",
            "Epoch: 184 | Loss: 0.15263602137565613 | Val. Loss: 0.21320481598377228\n",
            "Epoch: 185 | Loss: 0.13344216346740723 | Val. Loss: 0.21107295155525208\n",
            "Epoch: 186 | Loss: 0.17542004585266113 | Val. Loss: 0.21049830317497253\n",
            "Epoch: 187 | Loss: 0.18950726091861725 | Val. Loss: 0.21024127304553986\n",
            "Epoch: 188 | Loss: 0.12070029973983765 | Val. Loss: 0.20910267531871796\n",
            "Epoch: 189 | Loss: 0.13743355870246887 | Val. Loss: 0.2085992395877838\n",
            "Epoch: 190 | Loss: 0.18493397533893585 | Val. Loss: 0.20759670436382294\n",
            "Epoch: 191 | Loss: 0.12779076397418976 | Val. Loss: 0.2072998285293579\n",
            "Epoch: 192 | Loss: 0.17078861594200134 | Val. Loss: 0.2068851739168167\n",
            "Epoch: 193 | Loss: 0.13488642871379852 | Val. Loss: 0.2070993185043335\n",
            "Epoch: 194 | Loss: 0.12203004956245422 | Val. Loss: 0.20466561615467072\n",
            "Epoch: 195 | Loss: 0.17841237783432007 | Val. Loss: 0.20686490833759308\n",
            "Epoch: 196 | Loss: 0.1288260966539383 | Val. Loss: 0.20416918396949768\n",
            "Epoch: 197 | Loss: 0.1718287169933319 | Val. Loss: 0.20481321215629578\n",
            "Epoch: 198 | Loss: 0.1868675947189331 | Val. Loss: 0.20238319039344788\n",
            "Epoch: 199 | Loss: 0.216717928647995 | Val. Loss: 0.2011002004146576\n",
            "Epoch: 200 | Loss: 0.10987374931573868 | Val. Loss: 0.20330730080604553\n",
            "Epoch: 201 | Loss: 0.28245314955711365 | Val. Loss: 0.20272795855998993\n",
            "Epoch: 202 | Loss: 0.1579674780368805 | Val. Loss: 0.20174315571784973\n",
            "Epoch: 203 | Loss: 0.15093488991260529 | Val. Loss: 0.20001430809497833\n",
            "Epoch: 204 | Loss: 0.22202084958553314 | Val. Loss: 0.1998966783285141\n",
            "Epoch: 205 | Loss: 0.12577059864997864 | Val. Loss: 0.20024915039539337\n",
            "Epoch: 206 | Loss: 0.16384032368659973 | Val. Loss: 0.1991008222103119\n",
            "Epoch: 207 | Loss: 0.20153100788593292 | Val. Loss: 0.19854821264743805\n",
            "Epoch: 208 | Loss: 0.15549765527248383 | Val. Loss: 0.1978093981742859\n",
            "Epoch: 209 | Loss: 0.11719055473804474 | Val. Loss: 0.1978265941143036\n",
            "Epoch: 210 | Loss: 0.17384569346904755 | Val. Loss: 0.1971348077058792\n",
            "Epoch: 211 | Loss: 0.17124219238758087 | Val. Loss: 0.1969584822654724\n",
            "Epoch: 212 | Loss: 0.1544572412967682 | Val. Loss: 0.1976732611656189\n",
            "Epoch: 213 | Loss: 0.09833312034606934 | Val. Loss: 0.19594570994377136\n",
            "Epoch: 214 | Loss: 0.1463918536901474 | Val. Loss: 0.19627180695533752\n",
            "Epoch: 215 | Loss: 0.2600604295730591 | Val. Loss: 0.19464179873466492\n",
            "Epoch: 216 | Loss: 0.18456028401851654 | Val. Loss: 0.19687597453594208\n",
            "Epoch: 217 | Loss: 0.12890110909938812 | Val. Loss: 0.19710798561573029\n",
            "Epoch: 218 | Loss: 0.13768531382083893 | Val. Loss: 0.19414116442203522\n",
            "Epoch: 219 | Loss: 0.13949254155158997 | Val. Loss: 0.19525282084941864\n",
            "Epoch: 220 | Loss: 0.11828498542308807 | Val. Loss: 0.1942245364189148\n",
            "Epoch: 221 | Loss: 0.17875266075134277 | Val. Loss: 0.19349968433380127\n",
            "Epoch: 222 | Loss: 0.19661900401115417 | Val. Loss: 0.19258737564086914\n",
            "Epoch: 223 | Loss: 0.13665224611759186 | Val. Loss: 0.19106745719909668\n",
            "Epoch: 224 | Loss: 0.0659279078245163 | Val. Loss: 0.18925340473651886\n",
            "Epoch: 225 | Loss: 0.12732647359371185 | Val. Loss: 0.19046106934547424\n",
            "Epoch: 226 | Loss: 0.12273922562599182 | Val. Loss: 0.1893780678510666\n",
            "Epoch: 227 | Loss: 0.12054979056119919 | Val. Loss: 0.18824094533920288\n",
            "Epoch: 228 | Loss: 0.11369805037975311 | Val. Loss: 0.18800029158592224\n",
            "Epoch: 229 | Loss: 0.07797468453645706 | Val. Loss: 0.18665340542793274\n",
            "Epoch: 230 | Loss: 0.14838433265686035 | Val. Loss: 0.18496139347553253\n",
            "Epoch: 231 | Loss: 0.15576566755771637 | Val. Loss: 0.18450458347797394\n",
            "Epoch: 232 | Loss: 0.136128231883049 | Val. Loss: 0.18379411101341248\n",
            "Epoch: 233 | Loss: 0.06965573132038116 | Val. Loss: 0.18275584280490875\n",
            "Epoch: 234 | Loss: 0.16390931606292725 | Val. Loss: 0.18087708950042725\n",
            "Epoch: 235 | Loss: 0.08268758654594421 | Val. Loss: 0.18063800036907196\n",
            "Epoch: 236 | Loss: 0.1275266855955124 | Val. Loss: 0.17948850989341736\n",
            "Epoch: 237 | Loss: 0.08594053983688354 | Val. Loss: 0.18007706105709076\n",
            "Epoch: 238 | Loss: 0.16459695994853973 | Val. Loss: 0.1791425496339798\n",
            "Epoch: 239 | Loss: 0.17897090315818787 | Val. Loss: 0.17958059906959534\n",
            "Epoch: 240 | Loss: 0.12275290489196777 | Val. Loss: 0.17979492247104645\n",
            "Epoch: 241 | Loss: 0.20233389735221863 | Val. Loss: 0.17896340787410736\n",
            "Epoch: 242 | Loss: 0.13848936557769775 | Val. Loss: 0.17860916256904602\n",
            "Epoch: 243 | Loss: 0.13549582660198212 | Val. Loss: 0.1776197850704193\n",
            "Epoch: 244 | Loss: 0.0956883579492569 | Val. Loss: 0.1765517145395279\n",
            "Epoch: 245 | Loss: 0.12956330180168152 | Val. Loss: 0.17748676240444183\n",
            "Epoch: 246 | Loss: 0.17945493757724762 | Val. Loss: 0.177961528301239\n",
            "Epoch: 247 | Loss: 0.07039649039506912 | Val. Loss: 0.17620310187339783\n",
            "Epoch: 248 | Loss: 0.13438372313976288 | Val. Loss: 0.17555585503578186\n",
            "Epoch: 249 | Loss: 0.13117757439613342 | Val. Loss: 0.17364080250263214\n",
            "Epoch: 250 | Loss: 0.13953210413455963 | Val. Loss: 0.17328983545303345\n",
            "Epoch: 251 | Loss: 0.09645780175924301 | Val. Loss: 0.1790429800748825\n",
            "Epoch: 252 | Loss: 0.09549769014120102 | Val. Loss: 0.17264845967292786\n",
            "Epoch: 253 | Loss: 0.10721992701292038 | Val. Loss: 0.17342497408390045\n",
            "Epoch: 254 | Loss: 0.16284820437431335 | Val. Loss: 0.1763898730278015\n",
            "Epoch: 255 | Loss: 0.17469224333763123 | Val. Loss: 0.17434057593345642\n",
            "Epoch: 256 | Loss: 0.13576431572437286 | Val. Loss: 0.17587865889072418\n",
            "Epoch: 257 | Loss: 0.12806062400341034 | Val. Loss: 0.17642833292484283\n",
            "Epoch: 258 | Loss: 0.05488579347729683 | Val. Loss: 0.17486572265625\n",
            "Epoch: 259 | Loss: 0.1538333147764206 | Val. Loss: 0.17111225426197052\n",
            "Epoch: 260 | Loss: 0.09459300339221954 | Val. Loss: 0.171726793050766\n",
            "Epoch: 261 | Loss: 0.20983481407165527 | Val. Loss: 0.17064788937568665\n",
            "Epoch: 262 | Loss: 0.1714877337217331 | Val. Loss: 0.16815327107906342\n",
            "Epoch: 263 | Loss: 0.1736161708831787 | Val. Loss: 0.1676206737756729\n",
            "Epoch: 264 | Loss: 0.10844328999519348 | Val. Loss: 0.16437247395515442\n",
            "Epoch: 265 | Loss: 0.0970936119556427 | Val. Loss: 0.16097195446491241\n",
            "Epoch: 266 | Loss: 0.06893928349018097 | Val. Loss: 0.15891031920909882\n",
            "Epoch: 267 | Loss: 0.10154203325510025 | Val. Loss: 0.16051173210144043\n",
            "Epoch: 268 | Loss: 0.133317232131958 | Val. Loss: 0.15753264725208282\n",
            "Epoch: 269 | Loss: 0.1471695750951767 | Val. Loss: 0.15854009985923767\n",
            "Epoch: 270 | Loss: 0.06804365664720535 | Val. Loss: 0.1552797257900238\n",
            "Epoch: 271 | Loss: 0.11097102612257004 | Val. Loss: 0.1541951447725296\n",
            "Epoch: 272 | Loss: 0.1186353787779808 | Val. Loss: 0.15463167428970337\n",
            "Epoch: 273 | Loss: 0.10165558010339737 | Val. Loss: 0.15289749205112457\n",
            "Epoch: 274 | Loss: 0.11139204353094101 | Val. Loss: 0.15154796838760376\n",
            "Epoch: 275 | Loss: 0.10662297904491425 | Val. Loss: 0.1527930200099945\n",
            "Epoch: 276 | Loss: 0.15295147895812988 | Val. Loss: 0.14956000447273254\n",
            "Epoch: 277 | Loss: 0.13746455311775208 | Val. Loss: 0.14907953143119812\n",
            "Epoch: 278 | Loss: 0.16909193992614746 | Val. Loss: 0.14970853924751282\n",
            "Epoch: 279 | Loss: 0.10015885531902313 | Val. Loss: 0.1498815268278122\n",
            "Epoch: 280 | Loss: 0.09841365367174149 | Val. Loss: 0.148511603474617\n",
            "Epoch: 281 | Loss: 0.08191052079200745 | Val. Loss: 0.14947077631950378\n",
            "Epoch: 282 | Loss: 0.13440805673599243 | Val. Loss: 0.1483224332332611\n",
            "Epoch: 283 | Loss: 0.07828377932310104 | Val. Loss: 0.14515957236289978\n",
            "Epoch: 284 | Loss: 0.13451740145683289 | Val. Loss: 0.14597785472869873\n",
            "Epoch: 285 | Loss: 0.13438373804092407 | Val. Loss: 0.14618605375289917\n",
            "Epoch: 286 | Loss: 0.14464114606380463 | Val. Loss: 0.14568661153316498\n",
            "Epoch: 287 | Loss: 0.12325326353311539 | Val. Loss: 0.14626047015190125\n",
            "Epoch: 288 | Loss: 0.0673195868730545 | Val. Loss: 0.14359205961227417\n",
            "Epoch: 289 | Loss: 0.05805749073624611 | Val. Loss: 0.14394423365592957\n",
            "Epoch: 290 | Loss: 0.05803080275654793 | Val. Loss: 0.14390558004379272\n",
            "Epoch: 291 | Loss: 0.06784364581108093 | Val. Loss: 0.14224502444267273\n",
            "Epoch: 292 | Loss: 0.11521949619054794 | Val. Loss: 0.143610879778862\n",
            "Epoch: 293 | Loss: 0.08384672552347183 | Val. Loss: 0.14170141518115997\n",
            "Epoch: 294 | Loss: 0.08475983142852783 | Val. Loss: 0.14209993183612823\n",
            "Epoch: 295 | Loss: 0.05433342605829239 | Val. Loss: 0.14145395159721375\n",
            "Epoch: 296 | Loss: 0.0852167084813118 | Val. Loss: 0.14304932951927185\n",
            "Epoch: 297 | Loss: 0.11635050177574158 | Val. Loss: 0.13983367383480072\n",
            "Epoch: 298 | Loss: 0.12773676216602325 | Val. Loss: 0.14073553681373596\n",
            "Epoch: 299 | Loss: 0.12591108679771423 | Val. Loss: 0.14318528771400452\n",
            "Epoch: 300 | Loss: 0.12389405816793442 | Val. Loss: 0.14003917574882507\n",
            "Epoch: 301 | Loss: 0.1756180226802826 | Val. Loss: 0.1383848637342453\n",
            "Epoch: 302 | Loss: 0.11683077365159988 | Val. Loss: 0.13927002251148224\n",
            "Epoch: 303 | Loss: 0.2210894078016281 | Val. Loss: 0.13956813514232635\n",
            "Epoch: 304 | Loss: 0.17413611710071564 | Val. Loss: 0.13864226639270782\n",
            "Epoch: 305 | Loss: 0.06906677037477493 | Val. Loss: 0.13893717527389526\n",
            "Epoch: 306 | Loss: 0.06278838962316513 | Val. Loss: 0.13955120742321014\n",
            "Epoch: 307 | Loss: 0.06385472416877747 | Val. Loss: 0.13901755213737488\n",
            "Epoch: 308 | Loss: 0.09872826188802719 | Val. Loss: 0.13902656733989716\n",
            "Epoch: 309 | Loss: 0.16253724694252014 | Val. Loss: 0.14123307168483734\n",
            "Epoch: 310 | Loss: 0.03501678258180618 | Val. Loss: 0.13848808407783508\n",
            "Epoch: 311 | Loss: 0.18507421016693115 | Val. Loss: 0.13759632408618927\n",
            "Epoch: 312 | Loss: 0.07847518473863602 | Val. Loss: 0.1403692662715912\n",
            "Epoch: 313 | Loss: 0.0638984739780426 | Val. Loss: 0.13695958256721497\n",
            "Epoch: 314 | Loss: 0.10508289933204651 | Val. Loss: 0.1371346414089203\n",
            "Epoch: 315 | Loss: 0.04689517244696617 | Val. Loss: 0.13776971399784088\n",
            "Epoch: 316 | Loss: 0.07119598239660263 | Val. Loss: 0.13555485010147095\n",
            "Epoch: 317 | Loss: 0.040163829922676086 | Val. Loss: 0.13631011545658112\n",
            "Epoch: 318 | Loss: 0.059310127049684525 | Val. Loss: 0.1349697858095169\n",
            "Epoch: 319 | Loss: 0.046093009412288666 | Val. Loss: 0.13403478264808655\n",
            "Epoch: 320 | Loss: 0.09967916458845139 | Val. Loss: 0.1346038281917572\n",
            "Epoch: 321 | Loss: 0.1419794261455536 | Val. Loss: 0.13417786359786987\n",
            "Epoch: 322 | Loss: 0.09183549880981445 | Val. Loss: 0.13221479952335358\n",
            "Epoch: 323 | Loss: 0.14779029786586761 | Val. Loss: 0.13011516630649567\n",
            "Epoch: 324 | Loss: 0.16782939434051514 | Val. Loss: 0.1336013525724411\n",
            "Epoch: 325 | Loss: 0.05682145431637764 | Val. Loss: 0.13106071949005127\n",
            "Epoch: 326 | Loss: 0.073048897087574 | Val. Loss: 0.12895479798316956\n",
            "Epoch: 327 | Loss: 0.19816546142101288 | Val. Loss: 0.1296398937702179\n",
            "Epoch: 328 | Loss: 0.03376088663935661 | Val. Loss: 0.12929251790046692\n",
            "Epoch: 329 | Loss: 0.051137879490852356 | Val. Loss: 0.1288939267396927\n",
            "Epoch: 330 | Loss: 0.15574505925178528 | Val. Loss: 0.12780314683914185\n",
            "Epoch: 331 | Loss: 0.07507868856191635 | Val. Loss: 0.12688657641410828\n",
            "Epoch: 332 | Loss: 0.11811896413564682 | Val. Loss: 0.12592384219169617\n",
            "Epoch: 333 | Loss: 0.11633086949586868 | Val. Loss: 0.12586140632629395\n",
            "Epoch: 334 | Loss: 0.056834496557712555 | Val. Loss: 0.12515732645988464\n",
            "Epoch: 335 | Loss: 0.05179446190595627 | Val. Loss: 0.12630890309810638\n",
            "Epoch: 336 | Loss: 0.0876137986779213 | Val. Loss: 0.12472309172153473\n",
            "Epoch: 337 | Loss: 0.0206283126026392 | Val. Loss: 0.1229773536324501\n",
            "Epoch: 338 | Loss: 0.03805612027645111 | Val. Loss: 0.12286163866519928\n",
            "Epoch: 339 | Loss: 0.07231451570987701 | Val. Loss: 0.12340293824672699\n",
            "Epoch: 340 | Loss: 0.09264111518859863 | Val. Loss: 0.12328340858221054\n",
            "Epoch: 341 | Loss: 0.09374359995126724 | Val. Loss: 0.12218624353408813\n",
            "Epoch: 342 | Loss: 0.041705068200826645 | Val. Loss: 0.12099232524633408\n",
            "Epoch: 343 | Loss: 0.055237043648958206 | Val. Loss: 0.12153618037700653\n",
            "Epoch: 344 | Loss: 0.0595986507833004 | Val. Loss: 0.12088745832443237\n",
            "Epoch: 345 | Loss: 0.07518754154443741 | Val. Loss: 0.11973923444747925\n",
            "Epoch: 346 | Loss: 0.0486455000936985 | Val. Loss: 0.11921568959951401\n",
            "Epoch: 347 | Loss: 0.13675180077552795 | Val. Loss: 0.1186499372124672\n",
            "Epoch: 348 | Loss: 0.1350691318511963 | Val. Loss: 0.11880646646022797\n",
            "Epoch: 349 | Loss: 0.04250325635075569 | Val. Loss: 0.11789196729660034\n",
            "Epoch: 350 | Loss: 0.061730120331048965 | Val. Loss: 0.11779884994029999\n",
            "Epoch: 351 | Loss: 0.053583454340696335 | Val. Loss: 0.11824808269739151\n",
            "Epoch: 352 | Loss: 0.045684393495321274 | Val. Loss: 0.11751686036586761\n",
            "Epoch: 353 | Loss: 0.10327252000570297 | Val. Loss: 0.11587488651275635\n",
            "Epoch: 354 | Loss: 0.07562565803527832 | Val. Loss: 0.11617045104503632\n",
            "Epoch: 355 | Loss: 0.04096045345067978 | Val. Loss: 0.11571343243122101\n",
            "Epoch: 356 | Loss: 0.025521500036120415 | Val. Loss: 0.11571544408798218\n",
            "Epoch: 357 | Loss: 0.05541699007153511 | Val. Loss: 0.11527106910943985\n",
            "Epoch: 358 | Loss: 0.05063105374574661 | Val. Loss: 0.1139397993683815\n",
            "Epoch: 359 | Loss: 0.04902847111225128 | Val. Loss: 0.11489588022232056\n",
            "Epoch: 360 | Loss: 0.04387395456433296 | Val. Loss: 0.11401093006134033\n",
            "Epoch: 361 | Loss: 0.036465633660554886 | Val. Loss: 0.11354938894510269\n",
            "Epoch: 362 | Loss: 0.06089622154831886 | Val. Loss: 0.11205440759658813\n",
            "Epoch: 363 | Loss: 0.08507297188043594 | Val. Loss: 0.11210646480321884\n",
            "Epoch: 364 | Loss: 0.027901917695999146 | Val. Loss: 0.11138533055782318\n",
            "Epoch: 365 | Loss: 0.03991705924272537 | Val. Loss: 0.11133827269077301\n",
            "Epoch: 366 | Loss: 0.04846182465553284 | Val. Loss: 0.11116296052932739\n",
            "Epoch: 367 | Loss: 0.03363778814673424 | Val. Loss: 0.11028013378381729\n",
            "Epoch: 368 | Loss: 0.07444847375154495 | Val. Loss: 0.1103522777557373\n",
            "Epoch: 369 | Loss: 0.038194939494132996 | Val. Loss: 0.10951795428991318\n",
            "Epoch: 370 | Loss: 0.04492076486349106 | Val. Loss: 0.1089881882071495\n",
            "Epoch: 371 | Loss: 0.07834737747907639 | Val. Loss: 0.10865100473165512\n",
            "Epoch: 372 | Loss: 0.0783739760518074 | Val. Loss: 0.10746783018112183\n",
            "Epoch: 373 | Loss: 0.08354821801185608 | Val. Loss: 0.10779615491628647\n",
            "Epoch: 374 | Loss: 0.10433806478977203 | Val. Loss: 0.1069367527961731\n",
            "Epoch: 375 | Loss: 0.11538338661193848 | Val. Loss: 0.10719070583581924\n",
            "Epoch: 376 | Loss: 0.10413811355829239 | Val. Loss: 0.10743176937103271\n",
            "Epoch: 377 | Loss: 0.04422596096992493 | Val. Loss: 0.1067161113023758\n",
            "Epoch: 378 | Loss: 0.07406243681907654 | Val. Loss: 0.10579551756381989\n",
            "Epoch: 379 | Loss: 0.15167894959449768 | Val. Loss: 0.10588278621435165\n",
            "Epoch: 380 | Loss: 0.11567988246679306 | Val. Loss: 0.10505320876836777\n",
            "Epoch: 381 | Loss: 0.03681103512644768 | Val. Loss: 0.10422377288341522\n",
            "Epoch: 382 | Loss: 0.09253446757793427 | Val. Loss: 0.10412738472223282\n",
            "Epoch: 383 | Loss: 0.08344999700784683 | Val. Loss: 0.10422605276107788\n",
            "Epoch: 384 | Loss: 0.09659186750650406 | Val. Loss: 0.10407010465860367\n",
            "Epoch: 385 | Loss: 0.05364532768726349 | Val. Loss: 0.10342951118946075\n",
            "Epoch: 386 | Loss: 0.08370818942785263 | Val. Loss: 0.10328809171915054\n",
            "Epoch: 387 | Loss: 0.10358422994613647 | Val. Loss: 0.10195563733577728\n",
            "Epoch: 388 | Loss: 0.07731634378433228 | Val. Loss: 0.10206189006567001\n",
            "Epoch: 389 | Loss: 0.05526958778500557 | Val. Loss: 0.10175321996212006\n",
            "Epoch: 390 | Loss: 0.03954246640205383 | Val. Loss: 0.10109982639551163\n",
            "Epoch: 391 | Loss: 0.0791783556342125 | Val. Loss: 0.1008550375699997\n",
            "Epoch: 392 | Loss: 0.024836506694555283 | Val. Loss: 0.10026812553405762\n",
            "Epoch: 393 | Loss: 0.08890178799629211 | Val. Loss: 0.10075747966766357\n",
            "Epoch: 394 | Loss: 0.026577891781926155 | Val. Loss: 0.10074321180582047\n",
            "Epoch: 395 | Loss: 0.06574319303035736 | Val. Loss: 0.09952764213085175\n",
            "Epoch: 396 | Loss: 0.051424141973257065 | Val. Loss: 0.09958390891551971\n",
            "Epoch: 397 | Loss: 0.13320572674274445 | Val. Loss: 0.10009807348251343\n",
            "Epoch: 398 | Loss: 0.09667177498340607 | Val. Loss: 0.0989806205034256\n",
            "Epoch: 399 | Loss: 0.019179539754986763 | Val. Loss: 0.0982903242111206\n",
            "Epoch: 400 | Loss: 0.0913294106721878 | Val. Loss: 0.09871579706668854\n",
            "Epoch: 401 | Loss: 0.11168389022350311 | Val. Loss: 0.09802715480327606\n",
            "Epoch: 402 | Loss: 0.10288093239068985 | Val. Loss: 0.09745199978351593\n",
            "Epoch: 403 | Loss: 0.09280100464820862 | Val. Loss: 0.09717585146427155\n",
            "Epoch: 404 | Loss: 0.06687741726636887 | Val. Loss: 0.09695246815681458\n",
            "Epoch: 405 | Loss: 0.09301111102104187 | Val. Loss: 0.09729044139385223\n",
            "Epoch: 406 | Loss: 0.024811293929815292 | Val. Loss: 0.09659253060817719\n",
            "Epoch: 407 | Loss: 0.019996289163827896 | Val. Loss: 0.09609822928905487\n",
            "Epoch: 408 | Loss: 0.13406620919704437 | Val. Loss: 0.0951639860868454\n",
            "Epoch: 409 | Loss: 0.0778694897890091 | Val. Loss: 0.0934014767408371\n",
            "Epoch: 410 | Loss: 0.05753466114401817 | Val. Loss: 0.09228955954313278\n",
            "Epoch: 411 | Loss: 0.1374349445104599 | Val. Loss: 0.09146430343389511\n",
            "Epoch: 412 | Loss: 0.08947602659463882 | Val. Loss: 0.09130949527025223\n",
            "Epoch: 413 | Loss: 0.07253694534301758 | Val. Loss: 0.08980001509189606\n",
            "Epoch: 414 | Loss: 0.029640445485711098 | Val. Loss: 0.08991455286741257\n",
            "Epoch: 415 | Loss: 0.03843710944056511 | Val. Loss: 0.09016997367143631\n",
            "Epoch: 416 | Loss: 0.036461278796195984 | Val. Loss: 0.08844161033630371\n",
            "Epoch: 417 | Loss: 0.07139061391353607 | Val. Loss: 0.08824728429317474\n",
            "Epoch: 418 | Loss: 0.052148036658763885 | Val. Loss: 0.08782318234443665\n",
            "Epoch: 419 | Loss: 0.19278490543365479 | Val. Loss: 0.08783189952373505\n",
            "Epoch: 420 | Loss: 0.04929807037115097 | Val. Loss: 0.08646073937416077\n",
            "Epoch: 421 | Loss: 0.06068982183933258 | Val. Loss: 0.08597515523433685\n",
            "Epoch: 422 | Loss: 0.0517081618309021 | Val. Loss: 0.0877809152007103\n",
            "Epoch: 423 | Loss: 0.10743118822574615 | Val. Loss: 0.08715231716632843\n",
            "Epoch: 424 | Loss: 0.04005961865186691 | Val. Loss: 0.08648321777582169\n",
            "Epoch: 425 | Loss: 0.05609886348247528 | Val. Loss: 0.0853511244058609\n",
            "Epoch: 426 | Loss: 0.048197418451309204 | Val. Loss: 0.08513800799846649\n",
            "Epoch: 427 | Loss: 0.07749824970960617 | Val. Loss: 0.08445193618535995\n",
            "Epoch: 428 | Loss: 0.1432504504919052 | Val. Loss: 0.08455376327037811\n",
            "Epoch: 429 | Loss: 0.10101722180843353 | Val. Loss: 0.08416412025690079\n",
            "Epoch: 430 | Loss: 0.04442331939935684 | Val. Loss: 0.083254374563694\n",
            "Epoch: 431 | Loss: 0.10090464353561401 | Val. Loss: 0.08305764943361282\n",
            "Epoch: 432 | Loss: 0.07406409829854965 | Val. Loss: 0.08313437551259995\n",
            "Epoch: 433 | Loss: 0.02631712704896927 | Val. Loss: 0.0830540657043457\n",
            "Epoch: 434 | Loss: 0.09353739023208618 | Val. Loss: 0.08201495558023453\n",
            "Epoch: 435 | Loss: 0.09999125450849533 | Val. Loss: 0.08192060887813568\n",
            "Epoch: 436 | Loss: 0.038301002234220505 | Val. Loss: 0.08167536556720734\n",
            "Epoch: 437 | Loss: 0.06504692137241364 | Val. Loss: 0.082380510866642\n",
            "Epoch: 438 | Loss: 0.051593709737062454 | Val. Loss: 0.08165433257818222\n",
            "Epoch: 439 | Loss: 0.03931820020079613 | Val. Loss: 0.08095821738243103\n",
            "Epoch: 440 | Loss: 0.01714501529932022 | Val. Loss: 0.08128449320793152\n",
            "Epoch: 441 | Loss: 0.07340877503156662 | Val. Loss: 0.08123172074556351\n",
            "Epoch: 442 | Loss: 0.0329134501516819 | Val. Loss: 0.08048006147146225\n",
            "Epoch: 443 | Loss: 0.050517547875642776 | Val. Loss: 0.0804242491722107\n",
            "Epoch: 444 | Loss: 0.121425561606884 | Val. Loss: 0.08051593601703644\n",
            "Epoch: 445 | Loss: 0.03746503219008446 | Val. Loss: 0.07931458204984665\n",
            "Epoch: 446 | Loss: 0.07902991026639938 | Val. Loss: 0.07946456968784332\n",
            "Epoch: 447 | Loss: 0.07712902128696442 | Val. Loss: 0.07910647988319397\n",
            "Epoch: 448 | Loss: 0.03251438960433006 | Val. Loss: 0.07950511574745178\n",
            "Epoch: 449 | Loss: 0.025719892233610153 | Val. Loss: 0.07943443953990936\n",
            "Epoch: 450 | Loss: 0.07162205129861832 | Val. Loss: 0.07929985970258713\n",
            "Epoch: 451 | Loss: 0.018598850816488266 | Val. Loss: 0.07925751060247421\n",
            "Epoch: 452 | Loss: 0.029470959678292274 | Val. Loss: 0.07844023406505585\n",
            "Epoch: 453 | Loss: 0.02510557323694229 | Val. Loss: 0.07883401960134506\n",
            "Epoch: 454 | Loss: 0.045709654688835144 | Val. Loss: 0.07909329235553741\n",
            "Epoch: 455 | Loss: 0.04671935737133026 | Val. Loss: 0.07792605459690094\n",
            "Epoch: 456 | Loss: 0.09142106771469116 | Val. Loss: 0.07843150943517685\n",
            "Epoch: 457 | Loss: 0.028565552085638046 | Val. Loss: 0.07802094519138336\n",
            "Epoch: 458 | Loss: 0.07345105707645416 | Val. Loss: 0.07859243452548981\n",
            "Epoch: 459 | Loss: 0.06924927979707718 | Val. Loss: 0.07798977941274643\n",
            "Epoch: 460 | Loss: 0.045442238450050354 | Val. Loss: 0.07715880870819092\n",
            "Epoch: 461 | Loss: 0.0444386750459671 | Val. Loss: 0.0776812806725502\n",
            "Epoch: 462 | Loss: 0.03507062792778015 | Val. Loss: 0.07667708396911621\n",
            "Epoch: 463 | Loss: 0.018183762207627296 | Val. Loss: 0.07664405554533005\n",
            "Epoch: 464 | Loss: 0.03649172931909561 | Val. Loss: 0.07607421278953552\n",
            "Epoch: 465 | Loss: 0.022530149668455124 | Val. Loss: 0.0764516294002533\n",
            "Epoch: 466 | Loss: 0.01583177223801613 | Val. Loss: 0.07564713060855865\n",
            "Epoch: 467 | Loss: 0.040641989558935165 | Val. Loss: 0.07596146315336227\n",
            "Epoch: 468 | Loss: 0.020725874230265617 | Val. Loss: 0.07670491188764572\n",
            "Epoch: 469 | Loss: 0.0690804272890091 | Val. Loss: 0.07570666074752808\n",
            "Epoch: 470 | Loss: 0.028857043012976646 | Val. Loss: 0.07518668472766876\n",
            "Epoch: 471 | Loss: 0.08488816022872925 | Val. Loss: 0.07436611503362656\n",
            "Epoch: 472 | Loss: 0.07468979805707932 | Val. Loss: 0.07397184520959854\n",
            "Epoch: 473 | Loss: 0.08441352099180222 | Val. Loss: 0.07342429459095001\n",
            "Epoch: 474 | Loss: 0.025933900848031044 | Val. Loss: 0.07359476387500763\n",
            "Epoch: 475 | Loss: 0.08238373696804047 | Val. Loss: 0.07302593439817429\n",
            "Epoch: 476 | Loss: 0.0367296040058136 | Val. Loss: 0.0733327642083168\n",
            "Epoch: 477 | Loss: 0.07527024298906326 | Val. Loss: 0.07231330871582031\n",
            "Epoch: 478 | Loss: 0.02654566988348961 | Val. Loss: 0.07221539318561554\n",
            "Epoch: 479 | Loss: 0.07638678699731827 | Val. Loss: 0.07240661233663559\n",
            "Epoch: 480 | Loss: 0.030759543180465698 | Val. Loss: 0.07174740731716156\n",
            "Epoch: 481 | Loss: 0.037585195153951645 | Val. Loss: 0.07200952619314194\n",
            "Epoch: 482 | Loss: 0.08326642960309982 | Val. Loss: 0.07125401496887207\n",
            "Epoch: 483 | Loss: 0.0690050795674324 | Val. Loss: 0.07179420441389084\n",
            "Epoch: 484 | Loss: 0.08295684307813644 | Val. Loss: 0.07102160155773163\n",
            "Epoch: 485 | Loss: 0.04768195003271103 | Val. Loss: 0.0710914209485054\n",
            "Epoch: 486 | Loss: 0.02494143508374691 | Val. Loss: 0.07064718008041382\n",
            "Epoch: 487 | Loss: 0.015254531055688858 | Val. Loss: 0.07039730250835419\n",
            "Epoch: 488 | Loss: 0.07339710742235184 | Val. Loss: 0.07063324749469757\n",
            "Epoch: 489 | Loss: 0.08051407337188721 | Val. Loss: 0.07027648389339447\n",
            "Epoch: 490 | Loss: 0.018372129648923874 | Val. Loss: 0.06983761489391327\n",
            "Epoch: 491 | Loss: 0.031258970499038696 | Val. Loss: 0.07045517861843109\n",
            "Epoch: 492 | Loss: 0.04844100773334503 | Val. Loss: 0.06988232582807541\n",
            "Epoch: 493 | Loss: 0.12591537833213806 | Val. Loss: 0.06901586055755615\n",
            "Epoch: 494 | Loss: 0.06175314635038376 | Val. Loss: 0.06945599615573883\n",
            "Epoch: 495 | Loss: 0.030286293476819992 | Val. Loss: 0.06907974183559418\n",
            "Epoch: 496 | Loss: 0.02605779655277729 | Val. Loss: 0.06921021640300751\n",
            "Epoch: 497 | Loss: 0.17003564536571503 | Val. Loss: 0.06958873569965363\n",
            "Epoch: 498 | Loss: 0.01889268308877945 | Val. Loss: 0.06912536919116974\n",
            "Epoch: 499 | Loss: 0.023197313770651817 | Val. Loss: 0.06830461323261261\n",
            "Epoch: 500 | Loss: 0.02929512970149517 | Val. Loss: 0.06764709204435349\n",
            "Epoch: 501 | Loss: 0.05712989345192909 | Val. Loss: 0.07155287265777588\n",
            "Epoch: 502 | Loss: 0.015399564057588577 | Val. Loss: 0.07095347344875336\n",
            "Epoch: 503 | Loss: 0.020889991894364357 | Val. Loss: 0.0692242905497551\n",
            "Epoch: 504 | Loss: 0.037469297647476196 | Val. Loss: 0.06794603168964386\n",
            "Epoch: 505 | Loss: 0.026673834770917892 | Val. Loss: 0.06762680411338806\n",
            "Epoch: 506 | Loss: 0.1388731598854065 | Val. Loss: 0.06731016933917999\n",
            "Epoch: 507 | Loss: 0.028506727889180183 | Val. Loss: 0.06792322546243668\n",
            "Epoch: 508 | Loss: 0.027578260749578476 | Val. Loss: 0.06708601117134094\n",
            "Epoch: 509 | Loss: 0.0719565823674202 | Val. Loss: 0.06685111671686172\n",
            "Epoch: 510 | Loss: 0.030381862074136734 | Val. Loss: 0.06808042526245117\n",
            "Epoch: 511 | Loss: 0.08919466286897659 | Val. Loss: 0.06752033531665802\n",
            "Epoch: 512 | Loss: 0.06451956927776337 | Val. Loss: 0.06711422652006149\n",
            "Epoch: 513 | Loss: 0.01535086426883936 | Val. Loss: 0.06655583530664444\n",
            "Epoch: 514 | Loss: 0.02709934674203396 | Val. Loss: 0.06642846018075943\n",
            "Epoch: 515 | Loss: 0.0240698903799057 | Val. Loss: 0.06690174341201782\n",
            "Epoch: 516 | Loss: 0.024507960304617882 | Val. Loss: 0.06661752611398697\n",
            "Epoch: 517 | Loss: 0.02604948729276657 | Val. Loss: 0.06613940745592117\n",
            "Epoch: 518 | Loss: 0.02942131832242012 | Val. Loss: 0.06636146456003189\n",
            "Epoch: 519 | Loss: 0.07635599374771118 | Val. Loss: 0.066241554915905\n",
            "Epoch: 520 | Loss: 0.02053222432732582 | Val. Loss: 0.06585650146007538\n",
            "Epoch: 521 | Loss: 0.0702754408121109 | Val. Loss: 0.0660446435213089\n",
            "Epoch: 522 | Loss: 0.01328886765986681 | Val. Loss: 0.06576330959796906\n",
            "Epoch: 523 | Loss: 0.031021729111671448 | Val. Loss: 0.06539017707109451\n",
            "Epoch: 524 | Loss: 0.09667135775089264 | Val. Loss: 0.06601406633853912\n",
            "Epoch: 525 | Loss: 0.021158326417207718 | Val. Loss: 0.06608156859874725\n",
            "Epoch: 526 | Loss: 0.013417569920420647 | Val. Loss: 0.06493112444877625\n",
            "Epoch: 527 | Loss: 0.015011703595519066 | Val. Loss: 0.06534095108509064\n",
            "Epoch: 528 | Loss: 0.06965266168117523 | Val. Loss: 0.06508733332157135\n",
            "Epoch: 529 | Loss: 0.02069329284131527 | Val. Loss: 0.06543532758951187\n",
            "Epoch: 530 | Loss: 0.02088400162756443 | Val. Loss: 0.06510858237743378\n",
            "Epoch: 531 | Loss: 0.013767354190349579 | Val. Loss: 0.06517745554447174\n",
            "Epoch: 532 | Loss: 0.07158581167459488 | Val. Loss: 0.06479328125715256\n",
            "Epoch: 533 | Loss: 0.07219696044921875 | Val. Loss: 0.06472449004650116\n",
            "Epoch: 534 | Loss: 0.012488112784922123 | Val. Loss: 0.0655180960893631\n",
            "Epoch: 535 | Loss: 0.022626493126153946 | Val. Loss: 0.0652051717042923\n",
            "Epoch: 536 | Loss: 0.016971169039607048 | Val. Loss: 0.06430846452713013\n",
            "Epoch: 537 | Loss: 0.07575720548629761 | Val. Loss: 0.06479079276323318\n",
            "Epoch: 538 | Loss: 0.0712980255484581 | Val. Loss: 0.06423185765743256\n",
            "Epoch: 539 | Loss: 0.04547574371099472 | Val. Loss: 0.06374197453260422\n",
            "Epoch: 540 | Loss: 0.07233589142560959 | Val. Loss: 0.06436086446046829\n",
            "Epoch: 541 | Loss: 0.07241564244031906 | Val. Loss: 0.06398509442806244\n",
            "Epoch: 542 | Loss: 0.014051312580704689 | Val. Loss: 0.06372120976448059\n",
            "Epoch: 543 | Loss: 0.01485414057970047 | Val. Loss: 0.06378898024559021\n",
            "Epoch: 544 | Loss: 0.02889557182788849 | Val. Loss: 0.06379899382591248\n",
            "Epoch: 545 | Loss: 0.08063813298940659 | Val. Loss: 0.06357885897159576\n",
            "Epoch: 546 | Loss: 0.06712785363197327 | Val. Loss: 0.06379199028015137\n",
            "Epoch: 547 | Loss: 0.015547006390988827 | Val. Loss: 0.06371232867240906\n",
            "Epoch: 548 | Loss: 0.021839909255504608 | Val. Loss: 0.06343232095241547\n",
            "Epoch: 549 | Loss: 0.022244511172175407 | Val. Loss: 0.06329767405986786\n",
            "Epoch: 550 | Loss: 0.024089273065328598 | Val. Loss: 0.06353713572025299\n",
            "Epoch: 551 | Loss: 0.012795907445251942 | Val. Loss: 0.0629512369632721\n",
            "Epoch: 552 | Loss: 0.07353807240724564 | Val. Loss: 0.06314866244792938\n",
            "Epoch: 553 | Loss: 0.0729057714343071 | Val. Loss: 0.062981978058815\n",
            "Epoch: 554 | Loss: 0.042390938848257065 | Val. Loss: 0.06285211443901062\n",
            "Epoch: 555 | Loss: 0.017472950741648674 | Val. Loss: 0.06286982446908951\n",
            "Epoch: 556 | Loss: 0.026574892923235893 | Val. Loss: 0.06299002468585968\n",
            "Epoch: 557 | Loss: 0.06494738161563873 | Val. Loss: 0.06301899254322052\n",
            "Epoch: 558 | Loss: 0.0128454165533185 | Val. Loss: 0.06275045871734619\n",
            "Epoch: 559 | Loss: 0.04004250094294548 | Val. Loss: 0.06277552992105484\n",
            "Epoch: 560 | Loss: 0.02958112396299839 | Val. Loss: 0.06226761266589165\n",
            "Epoch: 561 | Loss: 0.06390328705310822 | Val. Loss: 0.0623166561126709\n",
            "Epoch: 562 | Loss: 0.03230694681406021 | Val. Loss: 0.06265192478895187\n",
            "Epoch: 563 | Loss: 0.024641647934913635 | Val. Loss: 0.06196203827857971\n",
            "Epoch: 564 | Loss: 0.024995893239974976 | Val. Loss: 0.06186332553625107\n",
            "Epoch: 565 | Loss: 0.01558355800807476 | Val. Loss: 0.06311827898025513\n",
            "Epoch: 566 | Loss: 0.0674365758895874 | Val. Loss: 0.0627053752541542\n",
            "Epoch: 567 | Loss: 0.02909172512590885 | Val. Loss: 0.06240957975387573\n",
            "Epoch: 568 | Loss: 0.1909017115831375 | Val. Loss: 0.062181442975997925\n",
            "Epoch: 569 | Loss: 0.012378823943436146 | Val. Loss: 0.062329210340976715\n",
            "Epoch: 570 | Loss: 0.08200515061616898 | Val. Loss: 0.06204789876937866\n",
            "Epoch: 571 | Loss: 0.010892225429415703 | Val. Loss: 0.06206598877906799\n",
            "Epoch: 572 | Loss: 0.0681392252445221 | Val. Loss: 0.06181980296969414\n",
            "Epoch: 573 | Loss: 0.026463113725185394 | Val. Loss: 0.061384666711091995\n",
            "Epoch: 574 | Loss: 0.017666513100266457 | Val. Loss: 0.06177308037877083\n",
            "Epoch: 575 | Loss: 0.0675133466720581 | Val. Loss: 0.061810873448848724\n",
            "Epoch: 576 | Loss: 0.02397061325609684 | Val. Loss: 0.06145831197500229\n",
            "Epoch: 577 | Loss: 0.0730455070734024 | Val. Loss: 0.06196054071187973\n",
            "Epoch: 578 | Loss: 0.01017308235168457 | Val. Loss: 0.06130530312657356\n",
            "Epoch: 579 | Loss: 0.007697773166000843 | Val. Loss: 0.06152646988630295\n",
            "Epoch: 580 | Loss: 0.022184429690241814 | Val. Loss: 0.06130132079124451\n",
            "Epoch: 581 | Loss: 0.01603229157626629 | Val. Loss: 0.0615089014172554\n",
            "Epoch: 582 | Loss: 0.07389317452907562 | Val. Loss: 0.06098158285021782\n",
            "Epoch: 583 | Loss: 0.009176796302199364 | Val. Loss: 0.061894409358501434\n",
            "Epoch: 584 | Loss: 0.06910932809114456 | Val. Loss: 0.06133170798420906\n",
            "Epoch: 585 | Loss: 0.018997937440872192 | Val. Loss: 0.061181049793958664\n",
            "Epoch: 586 | Loss: 0.01779855042695999 | Val. Loss: 0.06117323786020279\n",
            "Epoch: 587 | Loss: 0.0069277905859053135 | Val. Loss: 0.060972779989242554\n",
            "Epoch: 588 | Loss: 0.019830305129289627 | Val. Loss: 0.061354149132966995\n",
            "Epoch: 589 | Loss: 0.030887456610798836 | Val. Loss: 0.060782261192798615\n",
            "Epoch: 590 | Loss: 0.019058948382735252 | Val. Loss: 0.060829661786556244\n",
            "Epoch: 591 | Loss: 0.040296249091625214 | Val. Loss: 0.06074449419975281\n",
            "Epoch: 592 | Loss: 0.06469693779945374 | Val. Loss: 0.06078999489545822\n",
            "Epoch: 593 | Loss: 0.08083494007587433 | Val. Loss: 0.060882024466991425\n",
            "Epoch: 594 | Loss: 0.022907279431819916 | Val. Loss: 0.060667891055345535\n",
            "Epoch: 595 | Loss: 0.015790538862347603 | Val. Loss: 0.06110839173197746\n",
            "Epoch: 596 | Loss: 0.013792670331895351 | Val. Loss: 0.06048062443733215\n",
            "Epoch: 597 | Loss: 0.08644377440214157 | Val. Loss: 0.0605773851275444\n",
            "Epoch: 598 | Loss: 0.07271495461463928 | Val. Loss: 0.06033221632242203\n",
            "Epoch: 599 | Loss: 0.11895876377820969 | Val. Loss: 0.060552455484867096\n",
            "Epoch: 600 | Loss: 0.015926571562886238 | Val. Loss: 0.060425303876399994\n",
            "Epoch: 601 | Loss: 0.0778535008430481 | Val. Loss: 0.06051965430378914\n",
            "Epoch: 602 | Loss: 0.023438632488250732 | Val. Loss: 0.060250043869018555\n",
            "Epoch: 603 | Loss: 0.06787079572677612 | Val. Loss: 0.06007416173815727\n",
            "Epoch: 604 | Loss: 0.022245023399591446 | Val. Loss: 0.06074840947985649\n",
            "Epoch: 605 | Loss: 0.012028718367218971 | Val. Loss: 0.05987156182527542\n",
            "Epoch: 606 | Loss: 0.015315273776650429 | Val. Loss: 0.05998709797859192\n",
            "Epoch: 607 | Loss: 0.014073589816689491 | Val. Loss: 0.059946250170469284\n",
            "Epoch: 608 | Loss: 0.07413384318351746 | Val. Loss: 0.059784721583127975\n",
            "Epoch: 609 | Loss: 0.026577450335025787 | Val. Loss: 0.060142021626234055\n",
            "Epoch: 610 | Loss: 0.07002135366201401 | Val. Loss: 0.05974077433347702\n",
            "Epoch: 611 | Loss: 0.025050416588783264 | Val. Loss: 0.05970848724246025\n",
            "Epoch: 612 | Loss: 0.014201255515217781 | Val. Loss: 0.05975038558244705\n",
            "Epoch: 613 | Loss: 0.1298603117465973 | Val. Loss: 0.059644915163517\n",
            "Epoch: 614 | Loss: 0.025313176214694977 | Val. Loss: 0.059457551687955856\n",
            "Epoch: 615 | Loss: 0.06612147390842438 | Val. Loss: 0.060162533074617386\n",
            "Epoch: 616 | Loss: 0.07145950943231583 | Val. Loss: 0.06015262007713318\n",
            "Epoch: 617 | Loss: 0.04913085326552391 | Val. Loss: 0.05928391218185425\n",
            "Epoch: 618 | Loss: 0.026609135791659355 | Val. Loss: 0.059739045798778534\n",
            "Epoch: 619 | Loss: 0.07003030925989151 | Val. Loss: 0.05927502363920212\n",
            "Epoch: 620 | Loss: 0.00831339880824089 | Val. Loss: 0.059664249420166016\n",
            "Epoch: 621 | Loss: 0.06859374046325684 | Val. Loss: 0.05932985618710518\n",
            "Epoch: 622 | Loss: 0.01962089352309704 | Val. Loss: 0.05949373170733452\n",
            "Epoch: 623 | Loss: 0.07161018997430801 | Val. Loss: 0.059464652091264725\n",
            "Epoch: 624 | Loss: 0.017558831721544266 | Val. Loss: 0.059079963713884354\n",
            "Epoch: 625 | Loss: 0.06545325368642807 | Val. Loss: 0.05927733704447746\n",
            "Epoch: 626 | Loss: 0.07248459756374359 | Val. Loss: 0.059007156640291214\n",
            "Epoch: 627 | Loss: 0.07516562193632126 | Val. Loss: 0.05920853093266487\n",
            "Epoch: 628 | Loss: 0.07173758745193481 | Val. Loss: 0.05915812402963638\n",
            "Epoch: 629 | Loss: 0.07143182307481766 | Val. Loss: 0.059082649648189545\n",
            "Epoch: 630 | Loss: 0.010868574492633343 | Val. Loss: 0.059002816677093506\n",
            "Epoch: 631 | Loss: 0.06470294296741486 | Val. Loss: 0.05887801572680473\n",
            "Epoch: 632 | Loss: 0.03385521471500397 | Val. Loss: 0.05885874107480049\n",
            "Epoch: 633 | Loss: 0.016650047153234482 | Val. Loss: 0.058826714754104614\n",
            "Epoch: 634 | Loss: 0.018223794177174568 | Val. Loss: 0.058921705931425095\n",
            "Epoch: 635 | Loss: 0.009892038069665432 | Val. Loss: 0.058866508305072784\n",
            "Epoch: 636 | Loss: 0.010613194666802883 | Val. Loss: 0.058923907577991486\n",
            "Epoch: 637 | Loss: 0.08420053124427795 | Val. Loss: 0.05867888405919075\n",
            "Epoch: 638 | Loss: 0.02092592976987362 | Val. Loss: 0.058882005512714386\n",
            "Epoch: 639 | Loss: 0.009539883583784103 | Val. Loss: 0.058704931288957596\n",
            "Epoch: 640 | Loss: 0.01490399893373251 | Val. Loss: 0.05867105722427368\n",
            "Epoch: 641 | Loss: 0.07271980494260788 | Val. Loss: 0.05856402590870857\n",
            "Epoch: 642 | Loss: 0.07628560066223145 | Val. Loss: 0.058509789407253265\n",
            "Epoch: 643 | Loss: 0.02452365681529045 | Val. Loss: 0.05863305926322937\n",
            "Epoch: 644 | Loss: 0.02817768231034279 | Val. Loss: 0.05862016603350639\n",
            "Epoch: 645 | Loss: 0.015974124893546104 | Val. Loss: 0.05845607444643974\n",
            "Epoch: 646 | Loss: 0.07644597440958023 | Val. Loss: 0.05843483284115791\n",
            "Epoch: 647 | Loss: 0.009524213150143623 | Val. Loss: 0.0585988387465477\n",
            "Epoch: 648 | Loss: 0.06372929364442825 | Val. Loss: 0.05852655693888664\n",
            "Epoch: 649 | Loss: 0.02030177228152752 | Val. Loss: 0.05839959532022476\n",
            "Epoch: 650 | Loss: 0.021779710426926613 | Val. Loss: 0.058253489434719086\n",
            "Epoch: 651 | Loss: 0.13625100255012512 | Val. Loss: 0.05843701213598251\n",
            "Epoch: 652 | Loss: 0.013321376405656338 | Val. Loss: 0.058022625744342804\n",
            "Epoch: 653 | Loss: 0.010607460513710976 | Val. Loss: 0.05825362727046013\n",
            "Epoch: 654 | Loss: 0.023384205996990204 | Val. Loss: 0.058194804936647415\n",
            "Epoch: 655 | Loss: 0.012598458677530289 | Val. Loss: 0.0580952949821949\n",
            "Epoch: 656 | Loss: 0.010153519921004772 | Val. Loss: 0.05797451734542847\n",
            "Epoch: 657 | Loss: 0.008026426658034325 | Val. Loss: 0.058240972459316254\n",
            "Epoch: 658 | Loss: 0.08431588113307953 | Val. Loss: 0.058098744601011276\n",
            "Epoch: 659 | Loss: 0.013314410112798214 | Val. Loss: 0.05791525915265083\n",
            "Epoch: 660 | Loss: 0.009524431079626083 | Val. Loss: 0.05805019661784172\n",
            "Epoch: 661 | Loss: 0.007113223895430565 | Val. Loss: 0.05833715945482254\n",
            "Epoch: 662 | Loss: 0.06590390205383301 | Val. Loss: 0.05785292387008667\n",
            "Epoch: 663 | Loss: 0.03022380731999874 | Val. Loss: 0.058152854442596436\n",
            "Epoch: 664 | Loss: 0.08378568291664124 | Val. Loss: 0.058140866458415985\n",
            "Epoch: 665 | Loss: 0.07651836425065994 | Val. Loss: 0.057801563292741776\n",
            "Epoch: 666 | Loss: 0.010250027291476727 | Val. Loss: 0.05780811980366707\n",
            "Epoch: 667 | Loss: 0.013386640697717667 | Val. Loss: 0.05798972770571709\n",
            "Epoch: 668 | Loss: 0.00895090214908123 | Val. Loss: 0.057996027171611786\n",
            "Epoch: 669 | Loss: 0.06962968409061432 | Val. Loss: 0.057816047221422195\n",
            "Epoch: 670 | Loss: 0.013906074687838554 | Val. Loss: 0.057891201227903366\n",
            "Epoch: 671 | Loss: 0.008947276510298252 | Val. Loss: 0.05753372982144356\n",
            "Epoch: 672 | Loss: 0.008573525585234165 | Val. Loss: 0.05763145536184311\n",
            "Epoch: 673 | Loss: 0.011915989220142365 | Val. Loss: 0.0576242133975029\n",
            "Epoch: 674 | Loss: 0.012156536802649498 | Val. Loss: 0.057375840842723846\n",
            "Epoch: 675 | Loss: 0.014931652694940567 | Val. Loss: 0.058163732290267944\n",
            "Epoch: 676 | Loss: 0.06820770353078842 | Val. Loss: 0.05769915133714676\n",
            "Epoch: 677 | Loss: 0.08515093475580215 | Val. Loss: 0.057514943182468414\n",
            "Epoch: 678 | Loss: 0.007899923250079155 | Val. Loss: 0.05726684257388115\n",
            "Epoch: 679 | Loss: 0.06954037398099899 | Val. Loss: 0.05740199610590935\n",
            "Epoch: 680 | Loss: 0.011128867976367474 | Val. Loss: 0.05736599490046501\n",
            "Epoch: 681 | Loss: 0.018293840810656548 | Val. Loss: 0.057262122631073\n",
            "Epoch: 682 | Loss: 0.1412181258201599 | Val. Loss: 0.05734484642744064\n",
            "Epoch: 683 | Loss: 0.011017028242349625 | Val. Loss: 0.05703091621398926\n",
            "Epoch: 684 | Loss: 0.06859256327152252 | Val. Loss: 0.057238079607486725\n",
            "Epoch: 685 | Loss: 0.005141514353454113 | Val. Loss: 0.057725172489881516\n",
            "Epoch: 686 | Loss: 0.06898083537817001 | Val. Loss: 0.057308029383420944\n",
            "Epoch: 687 | Loss: 0.020743058994412422 | Val. Loss: 0.05717933177947998\n",
            "Epoch: 688 | Loss: 0.010718626901507378 | Val. Loss: 0.05763229727745056\n",
            "Epoch: 689 | Loss: 0.012098895385861397 | Val. Loss: 0.0572204664349556\n",
            "Epoch: 690 | Loss: 0.06824112683534622 | Val. Loss: 0.057196397334337234\n",
            "Epoch: 691 | Loss: 0.023253511637449265 | Val. Loss: 0.05705750733613968\n",
            "Epoch: 692 | Loss: 0.012072497978806496 | Val. Loss: 0.057082027196884155\n",
            "Epoch: 693 | Loss: 0.13678152859210968 | Val. Loss: 0.056940414011478424\n",
            "Epoch: 694 | Loss: 0.01216113194823265 | Val. Loss: 0.056877173483371735\n",
            "Epoch: 695 | Loss: 0.009210889227688313 | Val. Loss: 0.057231854647397995\n",
            "Epoch: 696 | Loss: 0.06710857152938843 | Val. Loss: 0.057116348296403885\n",
            "Epoch: 697 | Loss: 0.016562089323997498 | Val. Loss: 0.05703546479344368\n",
            "Epoch: 698 | Loss: 0.07505006343126297 | Val. Loss: 0.05696604400873184\n",
            "Epoch: 699 | Loss: 0.009566627442836761 | Val. Loss: 0.056837063282728195\n",
            "Epoch: 700 | Loss: 0.06783340871334076 | Val. Loss: 0.056997936218976974\n",
            "Epoch: 701 | Loss: 0.020899882540106773 | Val. Loss: 0.05695529654622078\n",
            "Epoch: 702 | Loss: 0.012900707311928272 | Val. Loss: 0.05676727741956711\n",
            "Epoch: 703 | Loss: 0.016641821712255478 | Val. Loss: 0.056960541754961014\n",
            "Epoch: 704 | Loss: 0.02255716547369957 | Val. Loss: 0.056500256061553955\n",
            "Epoch: 705 | Loss: 0.011414111591875553 | Val. Loss: 0.056374989449977875\n",
            "Epoch: 706 | Loss: 0.024501608684659004 | Val. Loss: 0.0567663200199604\n",
            "Epoch: 707 | Loss: 0.008798370137810707 | Val. Loss: 0.0564144067466259\n",
            "Epoch: 708 | Loss: 0.07069501280784607 | Val. Loss: 0.05648757144808769\n",
            "Epoch: 709 | Loss: 0.013203483074903488 | Val. Loss: 0.057133644819259644\n",
            "Epoch: 710 | Loss: 0.07182079553604126 | Val. Loss: 0.05698153376579285\n",
            "Epoch: 711 | Loss: 0.07068958878517151 | Val. Loss: 0.05643998458981514\n",
            "Epoch: 712 | Loss: 0.012696174904704094 | Val. Loss: 0.05661965161561966\n",
            "Epoch: 713 | Loss: 0.07957407832145691 | Val. Loss: 0.056337010115385056\n",
            "Epoch: 714 | Loss: 0.07078905403614044 | Val. Loss: 0.05669914558529854\n",
            "Epoch: 715 | Loss: 0.06429096311330795 | Val. Loss: 0.056444160640239716\n",
            "Epoch: 716 | Loss: 0.06982938945293427 | Val. Loss: 0.056276220828294754\n",
            "Epoch: 717 | Loss: 0.08025650680065155 | Val. Loss: 0.05708061903715134\n",
            "Epoch: 718 | Loss: 0.02769297920167446 | Val. Loss: 0.05663622170686722\n",
            "Epoch: 719 | Loss: 0.06917080283164978 | Val. Loss: 0.05619727447628975\n",
            "Epoch: 720 | Loss: 0.06846173107624054 | Val. Loss: 0.056370921432971954\n",
            "Epoch: 721 | Loss: 0.006980105768889189 | Val. Loss: 0.056348592042922974\n",
            "Epoch: 722 | Loss: 0.009467164054512978 | Val. Loss: 0.05628643557429314\n",
            "Epoch: 723 | Loss: 0.020840276032686234 | Val. Loss: 0.056274354457855225\n",
            "Epoch: 724 | Loss: 0.07852958887815475 | Val. Loss: 0.056419093161821365\n",
            "Epoch: 725 | Loss: 0.08727345615625381 | Val. Loss: 0.05628889054059982\n",
            "Epoch: 726 | Loss: 0.016351746395230293 | Val. Loss: 0.05637568235397339\n",
            "Epoch: 727 | Loss: 0.12383249402046204 | Val. Loss: 0.05627992004156113\n",
            "Epoch: 728 | Loss: 0.06824211776256561 | Val. Loss: 0.05644218996167183\n",
            "Epoch: 729 | Loss: 0.006699045654386282 | Val. Loss: 0.05624474212527275\n",
            "Epoch: 730 | Loss: 0.08301036804914474 | Val. Loss: 0.056172896176576614\n",
            "Epoch: 731 | Loss: 0.009145474061369896 | Val. Loss: 0.056103624403476715\n",
            "Epoch: 732 | Loss: 0.017412006855010986 | Val. Loss: 0.05605930835008621\n",
            "Epoch: 733 | Loss: 0.018595166504383087 | Val. Loss: 0.056124646216630936\n",
            "Epoch: 734 | Loss: 0.0059826551005244255 | Val. Loss: 0.05620238184928894\n",
            "Epoch: 735 | Loss: 0.005359926261007786 | Val. Loss: 0.056207068264484406\n",
            "Epoch: 736 | Loss: 0.06962616741657257 | Val. Loss: 0.056096453219652176\n",
            "Epoch: 737 | Loss: 0.005003492813557386 | Val. Loss: 0.05580655857920647\n",
            "Epoch: 738 | Loss: 0.06386513262987137 | Val. Loss: 0.05603431910276413\n",
            "Epoch: 739 | Loss: 0.066350057721138 | Val. Loss: 0.05576552823185921\n",
            "Epoch: 740 | Loss: 0.008219525218009949 | Val. Loss: 0.055900830775499344\n",
            "Epoch: 741 | Loss: 0.07819412648677826 | Val. Loss: 0.05578543618321419\n",
            "Epoch: 742 | Loss: 0.00732846837490797 | Val. Loss: 0.055869825184345245\n",
            "Epoch: 743 | Loss: 0.1287086009979248 | Val. Loss: 0.05587119981646538\n",
            "Epoch: 744 | Loss: 0.014835985377430916 | Val. Loss: 0.0558159239590168\n",
            "Epoch: 745 | Loss: 0.0675719678401947 | Val. Loss: 0.0557795874774456\n",
            "Epoch: 746 | Loss: 0.1219649389386177 | Val. Loss: 0.05573565512895584\n",
            "Epoch: 747 | Loss: 0.010501986369490623 | Val. Loss: 0.05590537190437317\n",
            "Epoch: 748 | Loss: 0.13692957162857056 | Val. Loss: 0.05582959204912186\n",
            "Epoch: 749 | Loss: 0.06816518306732178 | Val. Loss: 0.05579662322998047\n",
            "Epoch: 750 | Loss: 0.07844933122396469 | Val. Loss: 0.055840034037828445\n",
            "Epoch: 751 | Loss: 0.07435761392116547 | Val. Loss: 0.05567995458841324\n",
            "Epoch: 752 | Loss: 0.021865425631403923 | Val. Loss: 0.055666279047727585\n",
            "Epoch: 753 | Loss: 0.010379069484770298 | Val. Loss: 0.05566364526748657\n",
            "Epoch: 754 | Loss: 0.011395396664738655 | Val. Loss: 0.05560100078582764\n",
            "Epoch: 755 | Loss: 0.010233866050839424 | Val. Loss: 0.05576658248901367\n",
            "Epoch: 756 | Loss: 0.007687430363148451 | Val. Loss: 0.055576421320438385\n",
            "Epoch: 757 | Loss: 0.01924404688179493 | Val. Loss: 0.05568210408091545\n",
            "Epoch: 758 | Loss: 0.016707338392734528 | Val. Loss: 0.05553695559501648\n",
            "Epoch: 759 | Loss: 0.0749257430434227 | Val. Loss: 0.055596984922885895\n",
            "Epoch: 760 | Loss: 0.0255179014056921 | Val. Loss: 0.055652279406785965\n",
            "Epoch: 761 | Loss: 0.06969854235649109 | Val. Loss: 0.05561521649360657\n",
            "Epoch: 762 | Loss: 0.08172839879989624 | Val. Loss: 0.055734388530254364\n",
            "Epoch: 763 | Loss: 0.010227319784462452 | Val. Loss: 0.05533995106816292\n",
            "Epoch: 764 | Loss: 0.06570463627576828 | Val. Loss: 0.055569618940353394\n",
            "Epoch: 765 | Loss: 0.06623651832342148 | Val. Loss: 0.055754613131284714\n",
            "Epoch: 766 | Loss: 0.0086701400578022 | Val. Loss: 0.055555351078510284\n",
            "Epoch: 767 | Loss: 0.01985820010304451 | Val. Loss: 0.05562363192439079\n",
            "Epoch: 768 | Loss: 0.08160330355167389 | Val. Loss: 0.05544334650039673\n",
            "Epoch: 769 | Loss: 0.022252967581152916 | Val. Loss: 0.055429607629776\n",
            "Epoch: 770 | Loss: 0.0651620402932167 | Val. Loss: 0.05547226220369339\n",
            "Epoch: 771 | Loss: 0.10962212830781937 | Val. Loss: 0.05551580339670181\n",
            "Epoch: 772 | Loss: 0.06637537479400635 | Val. Loss: 0.055341340601444244\n",
            "Epoch: 773 | Loss: 0.007890819571912289 | Val. Loss: 0.05532824993133545\n",
            "Epoch: 774 | Loss: 0.06777961552143097 | Val. Loss: 0.05542231351137161\n",
            "Epoch: 775 | Loss: 0.06858029961585999 | Val. Loss: 0.055262885987758636\n",
            "Epoch: 776 | Loss: 0.008364621549844742 | Val. Loss: 0.055375147610902786\n",
            "Epoch: 777 | Loss: 0.0071872235275805 | Val. Loss: 0.05524503067135811\n",
            "Epoch: 778 | Loss: 0.027961326763033867 | Val. Loss: 0.05522172525525093\n",
            "Epoch: 779 | Loss: 0.020439887419342995 | Val. Loss: 0.05530233308672905\n",
            "Epoch: 780 | Loss: 0.009543340653181076 | Val. Loss: 0.055163122713565826\n",
            "Epoch: 781 | Loss: 0.06579532474279404 | Val. Loss: 0.05534081906080246\n",
            "Epoch: 782 | Loss: 0.06587720662355423 | Val. Loss: 0.05528423935174942\n",
            "Epoch: 783 | Loss: 0.008536274544894695 | Val. Loss: 0.055297672748565674\n",
            "Epoch: 784 | Loss: 0.00859121698886156 | Val. Loss: 0.05510075017809868\n",
            "Epoch: 785 | Loss: 0.006093975622206926 | Val. Loss: 0.055105239152908325\n",
            "Epoch: 786 | Loss: 0.009859316051006317 | Val. Loss: 0.055260490626096725\n",
            "Epoch: 787 | Loss: 0.00892916601151228 | Val. Loss: 0.05508659407496452\n",
            "Epoch: 788 | Loss: 0.007338879629969597 | Val. Loss: 0.055307209491729736\n",
            "Epoch: 789 | Loss: 0.06561429798603058 | Val. Loss: 0.055189382284879684\n",
            "Epoch: 790 | Loss: 0.016646385192871094 | Val. Loss: 0.05512334778904915\n",
            "Epoch: 791 | Loss: 0.006237594410777092 | Val. Loss: 0.05502326041460037\n",
            "Epoch: 792 | Loss: 0.01994188316166401 | Val. Loss: 0.055050719529390335\n",
            "Epoch: 793 | Loss: 0.023050736635923386 | Val. Loss: 0.05511251837015152\n",
            "Epoch: 794 | Loss: 0.14090189337730408 | Val. Loss: 0.054986946284770966\n",
            "Epoch: 795 | Loss: 0.005990735720843077 | Val. Loss: 0.05522648245096207\n",
            "Epoch: 796 | Loss: 0.010137741453945637 | Val. Loss: 0.055130474269390106\n",
            "Epoch: 797 | Loss: 0.02416110970079899 | Val. Loss: 0.055129457265138626\n",
            "Epoch: 798 | Loss: 0.12483787536621094 | Val. Loss: 0.05505805090069771\n",
            "Epoch: 799 | Loss: 0.008201541379094124 | Val. Loss: 0.054947756230831146\n",
            "Epoch: 800 | Loss: 0.007739550434052944 | Val. Loss: 0.05500613898038864\n",
            "Epoch: 801 | Loss: 0.08702784776687622 | Val. Loss: 0.0549328438937664\n",
            "Epoch: 802 | Loss: 0.13524913787841797 | Val. Loss: 0.054923880845308304\n",
            "Epoch: 803 | Loss: 0.06889928877353668 | Val. Loss: 0.0549672469496727\n",
            "Epoch: 804 | Loss: 0.018049759790301323 | Val. Loss: 0.054984886199235916\n",
            "Epoch: 805 | Loss: 0.06845297664403915 | Val. Loss: 0.055032502859830856\n",
            "Epoch: 806 | Loss: 0.07737500220537186 | Val. Loss: 0.054922543466091156\n",
            "Epoch: 807 | Loss: 0.06690044701099396 | Val. Loss: 0.054920315742492676\n",
            "Epoch: 808 | Loss: 0.02828347310423851 | Val. Loss: 0.05488693714141846\n",
            "Epoch: 809 | Loss: 0.006203590892255306 | Val. Loss: 0.05503811687231064\n",
            "Epoch: 810 | Loss: 0.06492894142866135 | Val. Loss: 0.054772816598415375\n",
            "Epoch: 811 | Loss: 0.08866280317306519 | Val. Loss: 0.05480394884943962\n",
            "Epoch: 812 | Loss: 0.0076505765318870544 | Val. Loss: 0.05495018512010574\n",
            "Epoch: 813 | Loss: 0.06495288759469986 | Val. Loss: 0.05473707988858223\n",
            "Epoch: 814 | Loss: 0.14087554812431335 | Val. Loss: 0.054677627980709076\n",
            "Epoch: 815 | Loss: 0.0985281690955162 | Val. Loss: 0.05483417585492134\n",
            "Epoch: 816 | Loss: 0.07986985146999359 | Val. Loss: 0.05475528910756111\n",
            "Epoch: 817 | Loss: 0.0065696160309016705 | Val. Loss: 0.054850876331329346\n",
            "Epoch: 818 | Loss: 0.01028327364474535 | Val. Loss: 0.054639726877212524\n",
            "Epoch: 819 | Loss: 0.07644329220056534 | Val. Loss: 0.0547218844294548\n",
            "Epoch: 820 | Loss: 0.06609945744276047 | Val. Loss: 0.05487518385052681\n",
            "Epoch: 821 | Loss: 0.016278261318802834 | Val. Loss: 0.05468893051147461\n",
            "Epoch: 822 | Loss: 0.006430860608816147 | Val. Loss: 0.054753322154283524\n",
            "Epoch: 823 | Loss: 0.06779312342405319 | Val. Loss: 0.05475873500108719\n",
            "Epoch: 824 | Loss: 0.00546678900718689 | Val. Loss: 0.05461980029940605\n",
            "Epoch: 825 | Loss: 0.008920520544052124 | Val. Loss: 0.05468740314245224\n",
            "Epoch: 826 | Loss: 0.06900620460510254 | Val. Loss: 0.05458851903676987\n",
            "Epoch: 827 | Loss: 0.06656602025032043 | Val. Loss: 0.054694246500730515\n",
            "Epoch: 828 | Loss: 0.06468846648931503 | Val. Loss: 0.05458597466349602\n",
            "Epoch: 829 | Loss: 0.01574752852320671 | Val. Loss: 0.05458275228738785\n",
            "Epoch: 830 | Loss: 0.07924023270606995 | Val. Loss: 0.05461162328720093\n",
            "Epoch: 831 | Loss: 0.008218297734856606 | Val. Loss: 0.05469809100031853\n",
            "Epoch: 832 | Loss: 0.06653233617544174 | Val. Loss: 0.054591305553913116\n",
            "Epoch: 833 | Loss: 0.006922691594809294 | Val. Loss: 0.05446057766675949\n",
            "Epoch: 834 | Loss: 0.007447150535881519 | Val. Loss: 0.05537070706486702\n",
            "Epoch: 835 | Loss: 0.00787157192826271 | Val. Loss: 0.055697668343782425\n",
            "Epoch: 836 | Loss: 0.005938504356890917 | Val. Loss: 0.0556693859398365\n",
            "Epoch: 837 | Loss: 0.006667668465524912 | Val. Loss: 0.055312883108854294\n",
            "Epoch: 838 | Loss: 0.01933872513473034 | Val. Loss: 0.0551990382373333\n",
            "Epoch: 839 | Loss: 0.07606951892375946 | Val. Loss: 0.05498902127146721\n",
            "Epoch: 840 | Loss: 0.06631659716367722 | Val. Loss: 0.05478522926568985\n",
            "Epoch: 841 | Loss: 0.010631928220391273 | Val. Loss: 0.054782282561063766\n",
            "Epoch: 842 | Loss: 0.009196101687848568 | Val. Loss: 0.05472593382000923\n",
            "Epoch: 843 | Loss: 0.06958421319723129 | Val. Loss: 0.054639674723148346\n",
            "Epoch: 844 | Loss: 0.008481037802994251 | Val. Loss: 0.05452001839876175\n",
            "Epoch: 845 | Loss: 0.0069593396037817 | Val. Loss: 0.054460614919662476\n",
            "Epoch: 846 | Loss: 0.06830902397632599 | Val. Loss: 0.054413389414548874\n",
            "Epoch: 847 | Loss: 0.0067726606503129005 | Val. Loss: 0.0543668270111084\n",
            "Epoch: 848 | Loss: 0.00814804621040821 | Val. Loss: 0.054325975477695465\n",
            "Epoch: 849 | Loss: 0.009116075001657009 | Val. Loss: 0.054155297577381134\n",
            "Epoch: 850 | Loss: 0.006751557812094688 | Val. Loss: 0.05445112660527229\n",
            "Epoch: 851 | Loss: 0.006742238532751799 | Val. Loss: 0.054296232759952545\n",
            "Epoch: 852 | Loss: 0.013431908562779427 | Val. Loss: 0.054088909178972244\n",
            "Epoch: 853 | Loss: 0.009172752499580383 | Val. Loss: 0.05402740091085434\n",
            "Epoch: 854 | Loss: 0.008997464552521706 | Val. Loss: 0.0541127510368824\n",
            "Epoch: 855 | Loss: 0.008779384195804596 | Val. Loss: 0.054143816232681274\n",
            "Epoch: 856 | Loss: 0.007495100609958172 | Val. Loss: 0.05408143252134323\n",
            "Epoch: 857 | Loss: 0.01052266638725996 | Val. Loss: 0.054011642932891846\n",
            "Epoch: 858 | Loss: 0.008584919385612011 | Val. Loss: 0.05411667376756668\n",
            "Epoch: 859 | Loss: 0.008186064660549164 | Val. Loss: 0.054106395691633224\n",
            "Epoch: 860 | Loss: 0.0676063522696495 | Val. Loss: 0.05398547649383545\n",
            "Epoch: 861 | Loss: 0.0682399570941925 | Val. Loss: 0.05388333648443222\n",
            "Epoch: 862 | Loss: 0.0672043114900589 | Val. Loss: 0.053892362862825394\n",
            "Epoch: 863 | Loss: 0.06437099725008011 | Val. Loss: 0.05388291925191879\n",
            "Epoch: 864 | Loss: 0.011731034144759178 | Val. Loss: 0.05384347587823868\n",
            "Epoch: 865 | Loss: 0.007707756478339434 | Val. Loss: 0.05386187508702278\n",
            "Epoch: 866 | Loss: 0.008784903213381767 | Val. Loss: 0.053874094039201736\n",
            "Epoch: 867 | Loss: 0.00798968318849802 | Val. Loss: 0.0538363941013813\n",
            "Epoch: 868 | Loss: 0.008227161131799221 | Val. Loss: 0.05382567644119263\n",
            "Epoch: 869 | Loss: 0.008976360782980919 | Val. Loss: 0.053811199963092804\n",
            "Epoch: 870 | Loss: 0.125941663980484 | Val. Loss: 0.053794361650943756\n",
            "Epoch: 871 | Loss: 0.00865551270544529 | Val. Loss: 0.053845860064029694\n",
            "Epoch: 872 | Loss: 0.06667999178171158 | Val. Loss: 0.05380455404520035\n",
            "Epoch: 873 | Loss: 0.008038371801376343 | Val. Loss: 0.053769152611494064\n",
            "Epoch: 874 | Loss: 0.009348271414637566 | Val. Loss: 0.053776562213897705\n",
            "Epoch: 875 | Loss: 0.005725970957428217 | Val. Loss: 0.053735338151454926\n",
            "Epoch: 876 | Loss: 0.06476188451051712 | Val. Loss: 0.05374966934323311\n",
            "Epoch: 877 | Loss: 0.012274757958948612 | Val. Loss: 0.053749363869428635\n",
            "Epoch: 878 | Loss: 0.007098574656993151 | Val. Loss: 0.053695790469646454\n",
            "Epoch: 879 | Loss: 0.007465698290616274 | Val. Loss: 0.05365658551454544\n",
            "Epoch: 880 | Loss: 0.009612621739506721 | Val. Loss: 0.053711842745542526\n",
            "Epoch: 881 | Loss: 0.010973167605698109 | Val. Loss: 0.053721725940704346\n",
            "Epoch: 882 | Loss: 0.0064204661175608635 | Val. Loss: 0.05367486923933029\n",
            "Epoch: 883 | Loss: 0.06773614138364792 | Val. Loss: 0.05368221923708916\n",
            "Epoch: 884 | Loss: 0.1832377016544342 | Val. Loss: 0.05365631729364395\n",
            "Epoch: 885 | Loss: 0.006795372813940048 | Val. Loss: 0.05368102714419365\n",
            "Epoch: 886 | Loss: 0.007647782564163208 | Val. Loss: 0.053648125380277634\n",
            "Epoch: 887 | Loss: 0.06760520488023758 | Val. Loss: 0.053662579506635666\n",
            "Epoch: 888 | Loss: 0.007012742571532726 | Val. Loss: 0.053660470992326736\n",
            "Epoch: 889 | Loss: 0.06566232442855835 | Val. Loss: 0.053662292659282684\n",
            "Epoch: 890 | Loss: 0.06574396044015884 | Val. Loss: 0.053666096180677414\n",
            "Epoch: 891 | Loss: 0.0074177030473947525 | Val. Loss: 0.05360753461718559\n",
            "Epoch: 892 | Loss: 0.010180897079408169 | Val. Loss: 0.05361006408929825\n",
            "Epoch: 893 | Loss: 0.008240287192165852 | Val. Loss: 0.053605955094099045\n",
            "Epoch: 894 | Loss: 0.06702275574207306 | Val. Loss: 0.053626615554094315\n",
            "Epoch: 895 | Loss: 0.008568012155592442 | Val. Loss: 0.05361027270555496\n",
            "Epoch: 896 | Loss: 0.008395664393901825 | Val. Loss: 0.05361594632267952\n",
            "Epoch: 897 | Loss: 0.008081053383648396 | Val. Loss: 0.05361228063702583\n",
            "Epoch: 898 | Loss: 0.06591679155826569 | Val. Loss: 0.05361422151327133\n",
            "Epoch: 899 | Loss: 0.006894769612699747 | Val. Loss: 0.05359187722206116\n",
            "Epoch: 900 | Loss: 0.0055266208946704865 | Val. Loss: 0.05355226993560791\n",
            "Epoch: 901 | Loss: 0.008184142410755157 | Val. Loss: 0.053589772433042526\n",
            "Epoch: 902 | Loss: 0.06785153597593307 | Val. Loss: 0.053594570606946945\n",
            "Epoch: 903 | Loss: 0.005442268680781126 | Val. Loss: 0.05353890731930733\n",
            "Epoch: 904 | Loss: 0.009391499683260918 | Val. Loss: 0.05355747416615486\n",
            "Epoch: 905 | Loss: 0.004900929052382708 | Val. Loss: 0.05356563255190849\n",
            "Epoch: 906 | Loss: 0.00881623663008213 | Val. Loss: 0.05358016490936279\n",
            "Epoch: 907 | Loss: 0.009727612137794495 | Val. Loss: 0.053550004959106445\n",
            "Epoch: 908 | Loss: 0.0664951279759407 | Val. Loss: 0.05354198068380356\n",
            "Epoch: 909 | Loss: 0.007301300764083862 | Val. Loss: 0.05358375236392021\n",
            "Epoch: 910 | Loss: 0.008177404291927814 | Val. Loss: 0.053554851561784744\n",
            "Epoch: 911 | Loss: 0.00809362344443798 | Val. Loss: 0.05362088233232498\n",
            "Epoch: 912 | Loss: 0.06652618944644928 | Val. Loss: 0.05354300141334534\n",
            "Epoch: 913 | Loss: 0.06682863086462021 | Val. Loss: 0.053493984043598175\n",
            "Epoch: 914 | Loss: 0.06726249307394028 | Val. Loss: 0.05348508805036545\n",
            "Epoch: 915 | Loss: 0.06657785177230835 | Val. Loss: 0.0535152368247509\n",
            "Epoch: 916 | Loss: 0.004844173789024353 | Val. Loss: 0.05352725833654404\n",
            "Epoch: 917 | Loss: 0.12603038549423218 | Val. Loss: 0.05355186015367508\n",
            "Epoch: 918 | Loss: 0.00846103671938181 | Val. Loss: 0.053486716002225876\n",
            "Epoch: 919 | Loss: 0.008814158849418163 | Val. Loss: 0.05352433770895004\n",
            "Epoch: 920 | Loss: 0.1246633231639862 | Val. Loss: 0.053536295890808105\n",
            "Epoch: 921 | Loss: 0.0054689012467861176 | Val. Loss: 0.05350256711244583\n",
            "Epoch: 922 | Loss: 0.004925127141177654 | Val. Loss: 0.05348172038793564\n",
            "Epoch: 923 | Loss: 0.006265612784773111 | Val. Loss: 0.05352967977523804\n",
            "Epoch: 924 | Loss: 0.0076284147799015045 | Val. Loss: 0.0534847155213356\n",
            "Epoch: 925 | Loss: 0.005492090247571468 | Val. Loss: 0.05350232124328613\n",
            "Epoch: 926 | Loss: 0.0064194281585514545 | Val. Loss: 0.05348627641797066\n",
            "Epoch: 927 | Loss: 0.006908588111400604 | Val. Loss: 0.05345302075147629\n",
            "Epoch: 928 | Loss: 0.011581042781472206 | Val. Loss: 0.053480904549360275\n",
            "Epoch: 929 | Loss: 0.06942253559827805 | Val. Loss: 0.05348717421293259\n",
            "Epoch: 930 | Loss: 0.006635040044784546 | Val. Loss: 0.05346529558300972\n",
            "Epoch: 931 | Loss: 0.06546155363321304 | Val. Loss: 0.0535067617893219\n",
            "Epoch: 932 | Loss: 0.008250892162322998 | Val. Loss: 0.05347873643040657\n",
            "Epoch: 933 | Loss: 0.006231978535652161 | Val. Loss: 0.05344535782933235\n",
            "Epoch: 934 | Loss: 0.06527319550514221 | Val. Loss: 0.053446173667907715\n",
            "Epoch: 935 | Loss: 0.0065384660847485065 | Val. Loss: 0.05345013737678528\n",
            "Epoch: 936 | Loss: 0.005808483809232712 | Val. Loss: 0.05344918370246887\n",
            "Epoch: 937 | Loss: 0.0062087844125926495 | Val. Loss: 0.053464166820049286\n",
            "Epoch: 938 | Loss: 0.06482274830341339 | Val. Loss: 0.05343799665570259\n",
            "Epoch: 939 | Loss: 0.005364288575947285 | Val. Loss: 0.05342795327305794\n",
            "Epoch: 940 | Loss: 0.0058756498619914055 | Val. Loss: 0.05348534509539604\n",
            "Epoch: 941 | Loss: 0.006546147167682648 | Val. Loss: 0.05341426283121109\n",
            "Epoch: 942 | Loss: 0.008674535900354385 | Val. Loss: 0.0534026101231575\n",
            "Epoch: 943 | Loss: 0.004367303103208542 | Val. Loss: 0.05339992791414261\n",
            "Epoch: 944 | Loss: 0.008998124860227108 | Val. Loss: 0.05343707278370857\n",
            "Epoch: 945 | Loss: 0.0072659594006836414 | Val. Loss: 0.05342555791139603\n",
            "Epoch: 946 | Loss: 0.005256460513919592 | Val. Loss: 0.053421009331941605\n",
            "Epoch: 947 | Loss: 0.007371025625616312 | Val. Loss: 0.05340711027383804\n",
            "Epoch: 948 | Loss: 0.06747019290924072 | Val. Loss: 0.0534176230430603\n",
            "Epoch: 949 | Loss: 0.005781938321888447 | Val. Loss: 0.05343601852655411\n",
            "Epoch: 950 | Loss: 0.006268560886383057 | Val. Loss: 0.05342947691679001\n",
            "Epoch: 951 | Loss: 0.12244721502065659 | Val. Loss: 0.05342612788081169\n",
            "Epoch: 952 | Loss: 0.06507203727960587 | Val. Loss: 0.053410954773426056\n",
            "Epoch: 953 | Loss: 0.06667455285787582 | Val. Loss: 0.053418345749378204\n",
            "Epoch: 954 | Loss: 0.007014847826212645 | Val. Loss: 0.053415972739458084\n",
            "Epoch: 955 | Loss: 0.06474439799785614 | Val. Loss: 0.053390584886074066\n",
            "Epoch: 956 | Loss: 0.006517565809190273 | Val. Loss: 0.05340266972780228\n",
            "Epoch: 957 | Loss: 0.06616964936256409 | Val. Loss: 0.05340241268277168\n",
            "Epoch: 958 | Loss: 0.006173169706016779 | Val. Loss: 0.05341031029820442\n",
            "Epoch: 959 | Loss: 0.06742434948682785 | Val. Loss: 0.053403664380311966\n",
            "Epoch: 960 | Loss: 0.008875111117959023 | Val. Loss: 0.053385328501462936\n",
            "Epoch: 961 | Loss: 0.007229033857584 | Val. Loss: 0.05337626859545708\n",
            "Epoch: 962 | Loss: 0.006471162661910057 | Val. Loss: 0.05337800830602646\n",
            "Epoch: 963 | Loss: 0.006620823405683041 | Val. Loss: 0.053377777338027954\n",
            "Epoch: 964 | Loss: 0.00711853289976716 | Val. Loss: 0.05339938402175903\n",
            "Epoch: 965 | Loss: 0.007924715988337994 | Val. Loss: 0.053385473787784576\n",
            "Epoch: 966 | Loss: 0.0676349475979805 | Val. Loss: 0.053381890058517456\n",
            "Epoch: 967 | Loss: 0.006657830439507961 | Val. Loss: 0.05335608124732971\n",
            "Epoch: 968 | Loss: 0.06688109040260315 | Val. Loss: 0.053380776196718216\n",
            "Epoch: 969 | Loss: 0.06672852486371994 | Val. Loss: 0.053384244441986084\n",
            "Epoch: 970 | Loss: 0.06692256778478622 | Val. Loss: 0.053378451615571976\n",
            "Epoch: 971 | Loss: 0.12747538089752197 | Val. Loss: 0.05337321013212204\n",
            "Epoch: 972 | Loss: 0.006161705125123262 | Val. Loss: 0.05334203317761421\n",
            "Epoch: 973 | Loss: 0.007371603045612574 | Val. Loss: 0.053362034261226654\n",
            "Epoch: 974 | Loss: 0.06556929647922516 | Val. Loss: 0.053372956812381744\n",
            "Epoch: 975 | Loss: 0.008578730747103691 | Val. Loss: 0.0533539354801178\n",
            "Epoch: 976 | Loss: 0.0679631307721138 | Val. Loss: 0.05335509032011032\n",
            "Epoch: 977 | Loss: 0.06653247773647308 | Val. Loss: 0.05336320400238037\n",
            "Epoch: 978 | Loss: 0.007610108703374863 | Val. Loss: 0.05334854871034622\n",
            "Epoch: 979 | Loss: 0.12632091343402863 | Val. Loss: 0.053363870829343796\n",
            "Epoch: 980 | Loss: 0.006999236065894365 | Val. Loss: 0.05334249138832092\n",
            "Epoch: 981 | Loss: 0.0069134896621108055 | Val. Loss: 0.05335287004709244\n",
            "Epoch: 982 | Loss: 0.005026336293667555 | Val. Loss: 0.05334901064634323\n",
            "Epoch: 983 | Loss: 0.006410789676010609 | Val. Loss: 0.053350090980529785\n",
            "Epoch: 984 | Loss: 0.06793089956045151 | Val. Loss: 0.05332932621240616\n",
            "Epoch: 985 | Loss: 0.06666102260351181 | Val. Loss: 0.053348250687122345\n",
            "Epoch: 986 | Loss: 0.004789988975971937 | Val. Loss: 0.05333445593714714\n",
            "Epoch: 987 | Loss: 0.0058998544700443745 | Val. Loss: 0.053334303200244904\n",
            "Epoch: 988 | Loss: 0.00792023167014122 | Val. Loss: 0.05334426835179329\n",
            "Epoch: 989 | Loss: 0.007709890138357878 | Val. Loss: 0.05333782359957695\n",
            "Epoch: 990 | Loss: 0.18620257079601288 | Val. Loss: 0.053372662514448166\n",
            "Epoch: 991 | Loss: 0.005900492426007986 | Val. Loss: 0.053337983787059784\n",
            "Epoch: 992 | Loss: 0.06599884480237961 | Val. Loss: 0.05335720628499985\n",
            "Epoch: 993 | Loss: 0.006546367891132832 | Val. Loss: 0.05331878736615181\n",
            "Epoch: 994 | Loss: 0.008887795731425285 | Val. Loss: 0.05332893133163452\n",
            "Epoch: 995 | Loss: 0.007536616176366806 | Val. Loss: 0.05331597477197647\n",
            "Epoch: 996 | Loss: 0.008028160780668259 | Val. Loss: 0.053309183567762375\n",
            "Epoch: 997 | Loss: 0.06542685627937317 | Val. Loss: 0.05332641676068306\n",
            "Epoch: 998 | Loss: 0.008174649439752102 | Val. Loss: 0.05331070348620415\n",
            "Epoch: 999 | Loss: 0.06505083292722702 | Val. Loss: 0.053323276340961456\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Loss')"
            ]
          },
          "metadata": {},
          "execution_count": 91
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testando o modelo\n",
        "\n",
        "# Converte os dados de teste em arrays numpy\n",
        "x_teste_np = data_test.iloc[:, :-1].to_numpy()\n",
        "d_teste_np = data_test.iloc[:, -1].to_numpy()\n",
        "\n",
        "# Aplica a normalização no conjunto de teste\n",
        "x_teste_np_norm = (x_teste_np - media_treino) / desvio_padrao_treino\n",
        "\n",
        "# Converte os arrays numpy em tensores PyTorch\n",
        "x_teste_tensor = torch.tensor(x_teste_np_norm, dtype=torch.float32).to(device=device)\n",
        "d_teste_tensor = torch.tensor(d_teste_np, dtype=torch.long).to(device=device)\n",
        "\n",
        "# Testa o modelo com os dados de teste\n",
        "y_teste_tensor = model(x_teste_tensor)\n",
        "y_teste_np = y_teste_tensor.cpu().detach().numpy()\n",
        "\n",
        "predicoes = np.argmax(y_teste_np, axis=1)\n",
        "acuracia = np.mean(predicoes == d_teste_np)\n",
        "\n",
        "# Taxa de erros\n",
        "\n",
        "Taxa_de_erro = (1 - acuracia) * 100\n",
        "\n",
        "print(f\"Acurácia: {acuracia*100:.2f}%\")\n",
        "print(f\"Taxa de erro: {Taxa_de_erro:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-3zun2OMPZd",
        "outputId": "0bd255d9-cc2a-4066-cdb2-49a742ab2516"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia: 98.67%\n",
            "Taxa de erro: 1.33%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "pB6QkHw9PF4Z",
        "outputId": "4b831cc2-ad7b-4f87-a475-ebf8d2607385"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        0    1    2      3      4    5    6      7    8    9   10   11   12  \\\n",
              "0    67.0  1.0  0.0  120.0  229.0  0.0  0.0  129.0  1.0  2.6  1.0  2.0  3.0   \n",
              "1    63.0  1.0  3.0  145.0  233.0  1.0  0.0  150.0  0.0  2.3  0.0  0.0  1.0   \n",
              "2    63.0  0.0  0.0  124.0  197.0  0.0  1.0  136.0  1.0  0.0  1.0  0.0  2.0   \n",
              "3    52.0  1.0  0.0  112.0  230.0  0.0  1.0  160.0  0.0  0.0  2.0  1.0  2.0   \n",
              "4    58.0  0.0  0.0  130.0  197.0  0.0  1.0  131.0  0.0  0.6  1.0  0.0  2.0   \n",
              "..    ...  ...  ...    ...    ...  ...  ...    ...  ...  ...  ...  ...  ...   \n",
              "220  59.0  1.0  1.0  140.0  221.0  0.0  1.0  164.0  1.0  0.0  2.0  0.0  2.0   \n",
              "221  60.0  1.0  0.0  125.0  258.0  0.0  0.0  141.0  1.0  2.8  1.0  1.0  3.0   \n",
              "222  47.0  1.0  0.0  110.0  275.0  0.0  0.0  118.0  1.0  1.0  1.0  1.0  2.0   \n",
              "223  50.0  0.0  0.0  110.0  254.0  0.0  0.0  159.0  0.0  0.0  2.0  0.0  2.0   \n",
              "224  54.0  1.0  0.0  120.0  188.0  0.0  1.0  113.0  0.0  1.4  1.0  1.0  3.0   \n",
              "\n",
              "      13  \n",
              "0    0.0  \n",
              "1    1.0  \n",
              "2    0.0  \n",
              "3    0.0  \n",
              "4    1.0  \n",
              "..   ...  \n",
              "220  1.0  \n",
              "221  0.0  \n",
              "222  0.0  \n",
              "223  1.0  \n",
              "224  0.0  \n",
              "\n",
              "[225 rows x 14 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c83aca1f-60f4-40fe-8ecb-9004fff88085\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>67.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>129.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>63.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>145.0</td>\n",
              "      <td>233.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>63.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>124.0</td>\n",
              "      <td>197.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>136.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>52.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>230.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>160.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>58.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>197.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>131.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220</th>\n",
              "      <td>59.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>140.0</td>\n",
              "      <td>221.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>164.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>221</th>\n",
              "      <td>60.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>125.0</td>\n",
              "      <td>258.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>141.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>222</th>\n",
              "      <td>47.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>275.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>118.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223</th>\n",
              "      <td>50.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>254.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>159.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>224</th>\n",
              "      <td>54.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>188.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>113.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>225 rows × 14 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c83aca1f-60f4-40fe-8ecb-9004fff88085')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c83aca1f-60f4-40fe-8ecb-9004fff88085 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c83aca1f-60f4-40fe-8ecb-9004fff88085');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-4e4ec518-1de8-483c-809d-7a90660e7665\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4e4ec518-1de8-483c-809d-7a90660e7665')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-4e4ec518-1de8-483c-809d-7a90660e7665 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_acb8440c-5552-449f-925e-3bb3679414ea\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('data_test')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_acb8440c-5552-449f-925e-3bb3679414ea button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('data_test');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data_test",
              "summary": "{\n  \"name\": \"data_test\",\n  \"rows\": 225,\n  \"fields\": [\n    {\n      \"column\": \"0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8.499474773688751,\n        \"min\": 35.0,\n        \"max\": 76.0,\n        \"num_unique_values\": 37,\n        \"samples\": [\n          55.0,\n          42.0,\n          53.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4383862904819672,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0750046142388958,\n        \"min\": 0.0,\n        \"max\": 3.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          3.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 16.271752230012602,\n        \"min\": 94.0,\n        \"max\": 180.0,\n        \"num_unique_values\": 43,\n        \"samples\": [\n          152.0,\n          132.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"4\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 45.49341832739227,\n        \"min\": 131.0,\n        \"max\": 417.0,\n        \"num_unique_values\": 111,\n        \"samples\": [\n          249.0,\n          268.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"5\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.33082388735465307,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"6\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5361162513118173,\n        \"min\": 0.0,\n        \"max\": 2.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"7\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 22.799549215816615,\n        \"min\": 95.0,\n        \"max\": 194.0,\n        \"num_unique_values\": 74,\n        \"samples\": [\n          131.0,\n          142.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"8\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.47696960070847183,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"9\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.1418913577989258,\n        \"min\": 0.0,\n        \"max\": 5.6,\n        \"num_unique_values\": 35,\n        \"samples\": [\n          2.4,\n          1.4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"10\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6281428914375227,\n        \"min\": 0.0,\n        \"max\": 2.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"11\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.9614184296264536,\n        \"min\": 0.0,\n        \"max\": 4.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.0,\n          3.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"12\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6164735862465499,\n        \"min\": 0.0,\n        \"max\": 3.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"13\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5009910812500195,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_dAL6ph3K_d"
      },
      "source": [
        "# Exercício 2\n",
        "\n",
        "Aplique o PCA nos dados de entrada e obtenha uma matriz de dados transformados representando os 12 componentes principais. Mostre o valor da porcentagem de variância explicada acumulada à medida que você considera um maior número de componentes principais.\n",
        "\n",
        "## Resolução"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kz4BFhRx3K_e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMRKZ9gP3K_g"
      },
      "source": [
        "# Exercício 3\n",
        "\n",
        "Implemente uma segunda rede neural para fazer a classificação usando o número de componentes principais necessário para incluir 90% da variância explicada. Calcule a acurácia obtida nos dados de teste.\n",
        "\n",
        "## Resolução"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TYxZCc03K_h"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQbeQopQ3K_j"
      },
      "source": [
        "# Exercício 4\n",
        "\n",
        "Repita os exercícios 1, 2 e 3, considerando a transformação dos dados usando o LDA no lugar do PCA. Use como referência o exemplo mostrado [neste Jupyter Notebook](./LDA_IRIS.ipynb)\n",
        "\n",
        "## Resolução"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbWImeSV3K_j"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}