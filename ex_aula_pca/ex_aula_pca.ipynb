{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26fxjoVADscQ"
      },
      "source": [
        "Para abrir o notebook no Google Colab, altere o domínio `github.com` para `githubtocolab.com`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sf-Xuxsf3K_T"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\">\n",
        "Para praticar programação, é importante que você erre, leia as mensagens de erro e tente corrigí-los.\n",
        "    \n",
        "Dessa forma, no Google Colab, é importante que você DESATIVE OS RECURSOS DE AUTOCOMPLETAR:\n",
        "\n",
        "- Menu Ferramentas -> Configurações\n",
        "- Na janela que é aberta:\n",
        "  - Seção Editor -> Desativar \"Mostrar sugestões de preenchimento de código com base no contexto\"\n",
        "  - Seção Assistência de IA -> Desabilitar itens\n",
        "\n",
        "Na versão em inglês:\n",
        "\n",
        "- Menu Tools -> Settings\n",
        "- Na janela que é aberta:\n",
        "  - Seção Editor -> Desativar \"Show context-powered code completions\"\n",
        "  - Seção AI Assistance -> Desabilitar itens\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c04thLl8Dzh4"
      },
      "source": [
        "# PSI5892 - Aula de Exercícios\n",
        "\n",
        "# Análise de componentes principais (PCA) e análise de discriminantes lineares (LDA)\n",
        "\n",
        "Neste exercício, você vai trabalhar com uma aplicação de redes neurais para  para a área de saúde. O objetivo é obter um modelo de predição de uma doença cardíaca baseado em dados com características extraídas de exames clínicos laboratoriais.\n",
        "\n",
        "## Dados disponibilizados\n",
        "\n",
        "Os dados para treinamento e teste do modelo estão disponíveis no formato CSV, em um arquivo zip disponível [neste link](./data.zip).\n",
        "\n",
        "Após extrair os arquivos, utiliza a biblioteca Pandas para carregar os `DataFrames` `data_train` e `data_test`, como mostrado a seguir:\n",
        "\n",
        "``` python\n",
        "import pandas as pd\n",
        "\n",
        "data_train = pd.read_csv(\"data_train.csv\").drop(columns=[\"Unnamed: 0\"])\n",
        "\n",
        "data_test = pd.read_csv(\"data_test.csv\").drop(columns=[\"Unnamed: 0\"])\n",
        "```\n",
        "\n",
        "Os dados consistem de 800 exemplos de treinamento e 225 para teste, cada um contendo 13 características de entrada, representadas pelas colunas de 0 a 12 e a saída desejada binária, indicando se o paciente é portador ou não da doença, representada pela coluna 13.\n",
        "\n",
        "O objetivo é treinar uma rede neural com estes dados, avaliar o desempenho e depois comparar com o desempenho obtido usando o PCA para realizar redução de dimensionalidade. Use como referência o exemplo mostrado [neste Jupyter Notebook](./PCA_IRIS.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thFmnLOj3K_Y"
      },
      "source": [
        "# Exercício 1\n",
        "\n",
        "Implemente uma rede neural para classificar se o indivíduo é portador ou não da doença cardíaca (coluna 13) usando como entrada os dados dos exames laboratoriais (colunas 1 a 12). Calcule a acurácia obtida nos dados de teste.\n",
        "\n",
        "## Resolução"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "x8pe0-5-3K_b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregamento dos dados\n",
        "\n",
        "data_train = pd.read_csv(\"data_train.csv\").drop(columns=[\"Unnamed: 0\"])\n",
        "data_test = pd.read_csv(\"data_test.csv\").drop(columns=[\"Unnamed: 0\"])"
      ],
      "metadata": {
        "id": "qehWt4DP3oS9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "659kqytE51Za",
        "outputId": "9487c9a5-f32d-463c-a08e-88f46184ef8b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        0    1    2      3      4    5    6      7    8    9   10   11   12  \\\n",
              "0    52.0  1.0  0.0  125.0  212.0  0.0  1.0  168.0  0.0  1.0  2.0  2.0  3.0   \n",
              "1    53.0  1.0  0.0  140.0  203.0  1.0  0.0  155.0  1.0  3.1  0.0  0.0  3.0   \n",
              "2    70.0  1.0  0.0  145.0  174.0  0.0  1.0  125.0  1.0  2.6  0.0  0.0  3.0   \n",
              "3    61.0  1.0  0.0  148.0  203.0  0.0  1.0  161.0  0.0  0.0  2.0  1.0  3.0   \n",
              "4    62.0  0.0  0.0  138.0  294.0  1.0  1.0  106.0  0.0  1.9  1.0  3.0  2.0   \n",
              "..    ...  ...  ...    ...    ...  ...  ...    ...  ...  ...  ...  ...  ...   \n",
              "795  62.0  1.0  1.0  128.0  208.0  1.0  0.0  140.0  0.0  0.0  2.0  0.0  2.0   \n",
              "796  41.0  1.0  1.0  135.0  203.0  0.0  1.0  132.0  0.0  0.0  1.0  0.0  1.0   \n",
              "797  65.0  0.0  0.0  150.0  225.0  0.0  0.0  114.0  0.0  1.0  1.0  3.0  3.0   \n",
              "798  59.0  1.0  3.0  170.0  288.0  0.0  0.0  159.0  0.0  0.2  1.0  0.0  3.0   \n",
              "799  43.0  1.0  0.0  115.0  303.0  0.0  1.0  181.0  0.0  1.2  1.0  0.0  2.0   \n",
              "\n",
              "      13  \n",
              "0    0.0  \n",
              "1    0.0  \n",
              "2    0.0  \n",
              "3    0.0  \n",
              "4    0.0  \n",
              "..   ...  \n",
              "795  1.0  \n",
              "796  1.0  \n",
              "797  0.0  \n",
              "798  0.0  \n",
              "799  1.0  \n",
              "\n",
              "[800 rows x 14 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6e57771d-91a4-4f10-9293-3d49e63c3aa0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>52.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>125.0</td>\n",
              "      <td>212.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>168.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>53.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>140.0</td>\n",
              "      <td>203.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>155.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>70.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>145.0</td>\n",
              "      <td>174.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>125.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>61.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>148.0</td>\n",
              "      <td>203.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>161.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>62.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>138.0</td>\n",
              "      <td>294.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>106.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.9</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>795</th>\n",
              "      <td>62.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>128.0</td>\n",
              "      <td>208.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>140.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>796</th>\n",
              "      <td>41.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>135.0</td>\n",
              "      <td>203.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>797</th>\n",
              "      <td>65.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>225.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>798</th>\n",
              "      <td>59.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>288.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>159.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>799</th>\n",
              "      <td>43.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>115.0</td>\n",
              "      <td>303.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>181.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>800 rows × 14 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6e57771d-91a4-4f10-9293-3d49e63c3aa0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6e57771d-91a4-4f10-9293-3d49e63c3aa0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6e57771d-91a4-4f10-9293-3d49e63c3aa0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-9f201e1e-b46c-4709-85f8-bf88fdf2a52b\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9f201e1e-b46c-4709-85f8-bf88fdf2a52b')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-9f201e1e-b46c-4709-85f8-bf88fdf2a52b button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_aedf6b2f-2a13-437c-b52f-c671bcf9173f\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('data_train')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_aedf6b2f-2a13-437c-b52f-c671bcf9173f button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('data_train');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data_train",
              "summary": "{\n  \"name\": \"data_train\",\n  \"rows\": 800,\n  \"fields\": [\n    {\n      \"column\": \"0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9.232059014220622,\n        \"min\": 29.0,\n        \"max\": 77.0,\n        \"num_unique_values\": 41,\n        \"samples\": [\n          65.0,\n          50.0,\n          54.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4657949722734747,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0156588458305733,\n        \"min\": 0.0,\n        \"max\": 3.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1.0,\n          3.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 17.857721656988417,\n        \"min\": 94.0,\n        \"max\": 200.0,\n        \"num_unique_values\": 49,\n        \"samples\": [\n          128.0,\n          172.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"4\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 53.06672764469413,\n        \"min\": 126.0,\n        \"max\": 564.0,\n        \"num_unique_values\": 152,\n        \"samples\": [\n          267.0,\n          262.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"5\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.36331933429607305,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"6\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5245833915292517,\n        \"min\": 0.0,\n        \"max\": 2.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"7\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 23.076981688574847,\n        \"min\": 71.0,\n        \"max\": 202.0,\n        \"num_unique_values\": 90,\n        \"samples\": [\n          180.0,\n          152.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"8\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4718466239634246,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"9\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.1848183376013328,\n        \"min\": 0.0,\n        \"max\": 6.2,\n        \"num_unique_values\": 40,\n        \"samples\": [\n          2.8,\n          0.4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"10\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6151202859667018,\n        \"min\": 0.0,\n        \"max\": 2.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"11\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0495953985349717,\n        \"min\": 0.0,\n        \"max\": 4.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.0,\n          4.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"12\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6212295531420899,\n        \"min\": 0.0,\n        \"max\": 3.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"13\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.49991238281133826,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_total = len(data_train)\n",
        "n_treino = int(n_total * 0.8) # Divisão 80/20 (treino/validação)\n",
        "\n",
        "indices_aleatorios = np.random.permutation(n_total)\n",
        "\n",
        "indices_treino = indices_aleatorios[:n_treino]\n",
        "indices_val = indices_aleatorios[n_treino:]\n",
        "\n",
        "# DataFrames de treino e validação\n",
        "df_treino = data_train.iloc[indices_treino]\n",
        "df_val = data_train.iloc[indices_val]\n",
        "\n",
        "print(len(df_treino))\n",
        "print(len(df_val))\n",
        "\n",
        "# Dados de treino\n",
        "x_treino_np = df_treino.iloc[:, :-1].to_numpy()\n",
        "d_treino_np = df_treino.iloc[:, -1].to_numpy()\n",
        "\n",
        "# Dados de validação\n",
        "x_val_np = df_val.iloc[:, :-1].to_numpy()\n",
        "d_val_np = df_val.iloc[:, -1].to_numpy()\n",
        "\n",
        "# ============================================================== #\n",
        "# Normalizar os dados de treino e val\n",
        "\n",
        "# Calcula a média das características\n",
        "media_treino = np.mean(x_treino_np, axis=0)\n",
        "\n",
        "# Calcula o desvio padrao das características\n",
        "desvio_padrao_treino = np.std(x_treino_np, axis=0, ddof=1)\n",
        "\n",
        "# Aplica a normalização no conjunto\n",
        "x_treino_np_norm = (x_treino_np - media_treino) / desvio_padrao_treino\n",
        "x_val_np_norm = (x_val_np - media_treino) / desvio_padrao_treino\n",
        "\n",
        "# ============================================================== #\n",
        "# Converter para tensores PyTorch\n",
        "x_treino_tensor = torch.tensor(x_treino_np_norm, dtype=torch.float32)\n",
        "d_treino_tensor = torch.tensor(d_treino_np, dtype=torch.long)\n",
        "\n",
        "x_val_tensor = torch.tensor(x_val_np_norm, dtype=torch.float32)\n",
        "d_val_tensor = torch.tensor(d_val_np, dtype=torch.long)\n",
        "\n",
        "# print(f\"x_treino: {x_treino_np.shape}\")\n",
        "# print(f\"d_treino: {d_treino_np.shape}\")\n",
        "# print(f\"x_val: {x_val_np.shape}\")\n",
        "# print(f\"d_val: {d_val_np.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkTDgfH7FVDQ",
        "outputId": "9eb7a124-4519-4992-b6c9-db3f42a0000b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "640\n",
            "160\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.model = nn.Sequential(\n",
        "        nn.Linear(13, 8),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.Linear(8, 4),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.Linear(4, 4),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.Linear(4, 2),\n",
        "    )\n",
        "\n",
        "    self._init_weights()\n",
        "\n",
        "  def _init_weights(self):\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Linear):\n",
        "        nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
        "\n",
        "        # Inicializa os bias com zero\n",
        "        if m.bias is not None:\n",
        "          nn.init.constant_(m.bias, 0)\n",
        "\n",
        "  def forward(self, x):\n",
        "    output = self.model(x)\n",
        "    return output"
      ],
      "metadata": {
        "id": "IAjXWGec9nSf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Model().to(device=device)\n",
        "\n",
        "# Taxa de aprendizado\n",
        "eta = 0.001\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=eta)\n",
        "\n",
        "Nb = 64 # Tamanho do mini-batch\n",
        "Ne = 1000 # Número de épocas\n"
      ],
      "metadata": {
        "id": "0itvtGvd-HCM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_set = TensorDataset(x_treino_tensor, d_treino_tensor)\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=Nb, shuffle=True)"
      ],
      "metadata": {
        "id": "Uhve4y04HqVW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Treinamento\n",
        "losses = []\n",
        "val_losses = []\n",
        "\n",
        "x_val_tensor = x_val_tensor.to(device=device)\n",
        "d_val_tensor = d_val_tensor.to(device=device)\n",
        "\n",
        "for epoch in range(Ne):\n",
        "  for n, (X, d) in enumerate(train_loader):\n",
        "\n",
        "    X = X.to(device=device)\n",
        "    d = d.to(device=device)\n",
        "\n",
        "    # Treinamento\n",
        "    model.train()\n",
        "    model.zero_grad()\n",
        "    y = model(X)\n",
        "    loss = loss_function(y, d)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Validação\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      y_val = model(x_val_tensor)\n",
        "      val_loss = loss_function(y_val, d_val_tensor)\n",
        "\n",
        "    losses.append(loss.item())\n",
        "    val_losses.append(val_loss.item())\n",
        "\n",
        "    if epoch % 1 == 0 and n == x_treino_tensor.shape[0]//Nb - 1:\n",
        "      print(f\"Epoch: {epoch} | Loss: {loss} | Val. Loss: {val_loss}\")\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(losses)\n",
        "plt.plot(val_losses, alpha=0.8)\n",
        "plt.legend([\"Loss\", \"Val. Loss\"])\n",
        "plt.xlabel(\"Batch\")\n",
        "plt.ylabel(\"Loss\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CmkDGipS-qmR",
        "outputId": "359e0602-462c-4988-87c6-f403c5e0def1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | Loss: 1.7540206909179688 | Val. Loss: 1.5266128778457642\n",
            "Epoch: 1 | Loss: 1.5860364437103271 | Val. Loss: 1.3925654888153076\n",
            "Epoch: 2 | Loss: 1.5156325101852417 | Val. Loss: 1.2720091342926025\n",
            "Epoch: 3 | Loss: 1.2544071674346924 | Val. Loss: 1.1686512231826782\n",
            "Epoch: 4 | Loss: 1.5605430603027344 | Val. Loss: 1.0788377523422241\n",
            "Epoch: 5 | Loss: 1.1205859184265137 | Val. Loss: 1.002772331237793\n",
            "Epoch: 6 | Loss: 1.0684101581573486 | Val. Loss: 0.9403995275497437\n",
            "Epoch: 7 | Loss: 0.8697930574417114 | Val. Loss: 0.8889294862747192\n",
            "Epoch: 8 | Loss: 1.0309972763061523 | Val. Loss: 0.8491054773330688\n",
            "Epoch: 9 | Loss: 0.9097887873649597 | Val. Loss: 0.8140474557876587\n",
            "Epoch: 10 | Loss: 0.7565056681632996 | Val. Loss: 0.783333957195282\n",
            "Epoch: 11 | Loss: 0.8276523947715759 | Val. Loss: 0.7580941915512085\n",
            "Epoch: 12 | Loss: 0.671380877494812 | Val. Loss: 0.7377622127532959\n",
            "Epoch: 13 | Loss: 0.6934356093406677 | Val. Loss: 0.718569815158844\n",
            "Epoch: 14 | Loss: 0.6616615056991577 | Val. Loss: 0.7011042833328247\n",
            "Epoch: 15 | Loss: 0.6526691317558289 | Val. Loss: 0.6839116811752319\n",
            "Epoch: 16 | Loss: 0.6838136911392212 | Val. Loss: 0.6688732504844666\n",
            "Epoch: 17 | Loss: 0.7101583480834961 | Val. Loss: 0.6550500988960266\n",
            "Epoch: 18 | Loss: 0.5998415946960449 | Val. Loss: 0.6423494219779968\n",
            "Epoch: 19 | Loss: 0.5817071199417114 | Val. Loss: 0.6301478743553162\n",
            "Epoch: 20 | Loss: 0.5978450179100037 | Val. Loss: 0.618878960609436\n",
            "Epoch: 21 | Loss: 0.5407554507255554 | Val. Loss: 0.6087018847465515\n",
            "Epoch: 22 | Loss: 0.6091539263725281 | Val. Loss: 0.5983489751815796\n",
            "Epoch: 23 | Loss: 0.5575817823410034 | Val. Loss: 0.5887802243232727\n",
            "Epoch: 24 | Loss: 0.5271266102790833 | Val. Loss: 0.5797072649002075\n",
            "Epoch: 25 | Loss: 0.5244558453559875 | Val. Loss: 0.5704773664474487\n",
            "Epoch: 26 | Loss: 0.5954431295394897 | Val. Loss: 0.5624080896377563\n",
            "Epoch: 27 | Loss: 0.5237610340118408 | Val. Loss: 0.5557790398597717\n",
            "Epoch: 28 | Loss: 0.6116839051246643 | Val. Loss: 0.5494793653488159\n",
            "Epoch: 29 | Loss: 0.4883327782154083 | Val. Loss: 0.5439907908439636\n",
            "Epoch: 30 | Loss: 0.4985620677471161 | Val. Loss: 0.5391349196434021\n",
            "Epoch: 31 | Loss: 0.4089709520339966 | Val. Loss: 0.5347268581390381\n",
            "Epoch: 32 | Loss: 0.5038484334945679 | Val. Loss: 0.5305357575416565\n",
            "Epoch: 33 | Loss: 0.4516470432281494 | Val. Loss: 0.5265823602676392\n",
            "Epoch: 34 | Loss: 0.47020992636680603 | Val. Loss: 0.5227241516113281\n",
            "Epoch: 35 | Loss: 0.49922844767570496 | Val. Loss: 0.519015908241272\n",
            "Epoch: 36 | Loss: 0.5229498147964478 | Val. Loss: 0.5150032043457031\n",
            "Epoch: 37 | Loss: 0.39995720982551575 | Val. Loss: 0.5109067559242249\n",
            "Epoch: 38 | Loss: 0.47971850633621216 | Val. Loss: 0.507228434085846\n",
            "Epoch: 39 | Loss: 0.4421602785587311 | Val. Loss: 0.50333571434021\n",
            "Epoch: 40 | Loss: 0.5411511659622192 | Val. Loss: 0.49983224272727966\n",
            "Epoch: 41 | Loss: 0.38678354024887085 | Val. Loss: 0.4967208504676819\n",
            "Epoch: 42 | Loss: 0.5282005071640015 | Val. Loss: 0.49366793036460876\n",
            "Epoch: 43 | Loss: 0.4101983308792114 | Val. Loss: 0.49064183235168457\n",
            "Epoch: 44 | Loss: 0.3889826834201813 | Val. Loss: 0.4872497022151947\n",
            "Epoch: 45 | Loss: 0.44819945096969604 | Val. Loss: 0.4839366376399994\n",
            "Epoch: 46 | Loss: 0.33732157945632935 | Val. Loss: 0.480578750371933\n",
            "Epoch: 47 | Loss: 0.36890077590942383 | Val. Loss: 0.47703805565834045\n",
            "Epoch: 48 | Loss: 0.455329954624176 | Val. Loss: 0.4736867845058441\n",
            "Epoch: 49 | Loss: 0.3602558970451355 | Val. Loss: 0.4700985550880432\n",
            "Epoch: 50 | Loss: 0.2973352074623108 | Val. Loss: 0.4667332172393799\n",
            "Epoch: 51 | Loss: 0.3233965337276459 | Val. Loss: 0.4635573923587799\n",
            "Epoch: 52 | Loss: 0.376184344291687 | Val. Loss: 0.4599834084510803\n",
            "Epoch: 53 | Loss: 0.358669251203537 | Val. Loss: 0.4560544490814209\n",
            "Epoch: 54 | Loss: 0.41890040040016174 | Val. Loss: 0.45253896713256836\n",
            "Epoch: 55 | Loss: 0.3295917510986328 | Val. Loss: 0.44864219427108765\n",
            "Epoch: 56 | Loss: 0.26969146728515625 | Val. Loss: 0.444896936416626\n",
            "Epoch: 57 | Loss: 0.3305671513080597 | Val. Loss: 0.4416086673736572\n",
            "Epoch: 58 | Loss: 0.34758710861206055 | Val. Loss: 0.43840640783309937\n",
            "Epoch: 59 | Loss: 0.44982051849365234 | Val. Loss: 0.43527594208717346\n",
            "Epoch: 60 | Loss: 0.3614334762096405 | Val. Loss: 0.4320624768733978\n",
            "Epoch: 61 | Loss: 0.39879438281059265 | Val. Loss: 0.429025262594223\n",
            "Epoch: 62 | Loss: 0.31986603140830994 | Val. Loss: 0.4266112744808197\n",
            "Epoch: 63 | Loss: 0.3596061170101166 | Val. Loss: 0.42436814308166504\n",
            "Epoch: 64 | Loss: 0.3439747095108032 | Val. Loss: 0.42189979553222656\n",
            "Epoch: 65 | Loss: 0.3128995895385742 | Val. Loss: 0.41948023438453674\n",
            "Epoch: 66 | Loss: 0.3454328775405884 | Val. Loss: 0.4166427552700043\n",
            "Epoch: 67 | Loss: 0.2672566771507263 | Val. Loss: 0.4140208661556244\n",
            "Epoch: 68 | Loss: 0.2612466514110565 | Val. Loss: 0.41161125898361206\n",
            "Epoch: 69 | Loss: 0.3333037793636322 | Val. Loss: 0.40891289710998535\n",
            "Epoch: 70 | Loss: 0.3845927119255066 | Val. Loss: 0.4071231484413147\n",
            "Epoch: 71 | Loss: 0.29999423027038574 | Val. Loss: 0.4046764373779297\n",
            "Epoch: 72 | Loss: 0.28011369705200195 | Val. Loss: 0.40175193548202515\n",
            "Epoch: 73 | Loss: 0.2957591712474823 | Val. Loss: 0.3986072540283203\n",
            "Epoch: 74 | Loss: 0.23610951006412506 | Val. Loss: 0.39481624960899353\n",
            "Epoch: 75 | Loss: 0.28543180227279663 | Val. Loss: 0.3921448588371277\n",
            "Epoch: 76 | Loss: 0.2828806936740875 | Val. Loss: 0.38962167501449585\n",
            "Epoch: 77 | Loss: 0.3532700538635254 | Val. Loss: 0.3866878151893616\n",
            "Epoch: 78 | Loss: 0.31618136167526245 | Val. Loss: 0.3841739594936371\n",
            "Epoch: 79 | Loss: 0.384475439786911 | Val. Loss: 0.3821110129356384\n",
            "Epoch: 80 | Loss: 0.3260232210159302 | Val. Loss: 0.3799871802330017\n",
            "Epoch: 81 | Loss: 0.25144457817077637 | Val. Loss: 0.3783489763736725\n",
            "Epoch: 82 | Loss: 0.390703946352005 | Val. Loss: 0.37638336420059204\n",
            "Epoch: 83 | Loss: 0.3452228009700775 | Val. Loss: 0.374396413564682\n",
            "Epoch: 84 | Loss: 0.37912556529045105 | Val. Loss: 0.3727259039878845\n",
            "Epoch: 85 | Loss: 0.32049089670181274 | Val. Loss: 0.3710741698741913\n",
            "Epoch: 86 | Loss: 0.2922276258468628 | Val. Loss: 0.36930981278419495\n",
            "Epoch: 87 | Loss: 0.2890319228172302 | Val. Loss: 0.36798539757728577\n",
            "Epoch: 88 | Loss: 0.32928135991096497 | Val. Loss: 0.36625704169273376\n",
            "Epoch: 89 | Loss: 0.22983814775943756 | Val. Loss: 0.36478760838508606\n",
            "Epoch: 90 | Loss: 0.27619874477386475 | Val. Loss: 0.3630526065826416\n",
            "Epoch: 91 | Loss: 0.2512180209159851 | Val. Loss: 0.36181432008743286\n",
            "Epoch: 92 | Loss: 0.3542770743370056 | Val. Loss: 0.36102762818336487\n",
            "Epoch: 93 | Loss: 0.22870582342147827 | Val. Loss: 0.35976463556289673\n",
            "Epoch: 94 | Loss: 0.342018723487854 | Val. Loss: 0.3590853810310364\n",
            "Epoch: 95 | Loss: 0.25521329045295715 | Val. Loss: 0.3579005300998688\n",
            "Epoch: 96 | Loss: 0.3175632059574127 | Val. Loss: 0.3568212389945984\n",
            "Epoch: 97 | Loss: 0.3569643199443817 | Val. Loss: 0.35545313358306885\n",
            "Epoch: 98 | Loss: 0.2592593729496002 | Val. Loss: 0.3547227382659912\n",
            "Epoch: 99 | Loss: 0.3312404751777649 | Val. Loss: 0.35419145226478577\n",
            "Epoch: 100 | Loss: 0.2998129427433014 | Val. Loss: 0.3532342314720154\n",
            "Epoch: 101 | Loss: 0.25382199883461 | Val. Loss: 0.3520890176296234\n",
            "Epoch: 102 | Loss: 0.24390985071659088 | Val. Loss: 0.35119396448135376\n",
            "Epoch: 103 | Loss: 0.1987564116716385 | Val. Loss: 0.3502238094806671\n",
            "Epoch: 104 | Loss: 0.2629891633987427 | Val. Loss: 0.34962791204452515\n",
            "Epoch: 105 | Loss: 0.28831011056900024 | Val. Loss: 0.3490492105484009\n",
            "Epoch: 106 | Loss: 0.20096363127231598 | Val. Loss: 0.34829747676849365\n",
            "Epoch: 107 | Loss: 0.20976480841636658 | Val. Loss: 0.34735915064811707\n",
            "Epoch: 108 | Loss: 0.22408977150917053 | Val. Loss: 0.34645912051200867\n",
            "Epoch: 109 | Loss: 0.2278459072113037 | Val. Loss: 0.34636950492858887\n",
            "Epoch: 110 | Loss: 0.18468578159809113 | Val. Loss: 0.34560027718544006\n",
            "Epoch: 111 | Loss: 0.32749781012535095 | Val. Loss: 0.34591445326805115\n",
            "Epoch: 112 | Loss: 0.2844236493110657 | Val. Loss: 0.34522107243537903\n",
            "Epoch: 113 | Loss: 0.3835902214050293 | Val. Loss: 0.34437888860702515\n",
            "Epoch: 114 | Loss: 0.3918342888355255 | Val. Loss: 0.344200074672699\n",
            "Epoch: 115 | Loss: 0.3061617314815521 | Val. Loss: 0.34257978200912476\n",
            "Epoch: 116 | Loss: 0.27653899788856506 | Val. Loss: 0.3422245383262634\n",
            "Epoch: 117 | Loss: 0.23257339000701904 | Val. Loss: 0.3418326675891876\n",
            "Epoch: 118 | Loss: 0.3871719241142273 | Val. Loss: 0.34202414751052856\n",
            "Epoch: 119 | Loss: 0.21009781956672668 | Val. Loss: 0.3413810133934021\n",
            "Epoch: 120 | Loss: 0.18675418198108673 | Val. Loss: 0.3410951793193817\n",
            "Epoch: 121 | Loss: 0.21046563982963562 | Val. Loss: 0.3410143256187439\n",
            "Epoch: 122 | Loss: 0.205630823969841 | Val. Loss: 0.3408004641532898\n",
            "Epoch: 123 | Loss: 0.37864574790000916 | Val. Loss: 0.33889690041542053\n",
            "Epoch: 124 | Loss: 0.3403640389442444 | Val. Loss: 0.33904916048049927\n",
            "Epoch: 125 | Loss: 0.23983213305473328 | Val. Loss: 0.33848828077316284\n",
            "Epoch: 126 | Loss: 0.42912349104881287 | Val. Loss: 0.3384825885295868\n",
            "Epoch: 127 | Loss: 0.19774959981441498 | Val. Loss: 0.3372795283794403\n",
            "Epoch: 128 | Loss: 0.2444034069776535 | Val. Loss: 0.3360430598258972\n",
            "Epoch: 129 | Loss: 0.2563215494155884 | Val. Loss: 0.33569207787513733\n",
            "Epoch: 130 | Loss: 0.2940611243247986 | Val. Loss: 0.33545932173728943\n",
            "Epoch: 131 | Loss: 0.2528606355190277 | Val. Loss: 0.3344356417655945\n",
            "Epoch: 132 | Loss: 0.3186436891555786 | Val. Loss: 0.33491426706314087\n",
            "Epoch: 133 | Loss: 0.22170387208461761 | Val. Loss: 0.33332115411758423\n",
            "Epoch: 134 | Loss: 0.2471558004617691 | Val. Loss: 0.3323928415775299\n",
            "Epoch: 135 | Loss: 0.3476172685623169 | Val. Loss: 0.3326869606971741\n",
            "Epoch: 136 | Loss: 0.2112664133310318 | Val. Loss: 0.33288827538490295\n",
            "Epoch: 137 | Loss: 0.2905470132827759 | Val. Loss: 0.3316170275211334\n",
            "Epoch: 138 | Loss: 0.21127095818519592 | Val. Loss: 0.33057916164398193\n",
            "Epoch: 139 | Loss: 0.21485856175422668 | Val. Loss: 0.33042004704475403\n",
            "Epoch: 140 | Loss: 0.20744208991527557 | Val. Loss: 0.32848864793777466\n",
            "Epoch: 141 | Loss: 0.21625524759292603 | Val. Loss: 0.3291431963443756\n",
            "Epoch: 142 | Loss: 0.31782475113868713 | Val. Loss: 0.3286948800086975\n",
            "Epoch: 143 | Loss: 0.14649610221385956 | Val. Loss: 0.3275000751018524\n",
            "Epoch: 144 | Loss: 0.2713598310947418 | Val. Loss: 0.32680386304855347\n",
            "Epoch: 145 | Loss: 0.4326189458370209 | Val. Loss: 0.32754650712013245\n",
            "Epoch: 146 | Loss: 0.19742116332054138 | Val. Loss: 0.32624638080596924\n",
            "Epoch: 147 | Loss: 0.24028657376766205 | Val. Loss: 0.326612651348114\n",
            "Epoch: 148 | Loss: 0.2722163200378418 | Val. Loss: 0.32664114236831665\n",
            "Epoch: 149 | Loss: 0.19635972380638123 | Val. Loss: 0.3252900242805481\n",
            "Epoch: 150 | Loss: 0.35194650292396545 | Val. Loss: 0.3248607814311981\n",
            "Epoch: 151 | Loss: 0.17664894461631775 | Val. Loss: 0.3253694772720337\n",
            "Epoch: 152 | Loss: 0.22843308746814728 | Val. Loss: 0.32447248697280884\n",
            "Epoch: 153 | Loss: 0.20529955625534058 | Val. Loss: 0.3234557509422302\n",
            "Epoch: 154 | Loss: 0.27593356370925903 | Val. Loss: 0.3235551118850708\n",
            "Epoch: 155 | Loss: 0.19396765530109406 | Val. Loss: 0.3223893344402313\n",
            "Epoch: 156 | Loss: 0.19593971967697144 | Val. Loss: 0.3217766284942627\n",
            "Epoch: 157 | Loss: 0.27371469140052795 | Val. Loss: 0.32196441292762756\n",
            "Epoch: 158 | Loss: 0.1808224618434906 | Val. Loss: 0.321468323469162\n",
            "Epoch: 159 | Loss: 0.19729813933372498 | Val. Loss: 0.3202994763851166\n",
            "Epoch: 160 | Loss: 0.16428066790103912 | Val. Loss: 0.32040444016456604\n",
            "Epoch: 161 | Loss: 0.233389213681221 | Val. Loss: 0.3191085159778595\n",
            "Epoch: 162 | Loss: 0.2873584032058716 | Val. Loss: 0.31975334882736206\n",
            "Epoch: 163 | Loss: 0.18345017731189728 | Val. Loss: 0.3202126622200012\n",
            "Epoch: 164 | Loss: 0.21975795924663544 | Val. Loss: 0.3191835582256317\n",
            "Epoch: 165 | Loss: 0.31808164715766907 | Val. Loss: 0.31882062554359436\n",
            "Epoch: 166 | Loss: 0.18705111742019653 | Val. Loss: 0.31846654415130615\n",
            "Epoch: 167 | Loss: 0.19696955382823944 | Val. Loss: 0.3173823356628418\n",
            "Epoch: 168 | Loss: 0.3545582890510559 | Val. Loss: 0.3168410360813141\n",
            "Epoch: 169 | Loss: 0.3148270845413208 | Val. Loss: 0.31621700525283813\n",
            "Epoch: 170 | Loss: 0.1588132381439209 | Val. Loss: 0.31490015983581543\n",
            "Epoch: 171 | Loss: 0.23215045034885406 | Val. Loss: 0.31527429819107056\n",
            "Epoch: 172 | Loss: 0.21956303715705872 | Val. Loss: 0.3147816061973572\n",
            "Epoch: 173 | Loss: 0.13863328099250793 | Val. Loss: 0.31530770659446716\n",
            "Epoch: 174 | Loss: 0.15225712954998016 | Val. Loss: 0.31363898515701294\n",
            "Epoch: 175 | Loss: 0.2342054396867752 | Val. Loss: 0.3132346570491791\n",
            "Epoch: 176 | Loss: 0.17833569645881653 | Val. Loss: 0.3137926161289215\n",
            "Epoch: 177 | Loss: 0.17446181178092957 | Val. Loss: 0.3132440745830536\n",
            "Epoch: 178 | Loss: 0.15059000253677368 | Val. Loss: 0.31239286065101624\n",
            "Epoch: 179 | Loss: 0.15993879735469818 | Val. Loss: 0.31143441796302795\n",
            "Epoch: 180 | Loss: 0.14608919620513916 | Val. Loss: 0.31174159049987793\n",
            "Epoch: 181 | Loss: 0.3439579904079437 | Val. Loss: 0.31254658102989197\n",
            "Epoch: 182 | Loss: 0.24617895483970642 | Val. Loss: 0.31071221828460693\n",
            "Epoch: 183 | Loss: 0.29123052954673767 | Val. Loss: 0.30865082144737244\n",
            "Epoch: 184 | Loss: 0.40529248118400574 | Val. Loss: 0.3099156618118286\n",
            "Epoch: 185 | Loss: 0.19750231504440308 | Val. Loss: 0.30852165818214417\n",
            "Epoch: 186 | Loss: 0.12609095871448517 | Val. Loss: 0.30801987648010254\n",
            "Epoch: 187 | Loss: 0.14817529916763306 | Val. Loss: 0.3076360523700714\n",
            "Epoch: 188 | Loss: 0.20175108313560486 | Val. Loss: 0.3085762858390808\n",
            "Epoch: 189 | Loss: 0.3596839904785156 | Val. Loss: 0.3076649308204651\n",
            "Epoch: 190 | Loss: 0.29816436767578125 | Val. Loss: 0.3064139187335968\n",
            "Epoch: 191 | Loss: 0.2850009500980377 | Val. Loss: 0.3051223158836365\n",
            "Epoch: 192 | Loss: 0.18965578079223633 | Val. Loss: 0.30399850010871887\n",
            "Epoch: 193 | Loss: 0.19658567011356354 | Val. Loss: 0.30470526218414307\n",
            "Epoch: 194 | Loss: 0.22143524885177612 | Val. Loss: 0.303757905960083\n",
            "Epoch: 195 | Loss: 0.27107757329940796 | Val. Loss: 0.3031519651412964\n",
            "Epoch: 196 | Loss: 0.2323247194290161 | Val. Loss: 0.30311089754104614\n",
            "Epoch: 197 | Loss: 0.3967501223087311 | Val. Loss: 0.3019944131374359\n",
            "Epoch: 198 | Loss: 0.1420758217573166 | Val. Loss: 0.301150381565094\n",
            "Epoch: 199 | Loss: 0.15939168632030487 | Val. Loss: 0.3013717532157898\n",
            "Epoch: 200 | Loss: 0.20701488852500916 | Val. Loss: 0.30040326714515686\n",
            "Epoch: 201 | Loss: 0.3106473684310913 | Val. Loss: 0.29950225353240967\n",
            "Epoch: 202 | Loss: 0.11192932724952698 | Val. Loss: 0.2992038130760193\n",
            "Epoch: 203 | Loss: 0.19897204637527466 | Val. Loss: 0.2973489761352539\n",
            "Epoch: 204 | Loss: 0.21985091269016266 | Val. Loss: 0.29671016335487366\n",
            "Epoch: 205 | Loss: 0.15496349334716797 | Val. Loss: 0.2961658835411072\n",
            "Epoch: 206 | Loss: 0.146730437874794 | Val. Loss: 0.29636889696121216\n",
            "Epoch: 207 | Loss: 0.1291540116071701 | Val. Loss: 0.29547253251075745\n",
            "Epoch: 208 | Loss: 0.22136172652244568 | Val. Loss: 0.2948129177093506\n",
            "Epoch: 209 | Loss: 0.18750567734241486 | Val. Loss: 0.29358434677124023\n",
            "Epoch: 210 | Loss: 0.3241696059703827 | Val. Loss: 0.2933073341846466\n",
            "Epoch: 211 | Loss: 0.24420571327209473 | Val. Loss: 0.2925799787044525\n",
            "Epoch: 212 | Loss: 0.20265328884124756 | Val. Loss: 0.2924982011318207\n",
            "Epoch: 213 | Loss: 0.21212057769298553 | Val. Loss: 0.28992390632629395\n",
            "Epoch: 214 | Loss: 0.2764618992805481 | Val. Loss: 0.2901298403739929\n",
            "Epoch: 215 | Loss: 0.18547354638576508 | Val. Loss: 0.28923505544662476\n",
            "Epoch: 216 | Loss: 0.18439261615276337 | Val. Loss: 0.28831690549850464\n",
            "Epoch: 217 | Loss: 0.23670841753482819 | Val. Loss: 0.2890344560146332\n",
            "Epoch: 218 | Loss: 0.19786326587200165 | Val. Loss: 0.28804898262023926\n",
            "Epoch: 219 | Loss: 0.27869704365730286 | Val. Loss: 0.2881969213485718\n",
            "Epoch: 220 | Loss: 0.2497871071100235 | Val. Loss: 0.2869175672531128\n",
            "Epoch: 221 | Loss: 0.17313367128372192 | Val. Loss: 0.28609663248062134\n",
            "Epoch: 222 | Loss: 0.1013689711689949 | Val. Loss: 0.2850966453552246\n",
            "Epoch: 223 | Loss: 0.2796529233455658 | Val. Loss: 0.28557321429252625\n",
            "Epoch: 224 | Loss: 0.17121003568172455 | Val. Loss: 0.2838173806667328\n",
            "Epoch: 225 | Loss: 0.17481428384780884 | Val. Loss: 0.2847481071949005\n",
            "Epoch: 226 | Loss: 0.14553318917751312 | Val. Loss: 0.2835978865623474\n",
            "Epoch: 227 | Loss: 0.17793072760105133 | Val. Loss: 0.28314724564552307\n",
            "Epoch: 228 | Loss: 0.2727726399898529 | Val. Loss: 0.2816290855407715\n",
            "Epoch: 229 | Loss: 0.31043463945388794 | Val. Loss: 0.2828648090362549\n",
            "Epoch: 230 | Loss: 0.2464982271194458 | Val. Loss: 0.28035968542099\n",
            "Epoch: 231 | Loss: 0.13956011831760406 | Val. Loss: 0.27968376874923706\n",
            "Epoch: 232 | Loss: 0.1936548501253128 | Val. Loss: 0.27863240242004395\n",
            "Epoch: 233 | Loss: 0.2266586422920227 | Val. Loss: 0.27903032302856445\n",
            "Epoch: 234 | Loss: 0.188385009765625 | Val. Loss: 0.2773878276348114\n",
            "Epoch: 235 | Loss: 0.20378568768501282 | Val. Loss: 0.27603569626808167\n",
            "Epoch: 236 | Loss: 0.17380915582180023 | Val. Loss: 0.27750343084335327\n",
            "Epoch: 237 | Loss: 0.17229300737380981 | Val. Loss: 0.27533164620399475\n",
            "Epoch: 238 | Loss: 0.20143826305866241 | Val. Loss: 0.2754564583301544\n",
            "Epoch: 239 | Loss: 0.11540597677230835 | Val. Loss: 0.27483728528022766\n",
            "Epoch: 240 | Loss: 0.2047324925661087 | Val. Loss: 0.2748888432979584\n",
            "Epoch: 241 | Loss: 0.22289180755615234 | Val. Loss: 0.2729105055332184\n",
            "Epoch: 242 | Loss: 0.23495344817638397 | Val. Loss: 0.2736840844154358\n",
            "Epoch: 243 | Loss: 0.1845143437385559 | Val. Loss: 0.27289190888404846\n",
            "Epoch: 244 | Loss: 0.2403811365365982 | Val. Loss: 0.27353137731552124\n",
            "Epoch: 245 | Loss: 0.2174489051103592 | Val. Loss: 0.2727382183074951\n",
            "Epoch: 246 | Loss: 0.2547747790813446 | Val. Loss: 0.271507203578949\n",
            "Epoch: 247 | Loss: 0.17165899276733398 | Val. Loss: 0.2703271508216858\n",
            "Epoch: 248 | Loss: 0.2346227914094925 | Val. Loss: 0.27110517024993896\n",
            "Epoch: 249 | Loss: 0.11595166474580765 | Val. Loss: 0.2698299288749695\n",
            "Epoch: 250 | Loss: 0.23534844815731049 | Val. Loss: 0.2689858078956604\n",
            "Epoch: 251 | Loss: 0.35173535346984863 | Val. Loss: 0.2706804871559143\n",
            "Epoch: 252 | Loss: 0.1371118128299713 | Val. Loss: 0.26950305700302124\n",
            "Epoch: 253 | Loss: 0.2343628704547882 | Val. Loss: 0.2695537805557251\n",
            "Epoch: 254 | Loss: 0.2657899856567383 | Val. Loss: 0.26803404092788696\n",
            "Epoch: 255 | Loss: 0.1327746957540512 | Val. Loss: 0.26745933294296265\n",
            "Epoch: 256 | Loss: 0.2177804708480835 | Val. Loss: 0.2675832211971283\n",
            "Epoch: 257 | Loss: 0.1425260603427887 | Val. Loss: 0.2666698396205902\n",
            "Epoch: 258 | Loss: 0.10387630015611649 | Val. Loss: 0.26629358530044556\n",
            "Epoch: 259 | Loss: 0.12547557055950165 | Val. Loss: 0.26704394817352295\n",
            "Epoch: 260 | Loss: 0.10726359486579895 | Val. Loss: 0.26545897126197815\n",
            "Epoch: 261 | Loss: 0.2353949248790741 | Val. Loss: 0.2661399245262146\n",
            "Epoch: 262 | Loss: 0.18433180451393127 | Val. Loss: 0.26532429456710815\n",
            "Epoch: 263 | Loss: 0.24963800609111786 | Val. Loss: 0.26606422662734985\n",
            "Epoch: 264 | Loss: 0.3560773432254791 | Val. Loss: 0.2641626000404358\n",
            "Epoch: 265 | Loss: 0.2007216364145279 | Val. Loss: 0.2632501423358917\n",
            "Epoch: 266 | Loss: 0.14347290992736816 | Val. Loss: 0.26371291279792786\n",
            "Epoch: 267 | Loss: 0.12123238295316696 | Val. Loss: 0.26166701316833496\n",
            "Epoch: 268 | Loss: 0.24605447053909302 | Val. Loss: 0.2622840702533722\n",
            "Epoch: 269 | Loss: 0.29824820160865784 | Val. Loss: 0.2609676718711853\n",
            "Epoch: 270 | Loss: 0.186636283993721 | Val. Loss: 0.2604709267616272\n",
            "Epoch: 271 | Loss: 0.21307842433452606 | Val. Loss: 0.26057881116867065\n",
            "Epoch: 272 | Loss: 0.1937946081161499 | Val. Loss: 0.26082679629325867\n",
            "Epoch: 273 | Loss: 0.26057448983192444 | Val. Loss: 0.2606561779975891\n",
            "Epoch: 274 | Loss: 0.21519844233989716 | Val. Loss: 0.25935056805610657\n",
            "Epoch: 275 | Loss: 0.19637492299079895 | Val. Loss: 0.25938230752944946\n",
            "Epoch: 276 | Loss: 0.1910572052001953 | Val. Loss: 0.2596713900566101\n",
            "Epoch: 277 | Loss: 0.21937794983386993 | Val. Loss: 0.2583775222301483\n",
            "Epoch: 278 | Loss: 0.1555221527814865 | Val. Loss: 0.2576645016670227\n",
            "Epoch: 279 | Loss: 0.1924182027578354 | Val. Loss: 0.2569967806339264\n",
            "Epoch: 280 | Loss: 0.15569253265857697 | Val. Loss: 0.2556043267250061\n",
            "Epoch: 281 | Loss: 0.22565597295761108 | Val. Loss: 0.25728344917297363\n",
            "Epoch: 282 | Loss: 0.09768244624137878 | Val. Loss: 0.2575603127479553\n",
            "Epoch: 283 | Loss: 0.2731645405292511 | Val. Loss: 0.25571244955062866\n",
            "Epoch: 284 | Loss: 0.09719167649745941 | Val. Loss: 0.2557777762413025\n",
            "Epoch: 285 | Loss: 0.1770908236503601 | Val. Loss: 0.2551462948322296\n",
            "Epoch: 286 | Loss: 0.1540544033050537 | Val. Loss: 0.25597208738327026\n",
            "Epoch: 287 | Loss: 0.2633436918258667 | Val. Loss: 0.2564106583595276\n",
            "Epoch: 288 | Loss: 0.24074804782867432 | Val. Loss: 0.2535869777202606\n",
            "Epoch: 289 | Loss: 0.23353233933448792 | Val. Loss: 0.25343698263168335\n",
            "Epoch: 290 | Loss: 0.24737045168876648 | Val. Loss: 0.25335872173309326\n",
            "Epoch: 291 | Loss: 0.181788831949234 | Val. Loss: 0.25378772616386414\n",
            "Epoch: 292 | Loss: 0.22667871415615082 | Val. Loss: 0.25323039293289185\n",
            "Epoch: 293 | Loss: 0.10340587049722672 | Val. Loss: 0.25320062041282654\n",
            "Epoch: 294 | Loss: 0.1824655383825302 | Val. Loss: 0.2510348856449127\n",
            "Epoch: 295 | Loss: 0.15549343824386597 | Val. Loss: 0.25051039457321167\n",
            "Epoch: 296 | Loss: 0.10214122384786606 | Val. Loss: 0.2509883642196655\n",
            "Epoch: 297 | Loss: 0.10111285001039505 | Val. Loss: 0.2508789002895355\n",
            "Epoch: 298 | Loss: 0.20952585339546204 | Val. Loss: 0.24976404011249542\n",
            "Epoch: 299 | Loss: 0.10916516929864883 | Val. Loss: 0.24996259808540344\n",
            "Epoch: 300 | Loss: 0.13661275804042816 | Val. Loss: 0.250479519367218\n",
            "Epoch: 301 | Loss: 0.15854507684707642 | Val. Loss: 0.2505541145801544\n",
            "Epoch: 302 | Loss: 0.2126232534646988 | Val. Loss: 0.24957498908042908\n",
            "Epoch: 303 | Loss: 0.17079217731952667 | Val. Loss: 0.2480086386203766\n",
            "Epoch: 304 | Loss: 0.15997493267059326 | Val. Loss: 0.24710333347320557\n",
            "Epoch: 305 | Loss: 0.20522841811180115 | Val. Loss: 0.24726548790931702\n",
            "Epoch: 306 | Loss: 0.16630534827709198 | Val. Loss: 0.24721714854240417\n",
            "Epoch: 307 | Loss: 0.12444179505109787 | Val. Loss: 0.24647590517997742\n",
            "Epoch: 308 | Loss: 0.1208919957280159 | Val. Loss: 0.24670127034187317\n",
            "Epoch: 309 | Loss: 0.13666652143001556 | Val. Loss: 0.2465815246105194\n",
            "Epoch: 310 | Loss: 0.17888353765010834 | Val. Loss: 0.24657920002937317\n",
            "Epoch: 311 | Loss: 0.1614418774843216 | Val. Loss: 0.24569030106067657\n",
            "Epoch: 312 | Loss: 0.19639509916305542 | Val. Loss: 0.24680618941783905\n",
            "Epoch: 313 | Loss: 0.14362256228923798 | Val. Loss: 0.24504020810127258\n",
            "Epoch: 314 | Loss: 0.14051344990730286 | Val. Loss: 0.2448645532131195\n",
            "Epoch: 315 | Loss: 0.09618525952100754 | Val. Loss: 0.24391162395477295\n",
            "Epoch: 316 | Loss: 0.20052461326122284 | Val. Loss: 0.24381902813911438\n",
            "Epoch: 317 | Loss: 0.29737749695777893 | Val. Loss: 0.24433358013629913\n",
            "Epoch: 318 | Loss: 0.10411911457777023 | Val. Loss: 0.24379782378673553\n",
            "Epoch: 319 | Loss: 0.2713410556316376 | Val. Loss: 0.24331095814704895\n",
            "Epoch: 320 | Loss: 0.10640796273946762 | Val. Loss: 0.24104495346546173\n",
            "Epoch: 321 | Loss: 0.11462181061506271 | Val. Loss: 0.24031086266040802\n",
            "Epoch: 322 | Loss: 0.20232778787612915 | Val. Loss: 0.2392943799495697\n",
            "Epoch: 323 | Loss: 0.18915483355522156 | Val. Loss: 0.23958809673786163\n",
            "Epoch: 324 | Loss: 0.1311034858226776 | Val. Loss: 0.23843100666999817\n",
            "Epoch: 325 | Loss: 0.17942902445793152 | Val. Loss: 0.23796173930168152\n",
            "Epoch: 326 | Loss: 0.1574568897485733 | Val. Loss: 0.23460431396961212\n",
            "Epoch: 327 | Loss: 0.17676086723804474 | Val. Loss: 0.23582510650157928\n",
            "Epoch: 328 | Loss: 0.24235904216766357 | Val. Loss: 0.23582962155342102\n",
            "Epoch: 329 | Loss: 0.2494697868824005 | Val. Loss: 0.23396635055541992\n",
            "Epoch: 330 | Loss: 0.12602892518043518 | Val. Loss: 0.23274540901184082\n",
            "Epoch: 331 | Loss: 0.23214295506477356 | Val. Loss: 0.23395636677742004\n",
            "Epoch: 332 | Loss: 0.11849164962768555 | Val. Loss: 0.23262441158294678\n",
            "Epoch: 333 | Loss: 0.17468854784965515 | Val. Loss: 0.23240074515342712\n",
            "Epoch: 334 | Loss: 0.1926829069852829 | Val. Loss: 0.23086707293987274\n",
            "Epoch: 335 | Loss: 0.1607658416032791 | Val. Loss: 0.23096124827861786\n",
            "Epoch: 336 | Loss: 0.13758791983127594 | Val. Loss: 0.22921395301818848\n",
            "Epoch: 337 | Loss: 0.10270914435386658 | Val. Loss: 0.22914724051952362\n",
            "Epoch: 338 | Loss: 0.24911238253116608 | Val. Loss: 0.2289445400238037\n",
            "Epoch: 339 | Loss: 0.12037917971611023 | Val. Loss: 0.22959399223327637\n",
            "Epoch: 340 | Loss: 0.07593216001987457 | Val. Loss: 0.2283993512392044\n",
            "Epoch: 341 | Loss: 0.15479117631912231 | Val. Loss: 0.22804555296897888\n",
            "Epoch: 342 | Loss: 0.2111741304397583 | Val. Loss: 0.22813014686107635\n",
            "Epoch: 343 | Loss: 0.23218874633312225 | Val. Loss: 0.22767849266529083\n",
            "Epoch: 344 | Loss: 0.2703403830528259 | Val. Loss: 0.22709624469280243\n",
            "Epoch: 345 | Loss: 0.1436297595500946 | Val. Loss: 0.22786077857017517\n",
            "Epoch: 346 | Loss: 0.21419987082481384 | Val. Loss: 0.2258903682231903\n",
            "Epoch: 347 | Loss: 0.12389660626649857 | Val. Loss: 0.22655954957008362\n",
            "Epoch: 348 | Loss: 0.16571079194545746 | Val. Loss: 0.224918395280838\n",
            "Epoch: 349 | Loss: 0.19059793651103973 | Val. Loss: 0.22536757588386536\n",
            "Epoch: 350 | Loss: 0.2016710638999939 | Val. Loss: 0.2246280014514923\n",
            "Epoch: 351 | Loss: 0.12252458930015564 | Val. Loss: 0.22460095584392548\n",
            "Epoch: 352 | Loss: 0.15528985857963562 | Val. Loss: 0.2257770299911499\n",
            "Epoch: 353 | Loss: 0.24415378272533417 | Val. Loss: 0.22724130749702454\n",
            "Epoch: 354 | Loss: 0.15272116661071777 | Val. Loss: 0.22622764110565186\n",
            "Epoch: 355 | Loss: 0.15024258196353912 | Val. Loss: 0.2253512442111969\n",
            "Epoch: 356 | Loss: 0.17208325862884521 | Val. Loss: 0.22505585849285126\n",
            "Epoch: 357 | Loss: 0.20616941154003143 | Val. Loss: 0.22485578060150146\n",
            "Epoch: 358 | Loss: 0.11601685732603073 | Val. Loss: 0.22363674640655518\n",
            "Epoch: 359 | Loss: 0.156454935669899 | Val. Loss: 0.22394318878650665\n",
            "Epoch: 360 | Loss: 0.2854839861392975 | Val. Loss: 0.22379150986671448\n",
            "Epoch: 361 | Loss: 0.2375793755054474 | Val. Loss: 0.22333796322345734\n",
            "Epoch: 362 | Loss: 0.08603685349225998 | Val. Loss: 0.22203513979911804\n",
            "Epoch: 363 | Loss: 0.11189082264900208 | Val. Loss: 0.22197572886943817\n",
            "Epoch: 364 | Loss: 0.11663506180047989 | Val. Loss: 0.221318319439888\n",
            "Epoch: 365 | Loss: 0.10282430052757263 | Val. Loss: 0.2202386111021042\n",
            "Epoch: 366 | Loss: 0.1367700845003128 | Val. Loss: 0.22059758007526398\n",
            "Epoch: 367 | Loss: 0.06007792055606842 | Val. Loss: 0.2196815460920334\n",
            "Epoch: 368 | Loss: 0.1536046266555786 | Val. Loss: 0.2191147357225418\n",
            "Epoch: 369 | Loss: 0.149328351020813 | Val. Loss: 0.21991920471191406\n",
            "Epoch: 370 | Loss: 0.17468693852424622 | Val. Loss: 0.21985387802124023\n",
            "Epoch: 371 | Loss: 0.15498048067092896 | Val. Loss: 0.21842432022094727\n",
            "Epoch: 372 | Loss: 0.17705756425857544 | Val. Loss: 0.21749238669872284\n",
            "Epoch: 373 | Loss: 0.14222688972949982 | Val. Loss: 0.21848997473716736\n",
            "Epoch: 374 | Loss: 0.13380791246891022 | Val. Loss: 0.21717755496501923\n",
            "Epoch: 375 | Loss: 0.11656086891889572 | Val. Loss: 0.2178429812192917\n",
            "Epoch: 376 | Loss: 0.14778432250022888 | Val. Loss: 0.21770711243152618\n",
            "Epoch: 377 | Loss: 0.16598927974700928 | Val. Loss: 0.21670696139335632\n",
            "Epoch: 378 | Loss: 0.11879081279039383 | Val. Loss: 0.21567101776599884\n",
            "Epoch: 379 | Loss: 0.10626285523176193 | Val. Loss: 0.2150346338748932\n",
            "Epoch: 380 | Loss: 0.2290116846561432 | Val. Loss: 0.21539807319641113\n",
            "Epoch: 381 | Loss: 0.12161391973495483 | Val. Loss: 0.21469298005104065\n",
            "Epoch: 382 | Loss: 0.0764903873205185 | Val. Loss: 0.21400649845600128\n",
            "Epoch: 383 | Loss: 0.1050555631518364 | Val. Loss: 0.2140512466430664\n",
            "Epoch: 384 | Loss: 0.1312171220779419 | Val. Loss: 0.21509985625743866\n",
            "Epoch: 385 | Loss: 0.18786491453647614 | Val. Loss: 0.21275949478149414\n",
            "Epoch: 386 | Loss: 0.2154957503080368 | Val. Loss: 0.2137175351381302\n",
            "Epoch: 387 | Loss: 0.1340307891368866 | Val. Loss: 0.21276429295539856\n",
            "Epoch: 388 | Loss: 0.24247081577777863 | Val. Loss: 0.2122335135936737\n",
            "Epoch: 389 | Loss: 0.1038280501961708 | Val. Loss: 0.21209940314292908\n",
            "Epoch: 390 | Loss: 0.09489154815673828 | Val. Loss: 0.2115335464477539\n",
            "Epoch: 391 | Loss: 0.07608992606401443 | Val. Loss: 0.2108001708984375\n",
            "Epoch: 392 | Loss: 0.15030576288700104 | Val. Loss: 0.21064713597297668\n",
            "Epoch: 393 | Loss: 0.11443808674812317 | Val. Loss: 0.21120433509349823\n",
            "Epoch: 394 | Loss: 0.16194599866867065 | Val. Loss: 0.20908983051776886\n",
            "Epoch: 395 | Loss: 0.12124574184417725 | Val. Loss: 0.20905470848083496\n",
            "Epoch: 396 | Loss: 0.20397213101387024 | Val. Loss: 0.2101273238658905\n",
            "Epoch: 397 | Loss: 0.17436495423316956 | Val. Loss: 0.2096448689699173\n",
            "Epoch: 398 | Loss: 0.12203704565763474 | Val. Loss: 0.20820733904838562\n",
            "Epoch: 399 | Loss: 0.17046791315078735 | Val. Loss: 0.2072083055973053\n",
            "Epoch: 400 | Loss: 0.11715005338191986 | Val. Loss: 0.20744776725769043\n",
            "Epoch: 401 | Loss: 0.1370282620191574 | Val. Loss: 0.20624935626983643\n",
            "Epoch: 402 | Loss: 0.17949102818965912 | Val. Loss: 0.20630666613578796\n",
            "Epoch: 403 | Loss: 0.13569825887680054 | Val. Loss: 0.20642957091331482\n",
            "Epoch: 404 | Loss: 0.22527293860912323 | Val. Loss: 0.20418159663677216\n",
            "Epoch: 405 | Loss: 0.18147242069244385 | Val. Loss: 0.2028474062681198\n",
            "Epoch: 406 | Loss: 0.1251845359802246 | Val. Loss: 0.2034757435321808\n",
            "Epoch: 407 | Loss: 0.23151037096977234 | Val. Loss: 0.20352225005626678\n",
            "Epoch: 408 | Loss: 0.112303726375103 | Val. Loss: 0.2012079954147339\n",
            "Epoch: 409 | Loss: 0.08178391307592392 | Val. Loss: 0.2015911042690277\n",
            "Epoch: 410 | Loss: 0.08705625683069229 | Val. Loss: 0.20005901157855988\n",
            "Epoch: 411 | Loss: 0.1583937257528305 | Val. Loss: 0.20097453892230988\n",
            "Epoch: 412 | Loss: 0.07217998802661896 | Val. Loss: 0.19914348423480988\n",
            "Epoch: 413 | Loss: 0.08404544740915298 | Val. Loss: 0.19889172911643982\n",
            "Epoch: 414 | Loss: 0.09365568310022354 | Val. Loss: 0.19817543029785156\n",
            "Epoch: 415 | Loss: 0.13735146820545197 | Val. Loss: 0.19854676723480225\n",
            "Epoch: 416 | Loss: 0.08948884904384613 | Val. Loss: 0.1973109245300293\n",
            "Epoch: 417 | Loss: 0.12525199353694916 | Val. Loss: 0.19565680623054504\n",
            "Epoch: 418 | Loss: 0.08054932206869125 | Val. Loss: 0.19605840742588043\n",
            "Epoch: 419 | Loss: 0.2095252126455307 | Val. Loss: 0.19680336117744446\n",
            "Epoch: 420 | Loss: 0.07293371111154556 | Val. Loss: 0.19469408690929413\n",
            "Epoch: 421 | Loss: 0.2673815190792084 | Val. Loss: 0.19669602811336517\n",
            "Epoch: 422 | Loss: 0.09803296625614166 | Val. Loss: 0.19482889771461487\n",
            "Epoch: 423 | Loss: 0.09134556353092194 | Val. Loss: 0.19387207925319672\n",
            "Epoch: 424 | Loss: 0.061380088329315186 | Val. Loss: 0.19374223053455353\n",
            "Epoch: 425 | Loss: 0.20186524093151093 | Val. Loss: 0.19347169995307922\n",
            "Epoch: 426 | Loss: 0.2052924931049347 | Val. Loss: 0.19312012195587158\n",
            "Epoch: 427 | Loss: 0.12217338383197784 | Val. Loss: 0.19178546965122223\n",
            "Epoch: 428 | Loss: 0.10133923590183258 | Val. Loss: 0.19161905348300934\n",
            "Epoch: 429 | Loss: 0.18407516181468964 | Val. Loss: 0.1919231414794922\n",
            "Epoch: 430 | Loss: 0.14019553363323212 | Val. Loss: 0.18988490104675293\n",
            "Epoch: 431 | Loss: 0.14728549122810364 | Val. Loss: 0.19111593067646027\n",
            "Epoch: 432 | Loss: 0.11302057653665543 | Val. Loss: 0.18944647908210754\n",
            "Epoch: 433 | Loss: 0.05690247192978859 | Val. Loss: 0.18879500031471252\n",
            "Epoch: 434 | Loss: 0.23430687189102173 | Val. Loss: 0.19025978446006775\n",
            "Epoch: 435 | Loss: 0.20910881459712982 | Val. Loss: 0.1873163878917694\n",
            "Epoch: 436 | Loss: 0.11899572610855103 | Val. Loss: 0.18731525540351868\n",
            "Epoch: 437 | Loss: 0.12725965678691864 | Val. Loss: 0.18851624429225922\n",
            "Epoch: 438 | Loss: 0.10866683721542358 | Val. Loss: 0.18576934933662415\n",
            "Epoch: 439 | Loss: 0.07739833742380142 | Val. Loss: 0.18887881934642792\n",
            "Epoch: 440 | Loss: 0.08311662822961807 | Val. Loss: 0.1870640367269516\n",
            "Epoch: 441 | Loss: 0.07893174886703491 | Val. Loss: 0.1858583390712738\n",
            "Epoch: 442 | Loss: 0.11012396961450577 | Val. Loss: 0.1864282190799713\n",
            "Epoch: 443 | Loss: 0.1562241166830063 | Val. Loss: 0.18559420108795166\n",
            "Epoch: 444 | Loss: 0.11525978147983551 | Val. Loss: 0.18497483432292938\n",
            "Epoch: 445 | Loss: 0.06554022431373596 | Val. Loss: 0.18499144911766052\n",
            "Epoch: 446 | Loss: 0.06686096638441086 | Val. Loss: 0.18494275212287903\n",
            "Epoch: 447 | Loss: 0.09812521934509277 | Val. Loss: 0.18430264294147491\n",
            "Epoch: 448 | Loss: 0.04368504881858826 | Val. Loss: 0.1843719482421875\n",
            "Epoch: 449 | Loss: 0.1010063886642456 | Val. Loss: 0.18365785479545593\n",
            "Epoch: 450 | Loss: 0.07489930093288422 | Val. Loss: 0.18295374512672424\n",
            "Epoch: 451 | Loss: 0.0907326191663742 | Val. Loss: 0.18266794085502625\n",
            "Epoch: 452 | Loss: 0.1372450292110443 | Val. Loss: 0.18295657634735107\n",
            "Epoch: 453 | Loss: 0.1896187663078308 | Val. Loss: 0.18092560768127441\n",
            "Epoch: 454 | Loss: 0.29891371726989746 | Val. Loss: 0.18226394057273865\n",
            "Epoch: 455 | Loss: 0.11856510490179062 | Val. Loss: 0.1819612830877304\n",
            "Epoch: 456 | Loss: 0.18536719679832458 | Val. Loss: 0.1806374043226242\n",
            "Epoch: 457 | Loss: 0.08575712144374847 | Val. Loss: 0.18216994404792786\n",
            "Epoch: 458 | Loss: 0.05720731243491173 | Val. Loss: 0.1807476282119751\n",
            "Epoch: 459 | Loss: 0.15266983211040497 | Val. Loss: 0.1810256540775299\n",
            "Epoch: 460 | Loss: 0.07221662253141403 | Val. Loss: 0.17944306135177612\n",
            "Epoch: 461 | Loss: 0.19434601068496704 | Val. Loss: 0.17943444848060608\n",
            "Epoch: 462 | Loss: 0.08401703834533691 | Val. Loss: 0.17982865869998932\n",
            "Epoch: 463 | Loss: 0.2671155631542206 | Val. Loss: 0.17976418137550354\n",
            "Epoch: 464 | Loss: 0.09802781790494919 | Val. Loss: 0.1791837364435196\n",
            "Epoch: 465 | Loss: 0.061115458607673645 | Val. Loss: 0.1781626045703888\n",
            "Epoch: 466 | Loss: 0.11095956712961197 | Val. Loss: 0.17951765656471252\n",
            "Epoch: 467 | Loss: 0.1344595104455948 | Val. Loss: 0.17698036134243011\n",
            "Epoch: 468 | Loss: 0.09217564016580582 | Val. Loss: 0.17761161923408508\n",
            "Epoch: 469 | Loss: 0.08797292411327362 | Val. Loss: 0.17819571495056152\n",
            "Epoch: 470 | Loss: 0.11347504705190659 | Val. Loss: 0.1775924116373062\n",
            "Epoch: 471 | Loss: 0.07624995708465576 | Val. Loss: 0.17738501727581024\n",
            "Epoch: 472 | Loss: 0.09503284096717834 | Val. Loss: 0.17706510424613953\n",
            "Epoch: 473 | Loss: 0.08242436498403549 | Val. Loss: 0.17808108031749725\n",
            "Epoch: 474 | Loss: 0.10673409700393677 | Val. Loss: 0.17643597722053528\n",
            "Epoch: 475 | Loss: 0.060497578233480453 | Val. Loss: 0.1751573532819748\n",
            "Epoch: 476 | Loss: 0.061361271888017654 | Val. Loss: 0.17488357424736023\n",
            "Epoch: 477 | Loss: 0.25704631209373474 | Val. Loss: 0.17659619450569153\n",
            "Epoch: 478 | Loss: 0.10587707161903381 | Val. Loss: 0.17504718899726868\n",
            "Epoch: 479 | Loss: 0.09566447883844376 | Val. Loss: 0.17291902005672455\n",
            "Epoch: 480 | Loss: 0.11183328926563263 | Val. Loss: 0.17461052536964417\n",
            "Epoch: 481 | Loss: 0.10188610851764679 | Val. Loss: 0.17457215487957\n",
            "Epoch: 482 | Loss: 0.11340376734733582 | Val. Loss: 0.17264874279499054\n",
            "Epoch: 483 | Loss: 0.06615518778562546 | Val. Loss: 0.17297519743442535\n",
            "Epoch: 484 | Loss: 0.2134169489145279 | Val. Loss: 0.17283806204795837\n",
            "Epoch: 485 | Loss: 0.05051446333527565 | Val. Loss: 0.17285127937793732\n",
            "Epoch: 486 | Loss: 0.11174459755420685 | Val. Loss: 0.17241691052913666\n",
            "Epoch: 487 | Loss: 0.21263347566127777 | Val. Loss: 0.1724044382572174\n",
            "Epoch: 488 | Loss: 0.0761679857969284 | Val. Loss: 0.17115040123462677\n",
            "Epoch: 489 | Loss: 0.08666607737541199 | Val. Loss: 0.17179498076438904\n",
            "Epoch: 490 | Loss: 0.051168426871299744 | Val. Loss: 0.16973745822906494\n",
            "Epoch: 491 | Loss: 0.11709058284759521 | Val. Loss: 0.1703878939151764\n",
            "Epoch: 492 | Loss: 0.07262171059846878 | Val. Loss: 0.17094314098358154\n",
            "Epoch: 493 | Loss: 0.0637580156326294 | Val. Loss: 0.1702020764350891\n",
            "Epoch: 494 | Loss: 0.18394504487514496 | Val. Loss: 0.17111937701702118\n",
            "Epoch: 495 | Loss: 0.08411132544279099 | Val. Loss: 0.16941967606544495\n",
            "Epoch: 496 | Loss: 0.07334861904382706 | Val. Loss: 0.16924729943275452\n",
            "Epoch: 497 | Loss: 0.14672350883483887 | Val. Loss: 0.16953670978546143\n",
            "Epoch: 498 | Loss: 0.04386161267757416 | Val. Loss: 0.1683330237865448\n",
            "Epoch: 499 | Loss: 0.11107862740755081 | Val. Loss: 0.1696118861436844\n",
            "Epoch: 500 | Loss: 0.14849498867988586 | Val. Loss: 0.1689288467168808\n",
            "Epoch: 501 | Loss: 0.25136446952819824 | Val. Loss: 0.1676870882511139\n",
            "Epoch: 502 | Loss: 0.05231683328747749 | Val. Loss: 0.16761933267116547\n",
            "Epoch: 503 | Loss: 0.06765151023864746 | Val. Loss: 0.1666214019060135\n",
            "Epoch: 504 | Loss: 0.06892772763967514 | Val. Loss: 0.1663924902677536\n",
            "Epoch: 505 | Loss: 0.06724816560745239 | Val. Loss: 0.16555944085121155\n",
            "Epoch: 506 | Loss: 0.12070731818675995 | Val. Loss: 0.16535256803035736\n",
            "Epoch: 507 | Loss: 0.24598529934883118 | Val. Loss: 0.16692784428596497\n",
            "Epoch: 508 | Loss: 0.12532901763916016 | Val. Loss: 0.16655412316322327\n",
            "Epoch: 509 | Loss: 0.16062238812446594 | Val. Loss: 0.16607485711574554\n",
            "Epoch: 510 | Loss: 0.19114433228969574 | Val. Loss: 0.1647692620754242\n",
            "Epoch: 511 | Loss: 0.10349874943494797 | Val. Loss: 0.16450515389442444\n",
            "Epoch: 512 | Loss: 0.16926255822181702 | Val. Loss: 0.1649261862039566\n",
            "Epoch: 513 | Loss: 0.2650725543498993 | Val. Loss: 0.16376236081123352\n",
            "Epoch: 514 | Loss: 0.10610083490610123 | Val. Loss: 0.1625751405954361\n",
            "Epoch: 515 | Loss: 0.0861457958817482 | Val. Loss: 0.16397792100906372\n",
            "Epoch: 516 | Loss: 0.14595086872577667 | Val. Loss: 0.16210266947746277\n",
            "Epoch: 517 | Loss: 0.14301352202892303 | Val. Loss: 0.1633283942937851\n",
            "Epoch: 518 | Loss: 0.08102147281169891 | Val. Loss: 0.16323219239711761\n",
            "Epoch: 519 | Loss: 0.0954292044043541 | Val. Loss: 0.16161885857582092\n",
            "Epoch: 520 | Loss: 0.14484518766403198 | Val. Loss: 0.1611546128988266\n",
            "Epoch: 521 | Loss: 0.07851304858922958 | Val. Loss: 0.1625407338142395\n",
            "Epoch: 522 | Loss: 0.3107664883136749 | Val. Loss: 0.1621766984462738\n",
            "Epoch: 523 | Loss: 0.15044963359832764 | Val. Loss: 0.15925170481204987\n",
            "Epoch: 524 | Loss: 0.18007944524288177 | Val. Loss: 0.16012980043888092\n",
            "Epoch: 525 | Loss: 0.07886526733636856 | Val. Loss: 0.16067984700202942\n",
            "Epoch: 526 | Loss: 0.1802668422460556 | Val. Loss: 0.15962214767932892\n",
            "Epoch: 527 | Loss: 0.1752985417842865 | Val. Loss: 0.15930072963237762\n",
            "Epoch: 528 | Loss: 0.07367625087499619 | Val. Loss: 0.16045886278152466\n",
            "Epoch: 529 | Loss: 0.13956347107887268 | Val. Loss: 0.15862742066383362\n",
            "Epoch: 530 | Loss: 0.06386467069387436 | Val. Loss: 0.15772318840026855\n",
            "Epoch: 531 | Loss: 0.09131384640932083 | Val. Loss: 0.1584395468235016\n",
            "Epoch: 532 | Loss: 0.11165715008974075 | Val. Loss: 0.15885122120380402\n",
            "Epoch: 533 | Loss: 0.03669896721839905 | Val. Loss: 0.15631352365016937\n",
            "Epoch: 534 | Loss: 0.1498337835073471 | Val. Loss: 0.15848864614963531\n",
            "Epoch: 535 | Loss: 0.14258190989494324 | Val. Loss: 0.15708616375923157\n",
            "Epoch: 536 | Loss: 0.06754221022129059 | Val. Loss: 0.1551990658044815\n",
            "Epoch: 537 | Loss: 0.092783622443676 | Val. Loss: 0.15699666738510132\n",
            "Epoch: 538 | Loss: 0.06544467806816101 | Val. Loss: 0.15631239116191864\n",
            "Epoch: 539 | Loss: 0.18328018486499786 | Val. Loss: 0.15668685734272003\n",
            "Epoch: 540 | Loss: 0.1129782572388649 | Val. Loss: 0.15689153969287872\n",
            "Epoch: 541 | Loss: 0.09903500229120255 | Val. Loss: 0.15431134402751923\n",
            "Epoch: 542 | Loss: 0.06602602452039719 | Val. Loss: 0.15508505702018738\n",
            "Epoch: 543 | Loss: 0.17980994284152985 | Val. Loss: 0.15585407614707947\n",
            "Epoch: 544 | Loss: 0.04795369133353233 | Val. Loss: 0.15305106341838837\n",
            "Epoch: 545 | Loss: 0.07397296279668808 | Val. Loss: 0.15297190845012665\n",
            "Epoch: 546 | Loss: 0.0377860888838768 | Val. Loss: 0.15280413627624512\n",
            "Epoch: 547 | Loss: 0.11101340502500534 | Val. Loss: 0.1524079293012619\n",
            "Epoch: 548 | Loss: 0.07310766726732254 | Val. Loss: 0.1515568494796753\n",
            "Epoch: 549 | Loss: 0.05875412002205849 | Val. Loss: 0.15070155262947083\n",
            "Epoch: 550 | Loss: 0.10915940254926682 | Val. Loss: 0.1515418142080307\n",
            "Epoch: 551 | Loss: 0.08514714241027832 | Val. Loss: 0.15070149302482605\n",
            "Epoch: 552 | Loss: 0.10628217458724976 | Val. Loss: 0.14963200688362122\n",
            "Epoch: 553 | Loss: 0.0965021401643753 | Val. Loss: 0.1491125524044037\n",
            "Epoch: 554 | Loss: 0.059221502393484116 | Val. Loss: 0.14750085771083832\n",
            "Epoch: 555 | Loss: 0.0778658464550972 | Val. Loss: 0.14740535616874695\n",
            "Epoch: 556 | Loss: 0.08611544966697693 | Val. Loss: 0.14731408655643463\n",
            "Epoch: 557 | Loss: 0.18222454190254211 | Val. Loss: 0.14647074043750763\n",
            "Epoch: 558 | Loss: 0.16648584604263306 | Val. Loss: 0.1460196077823639\n",
            "Epoch: 559 | Loss: 0.18223819136619568 | Val. Loss: 0.14554740488529205\n",
            "Epoch: 560 | Loss: 0.18223148584365845 | Val. Loss: 0.1444128304719925\n",
            "Epoch: 561 | Loss: 0.1278679519891739 | Val. Loss: 0.14473587274551392\n",
            "Epoch: 562 | Loss: 0.16221293807029724 | Val. Loss: 0.1436106264591217\n",
            "Epoch: 563 | Loss: 0.083511583507061 | Val. Loss: 0.1407882273197174\n",
            "Epoch: 564 | Loss: 0.10192926228046417 | Val. Loss: 0.14166948199272156\n",
            "Epoch: 565 | Loss: 0.09721282124519348 | Val. Loss: 0.14176848530769348\n",
            "Epoch: 566 | Loss: 0.24454286694526672 | Val. Loss: 0.1407621204853058\n",
            "Epoch: 567 | Loss: 0.06940183788537979 | Val. Loss: 0.13899150490760803\n",
            "Epoch: 568 | Loss: 0.1501394659280777 | Val. Loss: 0.13948601484298706\n",
            "Epoch: 569 | Loss: 0.06896206736564636 | Val. Loss: 0.13775144517421722\n",
            "Epoch: 570 | Loss: 0.16516649723052979 | Val. Loss: 0.13888803124427795\n",
            "Epoch: 571 | Loss: 0.13378293812274933 | Val. Loss: 0.13816677033901215\n",
            "Epoch: 572 | Loss: 0.05868429318070412 | Val. Loss: 0.13673077523708344\n",
            "Epoch: 573 | Loss: 0.22866015136241913 | Val. Loss: 0.13698168098926544\n",
            "Epoch: 574 | Loss: 0.3153819441795349 | Val. Loss: 0.13494683802127838\n",
            "Epoch: 575 | Loss: 0.16645042598247528 | Val. Loss: 0.1351073682308197\n",
            "Epoch: 576 | Loss: 0.054203398525714874 | Val. Loss: 0.13399867713451385\n",
            "Epoch: 577 | Loss: 0.18171514570713043 | Val. Loss: 0.13383474946022034\n",
            "Epoch: 578 | Loss: 0.07100289314985275 | Val. Loss: 0.13388647139072418\n",
            "Epoch: 579 | Loss: 0.24214594066143036 | Val. Loss: 0.1336761862039566\n",
            "Epoch: 580 | Loss: 0.13617675006389618 | Val. Loss: 0.13324910402297974\n",
            "Epoch: 581 | Loss: 0.06870108842849731 | Val. Loss: 0.13121986389160156\n",
            "Epoch: 582 | Loss: 0.12993879616260529 | Val. Loss: 0.13115474581718445\n",
            "Epoch: 583 | Loss: 0.0534437857568264 | Val. Loss: 0.130162313580513\n",
            "Epoch: 584 | Loss: 0.19187453389167786 | Val. Loss: 0.1306992620229721\n",
            "Epoch: 585 | Loss: 0.09858764708042145 | Val. Loss: 0.13154390454292297\n",
            "Epoch: 586 | Loss: 0.15805009007453918 | Val. Loss: 0.12877032160758972\n",
            "Epoch: 587 | Loss: 0.14780914783477783 | Val. Loss: 0.12845686078071594\n",
            "Epoch: 588 | Loss: 0.14445596933364868 | Val. Loss: 0.12830685079097748\n",
            "Epoch: 589 | Loss: 0.06897643953561783 | Val. Loss: 0.12767261266708374\n",
            "Epoch: 590 | Loss: 0.08314115554094315 | Val. Loss: 0.1258147805929184\n",
            "Epoch: 591 | Loss: 0.1891365349292755 | Val. Loss: 0.12636028230190277\n",
            "Epoch: 592 | Loss: 0.1322655826807022 | Val. Loss: 0.12606088817119598\n",
            "Epoch: 593 | Loss: 0.2075914591550827 | Val. Loss: 0.12412136793136597\n",
            "Epoch: 594 | Loss: 0.1127791479229927 | Val. Loss: 0.1250396966934204\n",
            "Epoch: 595 | Loss: 0.09983304888010025 | Val. Loss: 0.12399488687515259\n",
            "Epoch: 596 | Loss: 0.057983074337244034 | Val. Loss: 0.12219394743442535\n",
            "Epoch: 597 | Loss: 0.07006221264600754 | Val. Loss: 0.12244127690792084\n",
            "Epoch: 598 | Loss: 0.09870808571577072 | Val. Loss: 0.12347637116909027\n",
            "Epoch: 599 | Loss: 0.16663479804992676 | Val. Loss: 0.1220492348074913\n",
            "Epoch: 600 | Loss: 0.07296172529459 | Val. Loss: 0.12238098680973053\n",
            "Epoch: 601 | Loss: 0.07234232127666473 | Val. Loss: 0.12111761420965195\n",
            "Epoch: 602 | Loss: 0.07018186151981354 | Val. Loss: 0.12197078764438629\n",
            "Epoch: 603 | Loss: 0.12321388721466064 | Val. Loss: 0.12089595943689346\n",
            "Epoch: 604 | Loss: 0.04131287708878517 | Val. Loss: 0.1195441484451294\n",
            "Epoch: 605 | Loss: 0.06921829283237457 | Val. Loss: 0.11917752027511597\n",
            "Epoch: 606 | Loss: 0.1531824767589569 | Val. Loss: 0.11975673586130142\n",
            "Epoch: 607 | Loss: 0.17014746367931366 | Val. Loss: 0.118991419672966\n",
            "Epoch: 608 | Loss: 0.06851604580879211 | Val. Loss: 0.11845717579126358\n",
            "Epoch: 609 | Loss: 0.0555277056992054 | Val. Loss: 0.1159067153930664\n",
            "Epoch: 610 | Loss: 0.05182817950844765 | Val. Loss: 0.11650433391332626\n",
            "Epoch: 611 | Loss: 0.05960451811552048 | Val. Loss: 0.11536504328250885\n",
            "Epoch: 612 | Loss: 0.17011883854866028 | Val. Loss: 0.11514043807983398\n",
            "Epoch: 613 | Loss: 0.08223747462034225 | Val. Loss: 0.11473327875137329\n",
            "Epoch: 614 | Loss: 0.20553553104400635 | Val. Loss: 0.11607251316308975\n",
            "Epoch: 615 | Loss: 0.26888665556907654 | Val. Loss: 0.11383102834224701\n",
            "Epoch: 616 | Loss: 0.0596734918653965 | Val. Loss: 0.11249546706676483\n",
            "Epoch: 617 | Loss: 0.0472533144056797 | Val. Loss: 0.11252983659505844\n",
            "Epoch: 618 | Loss: 0.13630428910255432 | Val. Loss: 0.11118184030056\n",
            "Epoch: 619 | Loss: 0.15669645369052887 | Val. Loss: 0.11135755479335785\n",
            "Epoch: 620 | Loss: 0.0731465220451355 | Val. Loss: 0.11041174083948135\n",
            "Epoch: 621 | Loss: 0.0720057487487793 | Val. Loss: 0.1101391464471817\n",
            "Epoch: 622 | Loss: 0.10991518944501877 | Val. Loss: 0.10967876762151718\n",
            "Epoch: 623 | Loss: 0.0657806321978569 | Val. Loss: 0.10890237987041473\n",
            "Epoch: 624 | Loss: 0.1060880571603775 | Val. Loss: 0.10938022285699844\n",
            "Epoch: 625 | Loss: 0.17238065600395203 | Val. Loss: 0.10796914249658585\n",
            "Epoch: 626 | Loss: 0.11292710900306702 | Val. Loss: 0.10778027772903442\n",
            "Epoch: 627 | Loss: 0.05313166230916977 | Val. Loss: 0.10734891891479492\n",
            "Epoch: 628 | Loss: 0.07961271703243256 | Val. Loss: 0.10774080455303192\n",
            "Epoch: 629 | Loss: 0.1734480857849121 | Val. Loss: 0.10713489353656769\n",
            "Epoch: 630 | Loss: 0.08430176228284836 | Val. Loss: 0.10707404464483261\n",
            "Epoch: 631 | Loss: 0.060411207377910614 | Val. Loss: 0.10567606985569\n",
            "Epoch: 632 | Loss: 0.19695866107940674 | Val. Loss: 0.10546709597110748\n",
            "Epoch: 633 | Loss: 0.17557932436466217 | Val. Loss: 0.10543090105056763\n",
            "Epoch: 634 | Loss: 0.16391392052173615 | Val. Loss: 0.10454706102609634\n",
            "Epoch: 635 | Loss: 0.04030824452638626 | Val. Loss: 0.1041329875588417\n",
            "Epoch: 636 | Loss: 0.0687030702829361 | Val. Loss: 0.10257862508296967\n",
            "Epoch: 637 | Loss: 0.09294864535331726 | Val. Loss: 0.10276313871145248\n",
            "Epoch: 638 | Loss: 0.08286912739276886 | Val. Loss: 0.10227879136800766\n",
            "Epoch: 639 | Loss: 0.23919779062271118 | Val. Loss: 0.10034842789173126\n",
            "Epoch: 640 | Loss: 0.10424142330884933 | Val. Loss: 0.10092730820178986\n",
            "Epoch: 641 | Loss: 0.03396281227469444 | Val. Loss: 0.1021958738565445\n",
            "Epoch: 642 | Loss: 0.08313887566328049 | Val. Loss: 0.10247288644313812\n",
            "Epoch: 643 | Loss: 0.04063531756401062 | Val. Loss: 0.10077975690364838\n",
            "Epoch: 644 | Loss: 0.09139014780521393 | Val. Loss: 0.0996725857257843\n",
            "Epoch: 645 | Loss: 0.04967379570007324 | Val. Loss: 0.09986232966184616\n",
            "Epoch: 646 | Loss: 0.057559024542570114 | Val. Loss: 0.09877049177885056\n",
            "Epoch: 647 | Loss: 0.04169805720448494 | Val. Loss: 0.09762920439243317\n",
            "Epoch: 648 | Loss: 0.10877308249473572 | Val. Loss: 0.09818393737077713\n",
            "Epoch: 649 | Loss: 0.16210000216960907 | Val. Loss: 0.09753914177417755\n",
            "Epoch: 650 | Loss: 0.0544971339404583 | Val. Loss: 0.09747385233640671\n",
            "Epoch: 651 | Loss: 0.060658156871795654 | Val. Loss: 0.09633094072341919\n",
            "Epoch: 652 | Loss: 0.0724756270647049 | Val. Loss: 0.09542814642190933\n",
            "Epoch: 653 | Loss: 0.17731302976608276 | Val. Loss: 0.09577890485525131\n",
            "Epoch: 654 | Loss: 0.07655562460422516 | Val. Loss: 0.09610272943973541\n",
            "Epoch: 655 | Loss: 0.064860500395298 | Val. Loss: 0.09322721511125565\n",
            "Epoch: 656 | Loss: 0.07516975700855255 | Val. Loss: 0.09469211846590042\n",
            "Epoch: 657 | Loss: 0.17841458320617676 | Val. Loss: 0.09382129460573196\n",
            "Epoch: 658 | Loss: 0.06834132969379425 | Val. Loss: 0.0941026583313942\n",
            "Epoch: 659 | Loss: 0.13069649040699005 | Val. Loss: 0.09277655929327011\n",
            "Epoch: 660 | Loss: 0.047624800354242325 | Val. Loss: 0.09233339130878448\n",
            "Epoch: 661 | Loss: 0.061052389442920685 | Val. Loss: 0.092013880610466\n",
            "Epoch: 662 | Loss: 0.14547355473041534 | Val. Loss: 0.09214842319488525\n",
            "Epoch: 663 | Loss: 0.06427598744630814 | Val. Loss: 0.09135012328624725\n",
            "Epoch: 664 | Loss: 0.09214325249195099 | Val. Loss: 0.09273773431777954\n",
            "Epoch: 665 | Loss: 0.050051141530275345 | Val. Loss: 0.09081961214542389\n",
            "Epoch: 666 | Loss: 0.07894027233123779 | Val. Loss: 0.0915275514125824\n",
            "Epoch: 667 | Loss: 0.23356929421424866 | Val. Loss: 0.08967386186122894\n",
            "Epoch: 668 | Loss: 0.060462869703769684 | Val. Loss: 0.08977866917848587\n",
            "Epoch: 669 | Loss: 0.09892328083515167 | Val. Loss: 0.0886576771736145\n",
            "Epoch: 670 | Loss: 0.0564880408346653 | Val. Loss: 0.08771215379238129\n",
            "Epoch: 671 | Loss: 0.06666909903287888 | Val. Loss: 0.08824227750301361\n",
            "Epoch: 672 | Loss: 0.08292948454618454 | Val. Loss: 0.08777917921543121\n",
            "Epoch: 673 | Loss: 0.08294016122817993 | Val. Loss: 0.08593245595693588\n",
            "Epoch: 674 | Loss: 0.0729292631149292 | Val. Loss: 0.08678482472896576\n",
            "Epoch: 675 | Loss: 0.063574880361557 | Val. Loss: 0.08643398433923721\n",
            "Epoch: 676 | Loss: 0.07785855978727341 | Val. Loss: 0.08591054379940033\n",
            "Epoch: 677 | Loss: 0.05732367932796478 | Val. Loss: 0.0858401507139206\n",
            "Epoch: 678 | Loss: 0.05462726578116417 | Val. Loss: 0.08463657647371292\n",
            "Epoch: 679 | Loss: 0.03147769719362259 | Val. Loss: 0.08439582586288452\n",
            "Epoch: 680 | Loss: 0.1207011416554451 | Val. Loss: 0.08442436158657074\n",
            "Epoch: 681 | Loss: 0.10106629878282547 | Val. Loss: 0.08480553328990936\n",
            "Epoch: 682 | Loss: 0.05917290970683098 | Val. Loss: 0.08365534245967865\n",
            "Epoch: 683 | Loss: 0.0931219682097435 | Val. Loss: 0.08242426067590714\n",
            "Epoch: 684 | Loss: 0.08365634083747864 | Val. Loss: 0.08290763199329376\n",
            "Epoch: 685 | Loss: 0.08862940967082977 | Val. Loss: 0.08232888579368591\n",
            "Epoch: 686 | Loss: 0.07702724635601044 | Val. Loss: 0.08232806622982025\n",
            "Epoch: 687 | Loss: 0.06336662918329239 | Val. Loss: 0.08207294344902039\n",
            "Epoch: 688 | Loss: 0.15514951944351196 | Val. Loss: 0.0822354182600975\n",
            "Epoch: 689 | Loss: 0.061951231211423874 | Val. Loss: 0.08023050427436829\n",
            "Epoch: 690 | Loss: 0.07752218842506409 | Val. Loss: 0.08034688979387283\n",
            "Epoch: 691 | Loss: 0.06313395500183105 | Val. Loss: 0.08018659800291061\n",
            "Epoch: 692 | Loss: 0.056213878095149994 | Val. Loss: 0.07942026853561401\n",
            "Epoch: 693 | Loss: 0.1013052687048912 | Val. Loss: 0.07846198230981827\n",
            "Epoch: 694 | Loss: 0.08057934790849686 | Val. Loss: 0.07776462286710739\n",
            "Epoch: 695 | Loss: 0.029329171404242516 | Val. Loss: 0.07705044746398926\n",
            "Epoch: 696 | Loss: 0.04740273579955101 | Val. Loss: 0.07700034230947495\n",
            "Epoch: 697 | Loss: 0.06706707924604416 | Val. Loss: 0.07643009722232819\n",
            "Epoch: 698 | Loss: 0.14282338321208954 | Val. Loss: 0.07615278661251068\n",
            "Epoch: 699 | Loss: 0.05298718810081482 | Val. Loss: 0.07751215994358063\n",
            "Epoch: 700 | Loss: 0.10190445184707642 | Val. Loss: 0.07586319744586945\n",
            "Epoch: 701 | Loss: 0.13710282742977142 | Val. Loss: 0.07622172683477402\n",
            "Epoch: 702 | Loss: 0.12465887516736984 | Val. Loss: 0.07547297328710556\n",
            "Epoch: 703 | Loss: 0.02733757346868515 | Val. Loss: 0.07599738985300064\n",
            "Epoch: 704 | Loss: 0.15320464968681335 | Val. Loss: 0.07466285675764084\n",
            "Epoch: 705 | Loss: 0.07977929711341858 | Val. Loss: 0.0744217187166214\n",
            "Epoch: 706 | Loss: 0.13374139368534088 | Val. Loss: 0.07327373325824738\n",
            "Epoch: 707 | Loss: 0.05396826192736626 | Val. Loss: 0.07395637780427933\n",
            "Epoch: 708 | Loss: 0.01953262835741043 | Val. Loss: 0.07435701787471771\n",
            "Epoch: 709 | Loss: 0.05723053216934204 | Val. Loss: 0.07328329980373383\n",
            "Epoch: 710 | Loss: 0.07210676372051239 | Val. Loss: 0.07321862876415253\n",
            "Epoch: 711 | Loss: 0.08916617184877396 | Val. Loss: 0.07372689247131348\n",
            "Epoch: 712 | Loss: 0.09223818778991699 | Val. Loss: 0.0725024938583374\n",
            "Epoch: 713 | Loss: 0.05536933243274689 | Val. Loss: 0.0728038027882576\n",
            "Epoch: 714 | Loss: 0.046462059020996094 | Val. Loss: 0.07274342328310013\n",
            "Epoch: 715 | Loss: 0.023386653512716293 | Val. Loss: 0.07197965681552887\n",
            "Epoch: 716 | Loss: 0.17357777059078217 | Val. Loss: 0.07250294834375381\n",
            "Epoch: 717 | Loss: 0.0931263267993927 | Val. Loss: 0.07310881465673447\n",
            "Epoch: 718 | Loss: 0.052349306643009186 | Val. Loss: 0.07158273458480835\n",
            "Epoch: 719 | Loss: 0.06308040022850037 | Val. Loss: 0.07070271670818329\n",
            "Epoch: 720 | Loss: 0.07113327085971832 | Val. Loss: 0.07095830142498016\n",
            "Epoch: 721 | Loss: 0.047553520649671555 | Val. Loss: 0.07115156948566437\n",
            "Epoch: 722 | Loss: 0.08575867861509323 | Val. Loss: 0.07160479575395584\n",
            "Epoch: 723 | Loss: 0.17212729156017303 | Val. Loss: 0.07074175029993057\n",
            "Epoch: 724 | Loss: 0.12467934191226959 | Val. Loss: 0.07101453840732574\n",
            "Epoch: 725 | Loss: 0.06818189471960068 | Val. Loss: 0.07145698368549347\n",
            "Epoch: 726 | Loss: 0.023107541725039482 | Val. Loss: 0.07107771933078766\n",
            "Epoch: 727 | Loss: 0.0611722432076931 | Val. Loss: 0.07087589055299759\n",
            "Epoch: 728 | Loss: 0.03093864768743515 | Val. Loss: 0.07025295495986938\n",
            "Epoch: 729 | Loss: 0.06910018622875214 | Val. Loss: 0.07084249705076218\n",
            "Epoch: 730 | Loss: 0.0971226617693901 | Val. Loss: 0.06988456845283508\n",
            "Epoch: 731 | Loss: 0.06566186994314194 | Val. Loss: 0.06968947499990463\n",
            "Epoch: 732 | Loss: 0.06198956444859505 | Val. Loss: 0.07014894485473633\n",
            "Epoch: 733 | Loss: 0.021947631612420082 | Val. Loss: 0.06985583901405334\n",
            "Epoch: 734 | Loss: 0.09738288819789886 | Val. Loss: 0.0697280615568161\n",
            "Epoch: 735 | Loss: 0.08236100524663925 | Val. Loss: 0.06982608884572983\n",
            "Epoch: 736 | Loss: 0.07464001327753067 | Val. Loss: 0.07012791931629181\n",
            "Epoch: 737 | Loss: 0.054287414997816086 | Val. Loss: 0.0695866197347641\n",
            "Epoch: 738 | Loss: 0.0465082973241806 | Val. Loss: 0.06910876929759979\n",
            "Epoch: 739 | Loss: 0.12627123296260834 | Val. Loss: 0.06973586976528168\n",
            "Epoch: 740 | Loss: 0.1131753921508789 | Val. Loss: 0.06908019632101059\n",
            "Epoch: 741 | Loss: 0.0732480064034462 | Val. Loss: 0.06917650997638702\n",
            "Epoch: 742 | Loss: 0.0553906187415123 | Val. Loss: 0.06824364513158798\n",
            "Epoch: 743 | Loss: 0.1418224722146988 | Val. Loss: 0.06830363720655441\n",
            "Epoch: 744 | Loss: 0.12099288403987885 | Val. Loss: 0.06908650696277618\n",
            "Epoch: 745 | Loss: 0.06193288415670395 | Val. Loss: 0.06776592880487442\n",
            "Epoch: 746 | Loss: 0.041078101843595505 | Val. Loss: 0.06762330234050751\n",
            "Epoch: 747 | Loss: 0.038638241589069366 | Val. Loss: 0.06793992221355438\n",
            "Epoch: 748 | Loss: 0.06416099518537521 | Val. Loss: 0.06829039752483368\n",
            "Epoch: 749 | Loss: 0.06105516105890274 | Val. Loss: 0.06839170306921005\n",
            "Epoch: 750 | Loss: 0.019227158278226852 | Val. Loss: 0.06700535118579865\n",
            "Epoch: 751 | Loss: 0.07415014505386353 | Val. Loss: 0.06684966385364532\n",
            "Epoch: 752 | Loss: 0.03438996523618698 | Val. Loss: 0.0675579309463501\n",
            "Epoch: 753 | Loss: 0.07004379481077194 | Val. Loss: 0.06732405722141266\n",
            "Epoch: 754 | Loss: 0.09267038106918335 | Val. Loss: 0.06655950844287872\n",
            "Epoch: 755 | Loss: 0.17644500732421875 | Val. Loss: 0.06637125462293625\n",
            "Epoch: 756 | Loss: 0.0611284002661705 | Val. Loss: 0.06636874377727509\n",
            "Epoch: 757 | Loss: 0.1665644496679306 | Val. Loss: 0.0661536231637001\n",
            "Epoch: 758 | Loss: 0.044433750212192535 | Val. Loss: 0.06605447828769684\n",
            "Epoch: 759 | Loss: 0.08482569456100464 | Val. Loss: 0.06586046516895294\n",
            "Epoch: 760 | Loss: 0.1444196254014969 | Val. Loss: 0.06608282774686813\n",
            "Epoch: 761 | Loss: 0.06876231729984283 | Val. Loss: 0.06543641537427902\n",
            "Epoch: 762 | Loss: 0.05805474519729614 | Val. Loss: 0.0664057657122612\n",
            "Epoch: 763 | Loss: 0.12516477704048157 | Val. Loss: 0.0655597671866417\n",
            "Epoch: 764 | Loss: 0.06054546311497688 | Val. Loss: 0.06547360122203827\n",
            "Epoch: 765 | Loss: 0.14230477809906006 | Val. Loss: 0.06595423072576523\n",
            "Epoch: 766 | Loss: 0.14300969243049622 | Val. Loss: 0.0650561973452568\n",
            "Epoch: 767 | Loss: 0.056455694139003754 | Val. Loss: 0.06503216922283173\n",
            "Epoch: 768 | Loss: 0.11902496218681335 | Val. Loss: 0.0652705654501915\n",
            "Epoch: 769 | Loss: 0.06781522184610367 | Val. Loss: 0.06526686996221542\n",
            "Epoch: 770 | Loss: 0.06860121339559555 | Val. Loss: 0.06482414156198502\n",
            "Epoch: 771 | Loss: 0.09788252413272858 | Val. Loss: 0.06499500572681427\n",
            "Epoch: 772 | Loss: 0.13771255314350128 | Val. Loss: 0.06442826986312866\n",
            "Epoch: 773 | Loss: 0.1249990239739418 | Val. Loss: 0.06502357125282288\n",
            "Epoch: 774 | Loss: 0.08916494995355606 | Val. Loss: 0.06455911695957184\n",
            "Epoch: 775 | Loss: 0.14695225656032562 | Val. Loss: 0.06428761780261993\n",
            "Epoch: 776 | Loss: 0.035934921354055405 | Val. Loss: 0.06431417167186737\n",
            "Epoch: 777 | Loss: 0.046106722205877304 | Val. Loss: 0.06441725790500641\n",
            "Epoch: 778 | Loss: 0.05413183942437172 | Val. Loss: 0.06480603665113449\n",
            "Epoch: 779 | Loss: 0.06296251714229584 | Val. Loss: 0.06545469909906387\n",
            "Epoch: 780 | Loss: 0.03925016149878502 | Val. Loss: 0.06353812664747238\n",
            "Epoch: 781 | Loss: 0.03692777827382088 | Val. Loss: 0.06363809108734131\n",
            "Epoch: 782 | Loss: 0.03161747753620148 | Val. Loss: 0.06367341428995132\n",
            "Epoch: 783 | Loss: 0.07605982571840286 | Val. Loss: 0.06317879259586334\n",
            "Epoch: 784 | Loss: 0.1471255123615265 | Val. Loss: 0.06328435242176056\n",
            "Epoch: 785 | Loss: 0.07121914625167847 | Val. Loss: 0.06414429843425751\n",
            "Epoch: 786 | Loss: 0.1116122454404831 | Val. Loss: 0.0629192441701889\n",
            "Epoch: 787 | Loss: 0.07226600497961044 | Val. Loss: 0.06399180740118027\n",
            "Epoch: 788 | Loss: 0.07101637125015259 | Val. Loss: 0.06286335736513138\n",
            "Epoch: 789 | Loss: 0.01986536756157875 | Val. Loss: 0.06304602324962616\n",
            "Epoch: 790 | Loss: 0.08708387613296509 | Val. Loss: 0.06271801888942719\n",
            "Epoch: 791 | Loss: 0.040604300796985626 | Val. Loss: 0.0628940612077713\n",
            "Epoch: 792 | Loss: 0.017443861812353134 | Val. Loss: 0.06287398189306259\n",
            "Epoch: 793 | Loss: 0.07524601370096207 | Val. Loss: 0.06222973391413689\n",
            "Epoch: 794 | Loss: 0.04262669384479523 | Val. Loss: 0.06226358935236931\n",
            "Epoch: 795 | Loss: 0.05273424834012985 | Val. Loss: 0.062503881752491\n",
            "Epoch: 796 | Loss: 0.06499461084604263 | Val. Loss: 0.06002480909228325\n",
            "Epoch: 797 | Loss: 0.05985087528824806 | Val. Loss: 0.06047837808728218\n",
            "Epoch: 798 | Loss: 0.04585253447294235 | Val. Loss: 0.06026832014322281\n",
            "Epoch: 799 | Loss: 0.02532072924077511 | Val. Loss: 0.06003955751657486\n",
            "Epoch: 800 | Loss: 0.028697961941361427 | Val. Loss: 0.06007493659853935\n",
            "Epoch: 801 | Loss: 0.06357377022504807 | Val. Loss: 0.05997816473245621\n",
            "Epoch: 802 | Loss: 0.17347383499145508 | Val. Loss: 0.05958617478609085\n",
            "Epoch: 803 | Loss: 0.04530490189790726 | Val. Loss: 0.06017305701971054\n",
            "Epoch: 804 | Loss: 0.04642796143889427 | Val. Loss: 0.05947868153452873\n",
            "Epoch: 805 | Loss: 0.062326811254024506 | Val. Loss: 0.059452205896377563\n",
            "Epoch: 806 | Loss: 0.05099750682711601 | Val. Loss: 0.05948048084974289\n",
            "Epoch: 807 | Loss: 0.08192805200815201 | Val. Loss: 0.05915096402168274\n",
            "Epoch: 808 | Loss: 0.05028185620903969 | Val. Loss: 0.05918710306286812\n",
            "Epoch: 809 | Loss: 0.1834084540605545 | Val. Loss: 0.05945463851094246\n",
            "Epoch: 810 | Loss: 0.12756462395191193 | Val. Loss: 0.05953140929341316\n",
            "Epoch: 811 | Loss: 0.052775971591472626 | Val. Loss: 0.0589950866997242\n",
            "Epoch: 812 | Loss: 0.048376429826021194 | Val. Loss: 0.059003908187150955\n",
            "Epoch: 813 | Loss: 0.051671192049980164 | Val. Loss: 0.059268154203891754\n",
            "Epoch: 814 | Loss: 0.18502327799797058 | Val. Loss: 0.05856968089938164\n",
            "Epoch: 815 | Loss: 0.05627291277050972 | Val. Loss: 0.05871213600039482\n",
            "Epoch: 816 | Loss: 0.13582950830459595 | Val. Loss: 0.05903608351945877\n",
            "Epoch: 817 | Loss: 0.06637750566005707 | Val. Loss: 0.05819504335522652\n",
            "Epoch: 818 | Loss: 0.07882751524448395 | Val. Loss: 0.05827737972140312\n",
            "Epoch: 819 | Loss: 0.04769587889313698 | Val. Loss: 0.0581701323390007\n",
            "Epoch: 820 | Loss: 0.09179138392210007 | Val. Loss: 0.058667249977588654\n",
            "Epoch: 821 | Loss: 0.04247299209237099 | Val. Loss: 0.05808647349476814\n",
            "Epoch: 822 | Loss: 0.1077573299407959 | Val. Loss: 0.05781165882945061\n",
            "Epoch: 823 | Loss: 0.17981068789958954 | Val. Loss: 0.05788537859916687\n",
            "Epoch: 824 | Loss: 0.05799103155732155 | Val. Loss: 0.05805988237261772\n",
            "Epoch: 825 | Loss: 0.15583504736423492 | Val. Loss: 0.05807845667004585\n",
            "Epoch: 826 | Loss: 0.09913968294858932 | Val. Loss: 0.05758708715438843\n",
            "Epoch: 827 | Loss: 0.040045030415058136 | Val. Loss: 0.05750756338238716\n",
            "Epoch: 828 | Loss: 0.048999857157468796 | Val. Loss: 0.058321285992860794\n",
            "Epoch: 829 | Loss: 0.09892214089632034 | Val. Loss: 0.05751710385084152\n",
            "Epoch: 830 | Loss: 0.013787060976028442 | Val. Loss: 0.057173192501068115\n",
            "Epoch: 831 | Loss: 0.14558367431163788 | Val. Loss: 0.056865446269512177\n",
            "Epoch: 832 | Loss: 0.16516388952732086 | Val. Loss: 0.05688856914639473\n",
            "Epoch: 833 | Loss: 0.0751497745513916 | Val. Loss: 0.0572676882147789\n",
            "Epoch: 834 | Loss: 0.09226443618535995 | Val. Loss: 0.05713881924748421\n",
            "Epoch: 835 | Loss: 0.18272095918655396 | Val. Loss: 0.056852810084819794\n",
            "Epoch: 836 | Loss: 0.02639925852417946 | Val. Loss: 0.056809086352586746\n",
            "Epoch: 837 | Loss: 0.021620500832796097 | Val. Loss: 0.056612778455019\n",
            "Epoch: 838 | Loss: 0.026923418045043945 | Val. Loss: 0.05663227289915085\n",
            "Epoch: 839 | Loss: 0.05942701920866966 | Val. Loss: 0.05653667449951172\n",
            "Epoch: 840 | Loss: 0.20052866637706757 | Val. Loss: 0.0563838966190815\n",
            "Epoch: 841 | Loss: 0.08145445585250854 | Val. Loss: 0.0562543161213398\n",
            "Epoch: 842 | Loss: 0.08690443634986877 | Val. Loss: 0.05628765746951103\n",
            "Epoch: 843 | Loss: 0.0613957941532135 | Val. Loss: 0.056112952530384064\n",
            "Epoch: 844 | Loss: 0.08456116914749146 | Val. Loss: 0.05598971247673035\n",
            "Epoch: 845 | Loss: 0.09531211853027344 | Val. Loss: 0.05606422945857048\n",
            "Epoch: 846 | Loss: 0.13902078568935394 | Val. Loss: 0.056053370237350464\n",
            "Epoch: 847 | Loss: 0.04325586184859276 | Val. Loss: 0.055904172360897064\n",
            "Epoch: 848 | Loss: 0.07133463025093079 | Val. Loss: 0.05566950887441635\n",
            "Epoch: 849 | Loss: 0.04632580652832985 | Val. Loss: 0.055666226893663406\n",
            "Epoch: 850 | Loss: 0.021783336997032166 | Val. Loss: 0.05548735335469246\n",
            "Epoch: 851 | Loss: 0.03461572900414467 | Val. Loss: 0.05536618083715439\n",
            "Epoch: 852 | Loss: 0.054926540702581406 | Val. Loss: 0.055488698184490204\n",
            "Epoch: 853 | Loss: 0.06010940670967102 | Val. Loss: 0.05568681284785271\n",
            "Epoch: 854 | Loss: 0.12963706254959106 | Val. Loss: 0.055413346737623215\n",
            "Epoch: 855 | Loss: 0.0283054169267416 | Val. Loss: 0.055291421711444855\n",
            "Epoch: 856 | Loss: 0.09853716194629669 | Val. Loss: 0.056446630507707596\n",
            "Epoch: 857 | Loss: 0.1812252253293991 | Val. Loss: 0.05524459481239319\n",
            "Epoch: 858 | Loss: 0.05361882597208023 | Val. Loss: 0.0549449548125267\n",
            "Epoch: 859 | Loss: 0.08528607338666916 | Val. Loss: 0.055016398429870605\n",
            "Epoch: 860 | Loss: 0.06787031143903732 | Val. Loss: 0.05489232391119003\n",
            "Epoch: 861 | Loss: 0.06699027866125107 | Val. Loss: 0.05480905622243881\n",
            "Epoch: 862 | Loss: 0.1333637833595276 | Val. Loss: 0.054539941251277924\n",
            "Epoch: 863 | Loss: 0.05158458650112152 | Val. Loss: 0.05464622378349304\n",
            "Epoch: 864 | Loss: 0.0938371941447258 | Val. Loss: 0.054563384503126144\n",
            "Epoch: 865 | Loss: 0.06500910967588425 | Val. Loss: 0.05443389341235161\n",
            "Epoch: 866 | Loss: 0.05373396724462509 | Val. Loss: 0.054381806403398514\n",
            "Epoch: 867 | Loss: 0.14944082498550415 | Val. Loss: 0.05435628816485405\n",
            "Epoch: 868 | Loss: 0.1782066524028778 | Val. Loss: 0.054377906024456024\n",
            "Epoch: 869 | Loss: 0.07440709322690964 | Val. Loss: 0.05408741161227226\n",
            "Epoch: 870 | Loss: 0.06043098121881485 | Val. Loss: 0.05433308333158493\n",
            "Epoch: 871 | Loss: 0.0732998251914978 | Val. Loss: 0.05398131161928177\n",
            "Epoch: 872 | Loss: 0.1620335727930069 | Val. Loss: 0.05403042957186699\n",
            "Epoch: 873 | Loss: 0.08995609730482101 | Val. Loss: 0.05393862724304199\n",
            "Epoch: 874 | Loss: 0.07625500857830048 | Val. Loss: 0.05396677181124687\n",
            "Epoch: 875 | Loss: 0.12742149829864502 | Val. Loss: 0.053787995129823685\n",
            "Epoch: 876 | Loss: 0.11908584833145142 | Val. Loss: 0.053693026304244995\n",
            "Epoch: 877 | Loss: 0.04518531635403633 | Val. Loss: 0.0536971278488636\n",
            "Epoch: 878 | Loss: 0.17341934144496918 | Val. Loss: 0.05344764515757561\n",
            "Epoch: 879 | Loss: 0.03269466757774353 | Val. Loss: 0.05327985808253288\n",
            "Epoch: 880 | Loss: 0.08541594445705414 | Val. Loss: 0.05357364937663078\n",
            "Epoch: 881 | Loss: 0.07107057422399521 | Val. Loss: 0.05393652245402336\n",
            "Epoch: 882 | Loss: 0.04128876328468323 | Val. Loss: 0.05312683433294296\n",
            "Epoch: 883 | Loss: 0.04713993892073631 | Val. Loss: 0.05307338759303093\n",
            "Epoch: 884 | Loss: 0.15853244066238403 | Val. Loss: 0.05301356315612793\n",
            "Epoch: 885 | Loss: 0.08846383541822433 | Val. Loss: 0.05299569293856621\n",
            "Epoch: 886 | Loss: 0.056014757603406906 | Val. Loss: 0.053044747561216354\n",
            "Epoch: 887 | Loss: 0.05693535879254341 | Val. Loss: 0.05296775698661804\n",
            "Epoch: 888 | Loss: 0.1496737003326416 | Val. Loss: 0.05269172042608261\n",
            "Epoch: 889 | Loss: 0.1203305721282959 | Val. Loss: 0.052755117416381836\n",
            "Epoch: 890 | Loss: 0.04755699262022972 | Val. Loss: 0.05280587077140808\n",
            "Epoch: 891 | Loss: 0.062190741300582886 | Val. Loss: 0.05259626358747482\n",
            "Epoch: 892 | Loss: 0.04527611657977104 | Val. Loss: 0.05285891145467758\n",
            "Epoch: 893 | Loss: 0.056583523750305176 | Val. Loss: 0.05253586173057556\n",
            "Epoch: 894 | Loss: 0.14026601612567902 | Val. Loss: 0.05253983661532402\n",
            "Epoch: 895 | Loss: 0.1428859829902649 | Val. Loss: 0.052309561520814896\n",
            "Epoch: 896 | Loss: 0.046460721641778946 | Val. Loss: 0.05223526805639267\n",
            "Epoch: 897 | Loss: 0.08330599218606949 | Val. Loss: 0.052183300256729126\n",
            "Epoch: 898 | Loss: 0.057094767689704895 | Val. Loss: 0.052020393311977386\n",
            "Epoch: 899 | Loss: 0.06858528405427933 | Val. Loss: 0.05205266550183296\n",
            "Epoch: 900 | Loss: 0.03728148713707924 | Val. Loss: 0.05208098888397217\n",
            "Epoch: 901 | Loss: 0.05460413172841072 | Val. Loss: 0.052017807960510254\n",
            "Epoch: 902 | Loss: 0.036448683589696884 | Val. Loss: 0.05201304703950882\n",
            "Epoch: 903 | Loss: 0.1416732519865036 | Val. Loss: 0.05246559903025627\n",
            "Epoch: 904 | Loss: 0.05012643709778786 | Val. Loss: 0.051888786256313324\n",
            "Epoch: 905 | Loss: 0.04994336888194084 | Val. Loss: 0.05158991739153862\n",
            "Epoch: 906 | Loss: 0.11311829835176468 | Val. Loss: 0.05161911994218826\n",
            "Epoch: 907 | Loss: 0.11582773923873901 | Val. Loss: 0.05174250155687332\n",
            "Epoch: 908 | Loss: 0.06162044778466225 | Val. Loss: 0.051666729152202606\n",
            "Epoch: 909 | Loss: 0.12255850434303284 | Val. Loss: 0.051642052829265594\n",
            "Epoch: 910 | Loss: 0.14275163412094116 | Val. Loss: 0.05151144787669182\n",
            "Epoch: 911 | Loss: 0.05628390237689018 | Val. Loss: 0.05139011889696121\n",
            "Epoch: 912 | Loss: 0.1280682384967804 | Val. Loss: 0.05136002227663994\n",
            "Epoch: 913 | Loss: 0.05354888364672661 | Val. Loss: 0.05178883671760559\n",
            "Epoch: 914 | Loss: 0.045765120536088943 | Val. Loss: 0.05185287445783615\n",
            "Epoch: 915 | Loss: 0.056026753038167953 | Val. Loss: 0.05203280970454216\n",
            "Epoch: 916 | Loss: 0.15632830560207367 | Val. Loss: 0.05200182646512985\n",
            "Epoch: 917 | Loss: 0.087473563849926 | Val. Loss: 0.05216662213206291\n",
            "Epoch: 918 | Loss: 0.0457799918949604 | Val. Loss: 0.05189412832260132\n",
            "Epoch: 919 | Loss: 0.07023734599351883 | Val. Loss: 0.05179160088300705\n",
            "Epoch: 920 | Loss: 0.016514359042048454 | Val. Loss: 0.05185557156801224\n",
            "Epoch: 921 | Loss: 0.08677023649215698 | Val. Loss: 0.05163590982556343\n",
            "Epoch: 922 | Loss: 0.034450508654117584 | Val. Loss: 0.04974495619535446\n",
            "Epoch: 923 | Loss: 0.12658725678920746 | Val. Loss: 0.049431826919317245\n",
            "Epoch: 924 | Loss: 0.053521767258644104 | Val. Loss: 0.04937591403722763\n",
            "Epoch: 925 | Loss: 0.06948063522577286 | Val. Loss: 0.0496506467461586\n",
            "Epoch: 926 | Loss: 0.04397524893283844 | Val. Loss: 0.04941076785326004\n",
            "Epoch: 927 | Loss: 0.045860908925533295 | Val. Loss: 0.05057568475604057\n",
            "Epoch: 928 | Loss: 0.07243271172046661 | Val. Loss: 0.05099586397409439\n",
            "Epoch: 929 | Loss: 0.05117160454392433 | Val. Loss: 0.0486275777220726\n",
            "Epoch: 930 | Loss: 0.17678605020046234 | Val. Loss: 0.04856078699231148\n",
            "Epoch: 931 | Loss: 0.1296868920326233 | Val. Loss: 0.04846569150686264\n",
            "Epoch: 932 | Loss: 0.006686675362288952 | Val. Loss: 0.05055929347872734\n",
            "Epoch: 933 | Loss: 0.05303405597805977 | Val. Loss: 0.04979009926319122\n",
            "Epoch: 934 | Loss: 0.06886892765760422 | Val. Loss: 0.04903024062514305\n",
            "Epoch: 935 | Loss: 0.1435966044664383 | Val. Loss: 0.04867342859506607\n",
            "Epoch: 936 | Loss: 0.06016241014003754 | Val. Loss: 0.04907617345452309\n",
            "Epoch: 937 | Loss: 0.03931626304984093 | Val. Loss: 0.04889366775751114\n",
            "Epoch: 938 | Loss: 0.01783078722655773 | Val. Loss: 0.048952750861644745\n",
            "Epoch: 939 | Loss: 0.17533887922763824 | Val. Loss: 0.05035468190908432\n",
            "Epoch: 940 | Loss: 0.09224612265825272 | Val. Loss: 0.04861084371805191\n",
            "Epoch: 941 | Loss: 0.025117365643382072 | Val. Loss: 0.04791397601366043\n",
            "Epoch: 942 | Loss: 0.06384681910276413 | Val. Loss: 0.047386284917593\n",
            "Epoch: 943 | Loss: 0.032302774488925934 | Val. Loss: 0.0474291667342186\n",
            "Epoch: 944 | Loss: 0.05467241629958153 | Val. Loss: 0.04766129329800606\n",
            "Epoch: 945 | Loss: 0.04282728582620621 | Val. Loss: 0.04968998208642006\n",
            "Epoch: 946 | Loss: 0.04161333292722702 | Val. Loss: 0.048959292471408844\n",
            "Epoch: 947 | Loss: 0.06070820242166519 | Val. Loss: 0.04837848246097565\n",
            "Epoch: 948 | Loss: 0.048654742538928986 | Val. Loss: 0.0483485646545887\n",
            "Epoch: 949 | Loss: 0.11260312050580978 | Val. Loss: 0.04821282997727394\n",
            "Epoch: 950 | Loss: 0.018209973350167274 | Val. Loss: 0.04752594605088234\n",
            "Epoch: 951 | Loss: 0.03691762685775757 | Val. Loss: 0.04811163991689682\n",
            "Epoch: 952 | Loss: 0.017132751643657684 | Val. Loss: 0.05016869306564331\n",
            "Epoch: 953 | Loss: 0.1407373994588852 | Val. Loss: 0.04938428848981857\n",
            "Epoch: 954 | Loss: 0.12664245069026947 | Val. Loss: 0.04833204299211502\n",
            "Epoch: 955 | Loss: 0.05761274695396423 | Val. Loss: 0.049017783254384995\n",
            "Epoch: 956 | Loss: 0.0668582171201706 | Val. Loss: 0.051986534148454666\n",
            "Epoch: 957 | Loss: 0.1331052929162979 | Val. Loss: 0.05257313326001167\n",
            "Epoch: 958 | Loss: 0.10468681901693344 | Val. Loss: 0.05157309025526047\n",
            "Epoch: 959 | Loss: 0.10127756744623184 | Val. Loss: 0.05188983678817749\n",
            "Epoch: 960 | Loss: 0.027152661234140396 | Val. Loss: 0.05302112177014351\n",
            "Epoch: 961 | Loss: 0.07013146579265594 | Val. Loss: 0.0532853826880455\n",
            "Epoch: 962 | Loss: 0.03702826797962189 | Val. Loss: 0.050827134400606155\n",
            "Epoch: 963 | Loss: 0.05393815413117409 | Val. Loss: 0.05079071596264839\n",
            "Epoch: 964 | Loss: 0.029352378100156784 | Val. Loss: 0.05099234730005264\n",
            "Epoch: 965 | Loss: 0.10704980045557022 | Val. Loss: 0.052104651927948\n",
            "Epoch: 966 | Loss: 0.06193435192108154 | Val. Loss: 0.05211680382490158\n",
            "Epoch: 967 | Loss: 0.028437865898013115 | Val. Loss: 0.053271710872650146\n",
            "Epoch: 968 | Loss: 0.03563620150089264 | Val. Loss: 0.05209621787071228\n",
            "Epoch: 969 | Loss: 0.033590126782655716 | Val. Loss: 0.05459132045507431\n",
            "Epoch: 970 | Loss: 0.07051139324903488 | Val. Loss: 0.05472596362233162\n",
            "Epoch: 971 | Loss: 0.050972700119018555 | Val. Loss: 0.05399135500192642\n",
            "Epoch: 972 | Loss: 0.0821235179901123 | Val. Loss: 0.05390129238367081\n",
            "Epoch: 973 | Loss: 0.024735411629080772 | Val. Loss: 0.05356254428625107\n",
            "Epoch: 974 | Loss: 0.11151372641324997 | Val. Loss: 0.05367221683263779\n",
            "Epoch: 975 | Loss: 0.1255912333726883 | Val. Loss: 0.05354272574186325\n",
            "Epoch: 976 | Loss: 0.023380819708108902 | Val. Loss: 0.05349402502179146\n",
            "Epoch: 977 | Loss: 0.03526591509580612 | Val. Loss: 0.053121306002140045\n",
            "Epoch: 978 | Loss: 0.07287599891424179 | Val. Loss: 0.05353013426065445\n",
            "Epoch: 979 | Loss: 0.0439646951854229 | Val. Loss: 0.05363859981298447\n",
            "Epoch: 980 | Loss: 0.04101180657744408 | Val. Loss: 0.053605515509843826\n",
            "Epoch: 981 | Loss: 0.05258072540163994 | Val. Loss: 0.05321980640292168\n",
            "Epoch: 982 | Loss: 0.012076622806489468 | Val. Loss: 0.053262222558259964\n",
            "Epoch: 983 | Loss: 0.12428642809391022 | Val. Loss: 0.052228160202503204\n",
            "Epoch: 984 | Loss: 0.16687960922718048 | Val. Loss: 0.05199713632464409\n",
            "Epoch: 985 | Loss: 0.10546369850635529 | Val. Loss: 0.051794152706861496\n",
            "Epoch: 986 | Loss: 0.09766271710395813 | Val. Loss: 0.051217298954725266\n",
            "Epoch: 987 | Loss: 0.046954959630966187 | Val. Loss: 0.05137603357434273\n",
            "Epoch: 988 | Loss: 0.045047350227832794 | Val. Loss: 0.051084280014038086\n",
            "Epoch: 989 | Loss: 0.02510567009449005 | Val. Loss: 0.05083965137600899\n",
            "Epoch: 990 | Loss: 0.02082710899412632 | Val. Loss: 0.052666425704956055\n",
            "Epoch: 991 | Loss: 0.058865297585725784 | Val. Loss: 0.05272536724805832\n",
            "Epoch: 992 | Loss: 0.1216830462217331 | Val. Loss: 0.05224699527025223\n",
            "Epoch: 993 | Loss: 0.056225020438432693 | Val. Loss: 0.05213798210024834\n",
            "Epoch: 994 | Loss: 0.05011265352368355 | Val. Loss: 0.052408695220947266\n",
            "Epoch: 995 | Loss: 0.10700345784425735 | Val. Loss: 0.051289092749357224\n",
            "Epoch: 996 | Loss: 0.1262970119714737 | Val. Loss: 0.051235903054475784\n",
            "Epoch: 997 | Loss: 0.015053105540573597 | Val. Loss: 0.051049210131168365\n",
            "Epoch: 998 | Loss: 0.0719328373670578 | Val. Loss: 0.05124186724424362\n",
            "Epoch: 999 | Loss: 0.0642952024936676 | Val. Loss: 0.05034442991018295\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Loss')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testando o modelo\n",
        "\n",
        "# Converte os dados de teste em arrays numpy\n",
        "x_teste_np = data_test.iloc[:, :-1].to_numpy()\n",
        "d_teste_np = data_test.iloc[:, -1].to_numpy()\n",
        "\n",
        "# Aplica a normalização no conjunto de teste\n",
        "x_teste_np_norm = (x_teste_np - media_treino) / desvio_padrao_treino\n",
        "\n",
        "# Converte os arrays numpy em tensores PyTorch\n",
        "x_teste_tensor = torch.tensor(x_teste_np_norm, dtype=torch.float32).to(device=device)\n",
        "d_teste_tensor = torch.tensor(d_teste_np, dtype=torch.long).to(device=device)\n",
        "\n",
        "# Testa o modelo com os dados de teste\n",
        "y_teste_tensor = model(x_teste_tensor)\n",
        "y_teste_np = y_teste_tensor.cpu().detach().numpy()\n",
        "\n",
        "predicoes = np.argmax(y_teste_np, axis=1)\n",
        "acuracia = np.mean(predicoes == d_teste_np)\n",
        "\n",
        "# Taxa de erros\n",
        "\n",
        "Taxa_de_erro = (1 - acuracia) * 100\n",
        "\n",
        "print(f\"Acurácia: {acuracia*100:.2f}%\")\n",
        "print(f\"Taxa de erro: {Taxa_de_erro:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-3zun2OMPZd",
        "outputId": "da91087a-73a4-4680-c68e-6f3514177006"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia: 94.22%\n",
            "Taxa de erro: 5.78%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "pB6QkHw9PF4Z",
        "outputId": "303d781e-1b5c-4633-edf6-aeb1e025eecb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        0    1    2      3      4    5    6      7    8    9   10   11   12  \\\n",
              "0    67.0  1.0  0.0  120.0  229.0  0.0  0.0  129.0  1.0  2.6  1.0  2.0  3.0   \n",
              "1    63.0  1.0  3.0  145.0  233.0  1.0  0.0  150.0  0.0  2.3  0.0  0.0  1.0   \n",
              "2    63.0  0.0  0.0  124.0  197.0  0.0  1.0  136.0  1.0  0.0  1.0  0.0  2.0   \n",
              "3    52.0  1.0  0.0  112.0  230.0  0.0  1.0  160.0  0.0  0.0  2.0  1.0  2.0   \n",
              "4    58.0  0.0  0.0  130.0  197.0  0.0  1.0  131.0  0.0  0.6  1.0  0.0  2.0   \n",
              "..    ...  ...  ...    ...    ...  ...  ...    ...  ...  ...  ...  ...  ...   \n",
              "220  59.0  1.0  1.0  140.0  221.0  0.0  1.0  164.0  1.0  0.0  2.0  0.0  2.0   \n",
              "221  60.0  1.0  0.0  125.0  258.0  0.0  0.0  141.0  1.0  2.8  1.0  1.0  3.0   \n",
              "222  47.0  1.0  0.0  110.0  275.0  0.0  0.0  118.0  1.0  1.0  1.0  1.0  2.0   \n",
              "223  50.0  0.0  0.0  110.0  254.0  0.0  0.0  159.0  0.0  0.0  2.0  0.0  2.0   \n",
              "224  54.0  1.0  0.0  120.0  188.0  0.0  1.0  113.0  0.0  1.4  1.0  1.0  3.0   \n",
              "\n",
              "      13  \n",
              "0    0.0  \n",
              "1    1.0  \n",
              "2    0.0  \n",
              "3    0.0  \n",
              "4    1.0  \n",
              "..   ...  \n",
              "220  1.0  \n",
              "221  0.0  \n",
              "222  0.0  \n",
              "223  1.0  \n",
              "224  0.0  \n",
              "\n",
              "[225 rows x 14 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-116c4db6-5221-47f7-97ec-80a54aaf15f8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>67.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>129.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>63.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>145.0</td>\n",
              "      <td>233.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>63.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>124.0</td>\n",
              "      <td>197.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>136.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>52.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>230.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>160.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>58.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>197.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>131.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220</th>\n",
              "      <td>59.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>140.0</td>\n",
              "      <td>221.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>164.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>221</th>\n",
              "      <td>60.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>125.0</td>\n",
              "      <td>258.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>141.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>222</th>\n",
              "      <td>47.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>275.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>118.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223</th>\n",
              "      <td>50.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>254.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>159.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>224</th>\n",
              "      <td>54.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>188.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>113.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>225 rows × 14 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-116c4db6-5221-47f7-97ec-80a54aaf15f8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-116c4db6-5221-47f7-97ec-80a54aaf15f8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-116c4db6-5221-47f7-97ec-80a54aaf15f8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-617d77cf-fa62-44c3-a7b7-cecd3be47523\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-617d77cf-fa62-44c3-a7b7-cecd3be47523')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-617d77cf-fa62-44c3-a7b7-cecd3be47523 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_2f1f3810-1676-447a-b376-c721cd024105\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('data_test')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_2f1f3810-1676-447a-b376-c721cd024105 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('data_test');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data_test",
              "summary": "{\n  \"name\": \"data_test\",\n  \"rows\": 225,\n  \"fields\": [\n    {\n      \"column\": \"0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8.499474773688751,\n        \"min\": 35.0,\n        \"max\": 76.0,\n        \"num_unique_values\": 37,\n        \"samples\": [\n          55.0,\n          42.0,\n          53.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4383862904819672,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0750046142388958,\n        \"min\": 0.0,\n        \"max\": 3.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          3.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 16.271752230012602,\n        \"min\": 94.0,\n        \"max\": 180.0,\n        \"num_unique_values\": 43,\n        \"samples\": [\n          152.0,\n          132.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"4\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 45.49341832739227,\n        \"min\": 131.0,\n        \"max\": 417.0,\n        \"num_unique_values\": 111,\n        \"samples\": [\n          249.0,\n          268.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"5\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.33082388735465307,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"6\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5361162513118173,\n        \"min\": 0.0,\n        \"max\": 2.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"7\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 22.799549215816615,\n        \"min\": 95.0,\n        \"max\": 194.0,\n        \"num_unique_values\": 74,\n        \"samples\": [\n          131.0,\n          142.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"8\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.47696960070847183,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"9\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.1418913577989258,\n        \"min\": 0.0,\n        \"max\": 5.6,\n        \"num_unique_values\": 35,\n        \"samples\": [\n          2.4,\n          1.4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"10\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6281428914375227,\n        \"min\": 0.0,\n        \"max\": 2.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"11\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.9614184296264536,\n        \"min\": 0.0,\n        \"max\": 4.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.0,\n          3.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"12\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6164735862465499,\n        \"min\": 0.0,\n        \"max\": 3.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"13\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5009910812500195,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_dAL6ph3K_d"
      },
      "source": [
        "# Exercício 2\n",
        "\n",
        "Aplique o PCA nos dados de entrada e obtenha uma matriz de dados transformados representando os 12 componentes principais. Mostre o valor da porcentagem de variância explicada acumulada à medida que você considera um maior número de componentes principais.\n",
        "\n",
        "## Resolução"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kz4BFhRx3K_e",
        "outputId": "3918badb-1af9-4b00-ffda-a365e333149d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "M     | Variância acumulada (%)\n",
            "------------------------------\n",
            "1     | 22.11%\n",
            "2     | 34.00%\n",
            "3     | 43.55%\n",
            "4     | 52.42%\n",
            "5     | 60.26%\n",
            "6     | 67.69%\n",
            "7     | 74.29%\n",
            "8     | 80.06%\n",
            "9     | 85.54%\n",
            "10    | 90.26%\n",
            "11    | 94.04%\n",
            "12    | 97.32%\n",
            "13    | 100.00%\n"
          ]
        }
      ],
      "source": [
        "# Vamos aplicar a PCA sobre os dados normalizados\n",
        "\n",
        "# x_treino_np_norm\n",
        "# d_treino_np\n",
        "# x_val_np_norm\n",
        "# d_val_np\n",
        "\n",
        "# D: número de características\n",
        "# N: número de exemplos\n",
        "# M: número de características no espaço projetado\n",
        "\n",
        "# P = UX (M X N)\n",
        "# U = matriz com os autovetores da matriz de covariância dos dados (M X D)\n",
        "# X = matriz com os dados normalizados (D X N)\n",
        "\n",
        "X_treino = x_treino_np_norm.T\n",
        "N_treino = X_treino.shape[1]\n",
        "\n",
        "# Matriz de covariância dos dados de treino\n",
        "S_treino = (X_treino @ X_treino.T) / N_treino # (D X D)\n",
        "\n",
        "autovalores, autovetores = np.linalg.eig(S_treino)\n",
        "\n",
        "# Ordenar os autovalores em ordem decrescente\n",
        "indices_ordenados = np.argsort(autovalores)\n",
        "indices_decrescente = indices_ordenados[::-1]\n",
        "\n",
        "autovalores = autovalores[indices_decrescente]\n",
        "autovetores = autovetores[:, indices_decrescente]\n",
        "\n",
        "# Variância explicada\n",
        "var_explicada = (autovalores / np.sum(autovalores)) * 100\n",
        "\n",
        "# Variância acumulada\n",
        "var_acumulada = np.cumsum(var_explicada)\n",
        "\n",
        "print(f\"{'M':<5} | {'Variância acumulada (%)'}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "for i, variancia in enumerate(var_acumulada):\n",
        "  M = i + 1\n",
        "  print(f\"{M:<5} | {variancia:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Projetar dados com M = 12 componentes principais\n",
        "\n",
        "M = 12\n",
        "U = autovetores[:, :M].T\n",
        "\n",
        "# Transformação linear P = UX\n",
        "P_treino = U @ X_treino\n",
        "print(\"Matriz dos dados transformados no subespaço de dimensão M=12\")\n",
        "print(P_treino)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSZM2FJCzXix",
        "outputId": "b79fc234-db66-4fed-fcbc-22e17bd8be97"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matriz dos dados transformados no subespaço de dimensão M=12\n",
            "[[ 2.60376974  0.31853518  3.81315614 ... -0.19022757 -0.77921265\n",
            "  -2.29611227]\n",
            " [-0.74405806  0.08056353  2.21133775 ...  0.15490221  0.23595092\n",
            "   0.19570328]\n",
            " [-0.79261968  1.32999366  0.97762609 ... -0.81254837 -0.55148643\n",
            "   0.06866296]\n",
            " ...\n",
            " [-0.94073426  1.02264157  0.12845426 ...  1.09006002 -1.14752368\n",
            "   0.92916888]\n",
            " [ 0.59545994 -1.66002031  0.5358125  ...  0.72415479 -0.18040992\n",
            "  -0.46054079]\n",
            " [-0.44770539 -0.16597185  0.85797342 ... -0.45465562  0.76734171\n",
            "  -0.49617354]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMRKZ9gP3K_g"
      },
      "source": [
        "# Exercício 3\n",
        "\n",
        "Implemente uma segunda rede neural para fazer a classificação usando o número de componentes principais necessário para incluir 90% da variância explicada. Calcule a acurácia obtida nos dados de teste.\n",
        "\n",
        "## Resolução"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como vimos na tabela do exercício 2, o número mínimo de componentes principais necessário para incluir 90% da variância explicada é M=10."
      ],
      "metadata": {
        "id": "6vbKHGrG0Wdl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5TYxZCc03K_h"
      },
      "outputs": [],
      "source": [
        "M = 10\n",
        "U = autovetores[:, :M].T\n",
        "\n",
        "# Transformação linear P = UX\n",
        "P_treino = U @ X_treino\n",
        "\n",
        "# Projetando também nos dados de validação\n",
        "X_val = x_val_np_norm.T\n",
        "P_val = U @ X_val\n",
        "\n",
        "# Convertendo em tensores Pytorch\n",
        "x_treino_pca = torch.tensor(P_treino.T, dtype=torch.float32)\n",
        "x_val_pca = torch.tensor(P_val.T, dtype=torch.float32)\n",
        "\n",
        "# Vou definir d_treino_tensor e d_val_tensor de novo mas é o mesmo\n",
        "d_treino_tensor = torch.tensor(d_treino_np, dtype=torch.long)\n",
        "d_val_tensor = torch.tensor(d_val_np, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelPCA(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.model = nn.Sequential(\n",
        "        nn.Linear(10, 8),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.Linear(8, 4),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.Linear(4, 4),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.Linear(4, 2),\n",
        "    )\n",
        "\n",
        "    self._init_weights()\n",
        "\n",
        "  def _init_weights(self):\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Linear):\n",
        "        nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
        "\n",
        "        # Inicializa os bias com zero\n",
        "        if m.bias is not None:\n",
        "          nn.init.constant_(m.bias, 0)\n",
        "\n",
        "  def forward(self, x):\n",
        "    output = self.model(x)\n",
        "    return output"
      ],
      "metadata": {
        "id": "Q7ylvxlY2iTK"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Aqui muda\n",
        "model_pca = ModelPCA().to(device=device)\n",
        "\n",
        "# Taxa de aprendizado\n",
        "eta = 0.001\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_pca.parameters(), lr=eta)\n",
        "\n",
        "Nb = 64 # Tamanho do mini-batch\n",
        "Ne = 1000 # Número de épocas\n"
      ],
      "metadata": {
        "id": "DWBODm3E24ES"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_set_pca = TensorDataset(x_treino_pca, d_treino_tensor)\n",
        "train_loader_pca = torch.utils.data.DataLoader(train_set_pca, batch_size=Nb, shuffle=True)"
      ],
      "metadata": {
        "id": "YICaAKhS3W02"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Treinamento\n",
        "losses = []\n",
        "val_losses = []\n",
        "\n",
        "x_val_pca = x_val_pca.to(device=device)\n",
        "d_val_tensor = d_val_tensor.to(device=device)\n",
        "\n",
        "for epoch in range(Ne):\n",
        "  for n, (X, d) in enumerate(train_loader_pca):\n",
        "\n",
        "    X = X.to(device=device)\n",
        "    d = d.to(device=device)\n",
        "\n",
        "    # Treinamento\n",
        "    model_pca.train()\n",
        "    model_pca.zero_grad()\n",
        "    y = model_pca(X)\n",
        "    loss = loss_function(y, d)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Validação\n",
        "    model_pca.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      y_val = model_pca(x_val_pca)\n",
        "      val_loss = loss_function(y_val, d_val_tensor)\n",
        "\n",
        "    losses.append(loss.item())\n",
        "    val_losses.append(val_loss.item())\n",
        "\n",
        "    if epoch % 1 == 0 and n == x_treino_pca.shape[0]//Nb - 1:\n",
        "      print(f\"Epoch: {epoch} | Loss: {loss} | Val. Loss: {val_loss}\")\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(losses)\n",
        "plt.plot(val_losses, alpha=0.8)\n",
        "plt.legend([\"Loss\", \"Val. Loss\"])\n",
        "plt.xlabel(\"Batch\")\n",
        "plt.ylabel(\"Loss\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PavHmvie3hym",
        "outputId": "5be14255-a61e-4297-f1f5-92bb7ca3239f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | Loss: 1.8583680391311646 | Val. Loss: 1.6212027072906494\n",
            "Epoch: 1 | Loss: 1.2970407009124756 | Val. Loss: 1.449122667312622\n",
            "Epoch: 2 | Loss: 1.301961898803711 | Val. Loss: 1.300028681755066\n",
            "Epoch: 3 | Loss: 1.4774218797683716 | Val. Loss: 1.1757051944732666\n",
            "Epoch: 4 | Loss: 1.1729419231414795 | Val. Loss: 1.0705630779266357\n",
            "Epoch: 5 | Loss: 1.2837003469467163 | Val. Loss: 0.9826695322990417\n",
            "Epoch: 6 | Loss: 1.0554507970809937 | Val. Loss: 0.9121969938278198\n",
            "Epoch: 7 | Loss: 0.9681206941604614 | Val. Loss: 0.8536349534988403\n",
            "Epoch: 8 | Loss: 0.9146990776062012 | Val. Loss: 0.8064015507698059\n",
            "Epoch: 9 | Loss: 0.7406601309776306 | Val. Loss: 0.7673071622848511\n",
            "Epoch: 10 | Loss: 0.6684960126876831 | Val. Loss: 0.7360856533050537\n",
            "Epoch: 11 | Loss: 0.7201763987541199 | Val. Loss: 0.7101364731788635\n",
            "Epoch: 12 | Loss: 0.7385163307189941 | Val. Loss: 0.687412679195404\n",
            "Epoch: 13 | Loss: 0.6577675342559814 | Val. Loss: 0.6675390005111694\n",
            "Epoch: 14 | Loss: 0.6953451633453369 | Val. Loss: 0.6509267687797546\n",
            "Epoch: 15 | Loss: 0.6045512557029724 | Val. Loss: 0.6354489326477051\n",
            "Epoch: 16 | Loss: 0.61836177110672 | Val. Loss: 0.6212010383605957\n",
            "Epoch: 17 | Loss: 0.630694568157196 | Val. Loss: 0.6087101697921753\n",
            "Epoch: 18 | Loss: 0.5260095596313477 | Val. Loss: 0.5967854261398315\n",
            "Epoch: 19 | Loss: 0.5491225719451904 | Val. Loss: 0.5862654447555542\n",
            "Epoch: 20 | Loss: 0.4903566837310791 | Val. Loss: 0.5764740705490112\n",
            "Epoch: 21 | Loss: 0.5062665939331055 | Val. Loss: 0.5676243305206299\n",
            "Epoch: 22 | Loss: 0.44054174423217773 | Val. Loss: 0.5587239861488342\n",
            "Epoch: 23 | Loss: 0.4997095763683319 | Val. Loss: 0.5507996082305908\n",
            "Epoch: 24 | Loss: 0.4743055999279022 | Val. Loss: 0.5432678461074829\n",
            "Epoch: 25 | Loss: 0.4276759922504425 | Val. Loss: 0.5359684824943542\n",
            "Epoch: 26 | Loss: 0.4911762475967407 | Val. Loss: 0.5289315581321716\n",
            "Epoch: 27 | Loss: 0.4089958071708679 | Val. Loss: 0.5233257412910461\n",
            "Epoch: 28 | Loss: 0.5038699507713318 | Val. Loss: 0.5175122618675232\n",
            "Epoch: 29 | Loss: 0.4102856516838074 | Val. Loss: 0.5121139287948608\n",
            "Epoch: 30 | Loss: 0.47592446208000183 | Val. Loss: 0.5073176622390747\n",
            "Epoch: 31 | Loss: 0.4090881645679474 | Val. Loss: 0.502868115901947\n",
            "Epoch: 32 | Loss: 0.41173991560935974 | Val. Loss: 0.498668909072876\n",
            "Epoch: 33 | Loss: 0.3958939015865326 | Val. Loss: 0.4948657155036926\n",
            "Epoch: 34 | Loss: 0.4292237460613251 | Val. Loss: 0.49140843749046326\n",
            "Epoch: 35 | Loss: 0.35469040274620056 | Val. Loss: 0.48818397521972656\n",
            "Epoch: 36 | Loss: 0.43377918004989624 | Val. Loss: 0.48494380712509155\n",
            "Epoch: 37 | Loss: 0.42694947123527527 | Val. Loss: 0.4821656346321106\n",
            "Epoch: 38 | Loss: 0.385842502117157 | Val. Loss: 0.4792190194129944\n",
            "Epoch: 39 | Loss: 0.48175477981567383 | Val. Loss: 0.47651153802871704\n",
            "Epoch: 40 | Loss: 0.3595970571041107 | Val. Loss: 0.473895400762558\n",
            "Epoch: 41 | Loss: 0.33611056208610535 | Val. Loss: 0.47151994705200195\n",
            "Epoch: 42 | Loss: 0.4162008762359619 | Val. Loss: 0.46919018030166626\n",
            "Epoch: 43 | Loss: 0.37482693791389465 | Val. Loss: 0.46702736616134644\n",
            "Epoch: 44 | Loss: 0.42374125123023987 | Val. Loss: 0.4649735987186432\n",
            "Epoch: 45 | Loss: 0.4893967807292938 | Val. Loss: 0.46263760328292847\n",
            "Epoch: 46 | Loss: 0.42082470655441284 | Val. Loss: 0.46031221747398376\n",
            "Epoch: 47 | Loss: 0.41069212555885315 | Val. Loss: 0.4584110379219055\n",
            "Epoch: 48 | Loss: 0.3971206843852997 | Val. Loss: 0.4566802382469177\n",
            "Epoch: 49 | Loss: 0.2875922620296478 | Val. Loss: 0.4555165767669678\n",
            "Epoch: 50 | Loss: 0.4521338641643524 | Val. Loss: 0.45345187187194824\n",
            "Epoch: 51 | Loss: 0.3620487451553345 | Val. Loss: 0.4512961804866791\n",
            "Epoch: 52 | Loss: 0.32346874475479126 | Val. Loss: 0.44965749979019165\n",
            "Epoch: 53 | Loss: 0.4087790846824646 | Val. Loss: 0.4483170509338379\n",
            "Epoch: 54 | Loss: 0.31611010432243347 | Val. Loss: 0.4453003406524658\n",
            "Epoch: 55 | Loss: 0.3150750398635864 | Val. Loss: 0.4445713460445404\n",
            "Epoch: 56 | Loss: 0.375013530254364 | Val. Loss: 0.44309115409851074\n",
            "Epoch: 57 | Loss: 0.3095625042915344 | Val. Loss: 0.44196802377700806\n",
            "Epoch: 58 | Loss: 0.24869215488433838 | Val. Loss: 0.44165071845054626\n",
            "Epoch: 59 | Loss: 0.28293198347091675 | Val. Loss: 0.4404725432395935\n",
            "Epoch: 60 | Loss: 0.3158092498779297 | Val. Loss: 0.43877559900283813\n",
            "Epoch: 61 | Loss: 0.2966912090778351 | Val. Loss: 0.4370911121368408\n",
            "Epoch: 62 | Loss: 0.2950895428657532 | Val. Loss: 0.4348753094673157\n",
            "Epoch: 63 | Loss: 0.38523563742637634 | Val. Loss: 0.4334467947483063\n",
            "Epoch: 64 | Loss: 0.46540069580078125 | Val. Loss: 0.43099480867385864\n",
            "Epoch: 65 | Loss: 0.2324398159980774 | Val. Loss: 0.4286211133003235\n",
            "Epoch: 66 | Loss: 0.18386748433113098 | Val. Loss: 0.42553550004959106\n",
            "Epoch: 67 | Loss: 0.35347744822502136 | Val. Loss: 0.423776239156723\n",
            "Epoch: 68 | Loss: 0.27427759766578674 | Val. Loss: 0.4218832850456238\n",
            "Epoch: 69 | Loss: 0.3865096867084503 | Val. Loss: 0.4196168780326843\n",
            "Epoch: 70 | Loss: 0.32859882712364197 | Val. Loss: 0.41767215728759766\n",
            "Epoch: 71 | Loss: 0.24249111115932465 | Val. Loss: 0.41488271951675415\n",
            "Epoch: 72 | Loss: 0.2491389364004135 | Val. Loss: 0.4128043055534363\n",
            "Epoch: 73 | Loss: 0.2759040892124176 | Val. Loss: 0.4117155075073242\n",
            "Epoch: 74 | Loss: 0.3052341341972351 | Val. Loss: 0.40916067361831665\n",
            "Epoch: 75 | Loss: 0.30403462052345276 | Val. Loss: 0.40763917565345764\n",
            "Epoch: 76 | Loss: 0.32023483514785767 | Val. Loss: 0.4063382148742676\n",
            "Epoch: 77 | Loss: 0.3487052619457245 | Val. Loss: 0.4055752754211426\n",
            "Epoch: 78 | Loss: 0.24778997898101807 | Val. Loss: 0.40279731154441833\n",
            "Epoch: 79 | Loss: 0.293731689453125 | Val. Loss: 0.4025264382362366\n",
            "Epoch: 80 | Loss: 0.2063131183385849 | Val. Loss: 0.40013670921325684\n",
            "Epoch: 81 | Loss: 0.29974013566970825 | Val. Loss: 0.39843398332595825\n",
            "Epoch: 82 | Loss: 0.39020660519599915 | Val. Loss: 0.3978222906589508\n",
            "Epoch: 83 | Loss: 0.25211650133132935 | Val. Loss: 0.39564818143844604\n",
            "Epoch: 84 | Loss: 0.20384638011455536 | Val. Loss: 0.3947291374206543\n",
            "Epoch: 85 | Loss: 0.21417677402496338 | Val. Loss: 0.3932780921459198\n",
            "Epoch: 86 | Loss: 0.3479516804218292 | Val. Loss: 0.3916286528110504\n",
            "Epoch: 87 | Loss: 0.3683755397796631 | Val. Loss: 0.3908384442329407\n",
            "Epoch: 88 | Loss: 0.3003567159175873 | Val. Loss: 0.3908672630786896\n",
            "Epoch: 89 | Loss: 0.21234089136123657 | Val. Loss: 0.39005857706069946\n",
            "Epoch: 90 | Loss: 0.22126425802707672 | Val. Loss: 0.3880307078361511\n",
            "Epoch: 91 | Loss: 0.22345764935016632 | Val. Loss: 0.38700103759765625\n",
            "Epoch: 92 | Loss: 0.3960326611995697 | Val. Loss: 0.38536521792411804\n",
            "Epoch: 93 | Loss: 0.29851263761520386 | Val. Loss: 0.3848991394042969\n",
            "Epoch: 94 | Loss: 0.26052623987197876 | Val. Loss: 0.3843483328819275\n",
            "Epoch: 95 | Loss: 0.2648875117301941 | Val. Loss: 0.38251012563705444\n",
            "Epoch: 96 | Loss: 0.22278417646884918 | Val. Loss: 0.38167864084243774\n",
            "Epoch: 97 | Loss: 0.2128068059682846 | Val. Loss: 0.3807438015937805\n",
            "Epoch: 98 | Loss: 0.37509647011756897 | Val. Loss: 0.37979429960250854\n",
            "Epoch: 99 | Loss: 0.19312997162342072 | Val. Loss: 0.3789997398853302\n",
            "Epoch: 100 | Loss: 0.25289037823677063 | Val. Loss: 0.3782828748226166\n",
            "Epoch: 101 | Loss: 0.30270400643348694 | Val. Loss: 0.3777024745941162\n",
            "Epoch: 102 | Loss: 0.37725141644477844 | Val. Loss: 0.37666282057762146\n",
            "Epoch: 103 | Loss: 0.3386869728565216 | Val. Loss: 0.37491554021835327\n",
            "Epoch: 104 | Loss: 0.24394254386425018 | Val. Loss: 0.3740989863872528\n",
            "Epoch: 105 | Loss: 0.18525870144367218 | Val. Loss: 0.37296250462532043\n",
            "Epoch: 106 | Loss: 0.2544919550418854 | Val. Loss: 0.3705959916114807\n",
            "Epoch: 107 | Loss: 0.21807162463665009 | Val. Loss: 0.369615763425827\n",
            "Epoch: 108 | Loss: 0.298551082611084 | Val. Loss: 0.3688243627548218\n",
            "Epoch: 109 | Loss: 0.3388119041919708 | Val. Loss: 0.3677233159542084\n",
            "Epoch: 110 | Loss: 0.22351910173892975 | Val. Loss: 0.36654120683670044\n",
            "Epoch: 111 | Loss: 0.2989619970321655 | Val. Loss: 0.36517971754074097\n",
            "Epoch: 112 | Loss: 0.16723892092704773 | Val. Loss: 0.3642410635948181\n",
            "Epoch: 113 | Loss: 0.3140981197357178 | Val. Loss: 0.3632752001285553\n",
            "Epoch: 114 | Loss: 0.31999388337135315 | Val. Loss: 0.36185508966445923\n",
            "Epoch: 115 | Loss: 0.30869919061660767 | Val. Loss: 0.3615902066230774\n",
            "Epoch: 116 | Loss: 0.38660943508148193 | Val. Loss: 0.360737144947052\n",
            "Epoch: 117 | Loss: 0.21337169408798218 | Val. Loss: 0.35920825600624084\n",
            "Epoch: 118 | Loss: 0.2189221829175949 | Val. Loss: 0.3584426939487457\n",
            "Epoch: 119 | Loss: 0.22688977420330048 | Val. Loss: 0.3582060933113098\n",
            "Epoch: 120 | Loss: 0.2247857302427292 | Val. Loss: 0.35700875520706177\n",
            "Epoch: 121 | Loss: 0.22243545949459076 | Val. Loss: 0.3565657436847687\n",
            "Epoch: 122 | Loss: 0.24800850450992584 | Val. Loss: 0.35591259598731995\n",
            "Epoch: 123 | Loss: 0.38188278675079346 | Val. Loss: 0.3552344739437103\n",
            "Epoch: 124 | Loss: 0.3051377236843109 | Val. Loss: 0.35395461320877075\n",
            "Epoch: 125 | Loss: 0.27850595116615295 | Val. Loss: 0.3526170253753662\n",
            "Epoch: 126 | Loss: 0.23635296523571014 | Val. Loss: 0.3504616916179657\n",
            "Epoch: 127 | Loss: 0.2500266134738922 | Val. Loss: 0.34948286414146423\n",
            "Epoch: 128 | Loss: 0.2002066671848297 | Val. Loss: 0.3494302034378052\n",
            "Epoch: 129 | Loss: 0.16252769529819489 | Val. Loss: 0.34804263710975647\n",
            "Epoch: 130 | Loss: 0.23388762772083282 | Val. Loss: 0.34724515676498413\n",
            "Epoch: 131 | Loss: 0.28163519501686096 | Val. Loss: 0.34555739164352417\n",
            "Epoch: 132 | Loss: 0.28110045194625854 | Val. Loss: 0.34446775913238525\n",
            "Epoch: 133 | Loss: 0.23063598573207855 | Val. Loss: 0.3429192900657654\n",
            "Epoch: 134 | Loss: 0.2586757242679596 | Val. Loss: 0.34263545274734497\n",
            "Epoch: 135 | Loss: 0.16758400201797485 | Val. Loss: 0.3411954939365387\n",
            "Epoch: 136 | Loss: 0.3405895531177521 | Val. Loss: 0.3409244418144226\n",
            "Epoch: 137 | Loss: 0.20285092294216156 | Val. Loss: 0.33987635374069214\n",
            "Epoch: 138 | Loss: 0.23935912549495697 | Val. Loss: 0.3389849066734314\n",
            "Epoch: 139 | Loss: 0.23169347643852234 | Val. Loss: 0.33777958154678345\n",
            "Epoch: 140 | Loss: 0.2274446338415146 | Val. Loss: 0.3372901678085327\n",
            "Epoch: 141 | Loss: 0.18277280032634735 | Val. Loss: 0.3369942307472229\n",
            "Epoch: 142 | Loss: 0.2623496651649475 | Val. Loss: 0.3356161415576935\n",
            "Epoch: 143 | Loss: 0.26004883646965027 | Val. Loss: 0.33498862385749817\n",
            "Epoch: 144 | Loss: 0.33347707986831665 | Val. Loss: 0.3346375823020935\n",
            "Epoch: 145 | Loss: 0.26770418882369995 | Val. Loss: 0.33325260877609253\n",
            "Epoch: 146 | Loss: 0.26678016781806946 | Val. Loss: 0.3329242169857025\n",
            "Epoch: 147 | Loss: 0.25961220264434814 | Val. Loss: 0.3319000005722046\n",
            "Epoch: 148 | Loss: 0.14269772171974182 | Val. Loss: 0.3301772475242615\n",
            "Epoch: 149 | Loss: 0.19918973743915558 | Val. Loss: 0.32993191480636597\n",
            "Epoch: 150 | Loss: 0.2663286328315735 | Val. Loss: 0.3300240933895111\n",
            "Epoch: 151 | Loss: 0.18166249990463257 | Val. Loss: 0.3296528458595276\n",
            "Epoch: 152 | Loss: 0.14944715797901154 | Val. Loss: 0.32835614681243896\n",
            "Epoch: 153 | Loss: 0.14134953916072845 | Val. Loss: 0.3272307217121124\n",
            "Epoch: 154 | Loss: 0.24373802542686462 | Val. Loss: 0.32689860463142395\n",
            "Epoch: 155 | Loss: 0.1791655570268631 | Val. Loss: 0.3262903094291687\n",
            "Epoch: 156 | Loss: 0.12406734377145767 | Val. Loss: 0.32468390464782715\n",
            "Epoch: 157 | Loss: 0.20665565133094788 | Val. Loss: 0.32350462675094604\n",
            "Epoch: 158 | Loss: 0.17864590883255005 | Val. Loss: 0.3230864703655243\n",
            "Epoch: 159 | Loss: 0.304510235786438 | Val. Loss: 0.3228565752506256\n",
            "Epoch: 160 | Loss: 0.26171010732650757 | Val. Loss: 0.32258132100105286\n",
            "Epoch: 161 | Loss: 0.1872921884059906 | Val. Loss: 0.32067978382110596\n",
            "Epoch: 162 | Loss: 0.21591924130916595 | Val. Loss: 0.3208443224430084\n",
            "Epoch: 163 | Loss: 0.31708940863609314 | Val. Loss: 0.3203006386756897\n",
            "Epoch: 164 | Loss: 0.32249051332473755 | Val. Loss: 0.31903544068336487\n",
            "Epoch: 165 | Loss: 0.2073177695274353 | Val. Loss: 0.3179483115673065\n",
            "Epoch: 166 | Loss: 0.3342251777648926 | Val. Loss: 0.31725427508354187\n",
            "Epoch: 167 | Loss: 0.1583745777606964 | Val. Loss: 0.31640517711639404\n",
            "Epoch: 168 | Loss: 0.20964249968528748 | Val. Loss: 0.31519845128059387\n",
            "Epoch: 169 | Loss: 0.20140276849269867 | Val. Loss: 0.31526851654052734\n",
            "Epoch: 170 | Loss: 0.31922024488449097 | Val. Loss: 0.31442201137542725\n",
            "Epoch: 171 | Loss: 0.17581626772880554 | Val. Loss: 0.3135439455509186\n",
            "Epoch: 172 | Loss: 0.21877352893352509 | Val. Loss: 0.31272390484809875\n",
            "Epoch: 173 | Loss: 0.10496195405721664 | Val. Loss: 0.31126633286476135\n",
            "Epoch: 174 | Loss: 0.2750891149044037 | Val. Loss: 0.3114766478538513\n",
            "Epoch: 175 | Loss: 0.13112634420394897 | Val. Loss: 0.31056052446365356\n",
            "Epoch: 176 | Loss: 0.1598408967256546 | Val. Loss: 0.3096463680267334\n",
            "Epoch: 177 | Loss: 0.14140017330646515 | Val. Loss: 0.3096616864204407\n",
            "Epoch: 178 | Loss: 0.3146522045135498 | Val. Loss: 0.30947062373161316\n",
            "Epoch: 179 | Loss: 0.2550067901611328 | Val. Loss: 0.3078796863555908\n",
            "Epoch: 180 | Loss: 0.1697457730770111 | Val. Loss: 0.307658851146698\n",
            "Epoch: 181 | Loss: 0.17439384758472443 | Val. Loss: 0.3070808947086334\n",
            "Epoch: 182 | Loss: 0.204025536775589 | Val. Loss: 0.3064069151878357\n",
            "Epoch: 183 | Loss: 0.19449496269226074 | Val. Loss: 0.30603381991386414\n",
            "Epoch: 184 | Loss: 0.18453627824783325 | Val. Loss: 0.30589550733566284\n",
            "Epoch: 185 | Loss: 0.10773274302482605 | Val. Loss: 0.3031964898109436\n",
            "Epoch: 186 | Loss: 0.17390012741088867 | Val. Loss: 0.3024211525917053\n",
            "Epoch: 187 | Loss: 0.17389346659183502 | Val. Loss: 0.30269235372543335\n",
            "Epoch: 188 | Loss: 0.1539066731929779 | Val. Loss: 0.30218052864074707\n",
            "Epoch: 189 | Loss: 0.2242136150598526 | Val. Loss: 0.3016257584095001\n",
            "Epoch: 190 | Loss: 0.1507309526205063 | Val. Loss: 0.3015121519565582\n",
            "Epoch: 191 | Loss: 0.20266252756118774 | Val. Loss: 0.3002820611000061\n",
            "Epoch: 192 | Loss: 0.18585391342639923 | Val. Loss: 0.2998254597187042\n",
            "Epoch: 193 | Loss: 0.13772250711917877 | Val. Loss: 0.2992790639400482\n",
            "Epoch: 194 | Loss: 0.21589772403240204 | Val. Loss: 0.29924479126930237\n",
            "Epoch: 195 | Loss: 0.3056551516056061 | Val. Loss: 0.29846394062042236\n",
            "Epoch: 196 | Loss: 0.11318348348140717 | Val. Loss: 0.29755499958992004\n",
            "Epoch: 197 | Loss: 0.12997321784496307 | Val. Loss: 0.297370970249176\n",
            "Epoch: 198 | Loss: 0.13736607134342194 | Val. Loss: 0.29714807868003845\n",
            "Epoch: 199 | Loss: 0.17101262509822845 | Val. Loss: 0.29501742124557495\n",
            "Epoch: 200 | Loss: 0.25011542439460754 | Val. Loss: 0.294173002243042\n",
            "Epoch: 201 | Loss: 0.19447322189807892 | Val. Loss: 0.29396191239356995\n",
            "Epoch: 202 | Loss: 0.2111765593290329 | Val. Loss: 0.29425153136253357\n",
            "Epoch: 203 | Loss: 0.22037921845912933 | Val. Loss: 0.29294854402542114\n",
            "Epoch: 204 | Loss: 0.18908795714378357 | Val. Loss: 0.2922358810901642\n",
            "Epoch: 205 | Loss: 0.23586106300354004 | Val. Loss: 0.2922973036766052\n",
            "Epoch: 206 | Loss: 0.17352506518363953 | Val. Loss: 0.29065775871276855\n",
            "Epoch: 207 | Loss: 0.2393014132976532 | Val. Loss: 0.2903030812740326\n",
            "Epoch: 208 | Loss: 0.1812790334224701 | Val. Loss: 0.2896102964878082\n",
            "Epoch: 209 | Loss: 0.25127196311950684 | Val. Loss: 0.289590984582901\n",
            "Epoch: 210 | Loss: 0.20434893667697906 | Val. Loss: 0.2888023853302002\n",
            "Epoch: 211 | Loss: 0.15395832061767578 | Val. Loss: 0.2880440950393677\n",
            "Epoch: 212 | Loss: 0.11314498633146286 | Val. Loss: 0.2875487506389618\n",
            "Epoch: 213 | Loss: 0.17119117081165314 | Val. Loss: 0.285887748003006\n",
            "Epoch: 214 | Loss: 0.13042403757572174 | Val. Loss: 0.28697454929351807\n",
            "Epoch: 215 | Loss: 0.1938675194978714 | Val. Loss: 0.28652969002723694\n",
            "Epoch: 216 | Loss: 0.20017646253108978 | Val. Loss: 0.2840016186237335\n",
            "Epoch: 217 | Loss: 0.18767280876636505 | Val. Loss: 0.28524479269981384\n",
            "Epoch: 218 | Loss: 0.13712012767791748 | Val. Loss: 0.28434285521507263\n",
            "Epoch: 219 | Loss: 0.2696433961391449 | Val. Loss: 0.2843596339225769\n",
            "Epoch: 220 | Loss: 0.14968253672122955 | Val. Loss: 0.2829122841358185\n",
            "Epoch: 221 | Loss: 0.13194157183170319 | Val. Loss: 0.28180423378944397\n",
            "Epoch: 222 | Loss: 0.13734124600887299 | Val. Loss: 0.281940221786499\n",
            "Epoch: 223 | Loss: 0.16950716078281403 | Val. Loss: 0.28139349818229675\n",
            "Epoch: 224 | Loss: 0.15553538501262665 | Val. Loss: 0.28112536668777466\n",
            "Epoch: 225 | Loss: 0.11224541813135147 | Val. Loss: 0.28044360876083374\n",
            "Epoch: 226 | Loss: 0.11560171097517014 | Val. Loss: 0.2802042067050934\n",
            "Epoch: 227 | Loss: 0.15011145174503326 | Val. Loss: 0.280037522315979\n",
            "Epoch: 228 | Loss: 0.2737729847431183 | Val. Loss: 0.27974241971969604\n",
            "Epoch: 229 | Loss: 0.18916113674640656 | Val. Loss: 0.27905720472335815\n",
            "Epoch: 230 | Loss: 0.09914818406105042 | Val. Loss: 0.27794620394706726\n",
            "Epoch: 231 | Loss: 0.12398941814899445 | Val. Loss: 0.27737316489219666\n",
            "Epoch: 232 | Loss: 0.14560623466968536 | Val. Loss: 0.2777253985404968\n",
            "Epoch: 233 | Loss: 0.15552738308906555 | Val. Loss: 0.27618223428726196\n",
            "Epoch: 234 | Loss: 0.16806548833847046 | Val. Loss: 0.2765182852745056\n",
            "Epoch: 235 | Loss: 0.27224621176719666 | Val. Loss: 0.27619773149490356\n",
            "Epoch: 236 | Loss: 0.0914081558585167 | Val. Loss: 0.27529051899909973\n",
            "Epoch: 237 | Loss: 0.10201719403266907 | Val. Loss: 0.2747870087623596\n",
            "Epoch: 238 | Loss: 0.17297063767910004 | Val. Loss: 0.2751092314720154\n",
            "Epoch: 239 | Loss: 0.11780332773923874 | Val. Loss: 0.27416807413101196\n",
            "Epoch: 240 | Loss: 0.1840798407793045 | Val. Loss: 0.2743704617023468\n",
            "Epoch: 241 | Loss: 0.2775915265083313 | Val. Loss: 0.27411288022994995\n",
            "Epoch: 242 | Loss: 0.17005108296871185 | Val. Loss: 0.2738632261753082\n",
            "Epoch: 243 | Loss: 0.1885259598493576 | Val. Loss: 0.2736275792121887\n",
            "Epoch: 244 | Loss: 0.14532767236232758 | Val. Loss: 0.27294760942459106\n",
            "Epoch: 245 | Loss: 0.1815391182899475 | Val. Loss: 0.27245521545410156\n",
            "Epoch: 246 | Loss: 0.11874589323997498 | Val. Loss: 0.2708820700645447\n",
            "Epoch: 247 | Loss: 0.11823848634958267 | Val. Loss: 0.2717928886413574\n",
            "Epoch: 248 | Loss: 0.18248195946216583 | Val. Loss: 0.27116096019744873\n",
            "Epoch: 249 | Loss: 0.09942452609539032 | Val. Loss: 0.2707538902759552\n",
            "Epoch: 250 | Loss: 0.17561477422714233 | Val. Loss: 0.26940199732780457\n",
            "Epoch: 251 | Loss: 0.1116693764925003 | Val. Loss: 0.26938480138778687\n",
            "Epoch: 252 | Loss: 0.16003327071666718 | Val. Loss: 0.26929205656051636\n",
            "Epoch: 253 | Loss: 0.2582049071788788 | Val. Loss: 0.27004900574684143\n",
            "Epoch: 254 | Loss: 0.1883048266172409 | Val. Loss: 0.26912349462509155\n",
            "Epoch: 255 | Loss: 0.13348861038684845 | Val. Loss: 0.2679206430912018\n",
            "Epoch: 256 | Loss: 0.25098052620887756 | Val. Loss: 0.26840993762016296\n",
            "Epoch: 257 | Loss: 0.1472460925579071 | Val. Loss: 0.26824015378952026\n",
            "Epoch: 258 | Loss: 0.16652649641036987 | Val. Loss: 0.2688030004501343\n",
            "Epoch: 259 | Loss: 0.2445150762796402 | Val. Loss: 0.2672414779663086\n",
            "Epoch: 260 | Loss: 0.16706405580043793 | Val. Loss: 0.2660253643989563\n",
            "Epoch: 261 | Loss: 0.10178060084581375 | Val. Loss: 0.2673639953136444\n",
            "Epoch: 262 | Loss: 0.14908656477928162 | Val. Loss: 0.26659026741981506\n",
            "Epoch: 263 | Loss: 0.10088926553726196 | Val. Loss: 0.2651662230491638\n",
            "Epoch: 264 | Loss: 0.18583905696868896 | Val. Loss: 0.2638303339481354\n",
            "Epoch: 265 | Loss: 0.21108675003051758 | Val. Loss: 0.26470428705215454\n",
            "Epoch: 266 | Loss: 0.12769867479801178 | Val. Loss: 0.26396387815475464\n",
            "Epoch: 267 | Loss: 0.16861669719219208 | Val. Loss: 0.26388850808143616\n",
            "Epoch: 268 | Loss: 0.09952974319458008 | Val. Loss: 0.2616800367832184\n",
            "Epoch: 269 | Loss: 0.12829062342643738 | Val. Loss: 0.2621228098869324\n",
            "Epoch: 270 | Loss: 0.10061164945363998 | Val. Loss: 0.2608441412448883\n",
            "Epoch: 271 | Loss: 0.12318453937768936 | Val. Loss: 0.2600941061973572\n",
            "Epoch: 272 | Loss: 0.19117240607738495 | Val. Loss: 0.2613334059715271\n",
            "Epoch: 273 | Loss: 0.2229090929031372 | Val. Loss: 0.25892913341522217\n",
            "Epoch: 274 | Loss: 0.20263975858688354 | Val. Loss: 0.2590111792087555\n",
            "Epoch: 275 | Loss: 0.16655035316944122 | Val. Loss: 0.2572751045227051\n",
            "Epoch: 276 | Loss: 0.17409676313400269 | Val. Loss: 0.25654035806655884\n",
            "Epoch: 277 | Loss: 0.14759111404418945 | Val. Loss: 0.25664645433425903\n",
            "Epoch: 278 | Loss: 0.31635817885398865 | Val. Loss: 0.2571392357349396\n",
            "Epoch: 279 | Loss: 0.15474307537078857 | Val. Loss: 0.2555507719516754\n",
            "Epoch: 280 | Loss: 0.13266141712665558 | Val. Loss: 0.2554084062576294\n",
            "Epoch: 281 | Loss: 0.12137855589389801 | Val. Loss: 0.2545333504676819\n",
            "Epoch: 282 | Loss: 0.1614273488521576 | Val. Loss: 0.25343531370162964\n",
            "Epoch: 283 | Loss: 0.09358072280883789 | Val. Loss: 0.251864492893219\n",
            "Epoch: 284 | Loss: 0.16641715168952942 | Val. Loss: 0.2542027533054352\n",
            "Epoch: 285 | Loss: 0.18492507934570312 | Val. Loss: 0.2520428001880646\n",
            "Epoch: 286 | Loss: 0.23186135292053223 | Val. Loss: 0.25189176201820374\n",
            "Epoch: 287 | Loss: 0.15858954191207886 | Val. Loss: 0.2504890263080597\n",
            "Epoch: 288 | Loss: 0.19166870415210724 | Val. Loss: 0.24930688738822937\n",
            "Epoch: 289 | Loss: 0.09907535463571548 | Val. Loss: 0.2486211061477661\n",
            "Epoch: 290 | Loss: 0.16158278286457062 | Val. Loss: 0.24667732417583466\n",
            "Epoch: 291 | Loss: 0.1352914273738861 | Val. Loss: 0.24597041308879852\n",
            "Epoch: 292 | Loss: 0.18837840855121613 | Val. Loss: 0.24501541256904602\n",
            "Epoch: 293 | Loss: 0.14962856471538544 | Val. Loss: 0.24594008922576904\n",
            "Epoch: 294 | Loss: 0.10917585343122482 | Val. Loss: 0.2443239986896515\n",
            "Epoch: 295 | Loss: 0.12400776147842407 | Val. Loss: 0.24318642914295197\n",
            "Epoch: 296 | Loss: 0.08593003451824188 | Val. Loss: 0.24317190051078796\n",
            "Epoch: 297 | Loss: 0.1276269108057022 | Val. Loss: 0.24244575202465057\n",
            "Epoch: 298 | Loss: 0.13528722524642944 | Val. Loss: 0.24194340407848358\n",
            "Epoch: 299 | Loss: 0.1958010047674179 | Val. Loss: 0.24117663502693176\n",
            "Epoch: 300 | Loss: 0.1318632811307907 | Val. Loss: 0.24155691266059875\n",
            "Epoch: 301 | Loss: 0.2346976101398468 | Val. Loss: 0.24178966879844666\n",
            "Epoch: 302 | Loss: 0.0783364325761795 | Val. Loss: 0.23299150168895721\n",
            "Epoch: 303 | Loss: 0.14427074790000916 | Val. Loss: 0.23299455642700195\n",
            "Epoch: 304 | Loss: 0.151552215218544 | Val. Loss: 0.2265630066394806\n",
            "Epoch: 305 | Loss: 0.11676467955112457 | Val. Loss: 0.23158490657806396\n",
            "Epoch: 306 | Loss: 0.14259673655033112 | Val. Loss: 0.22667011618614197\n",
            "Epoch: 307 | Loss: 0.09300442039966583 | Val. Loss: 0.22750087082386017\n",
            "Epoch: 308 | Loss: 0.1055837944149971 | Val. Loss: 0.22280970215797424\n",
            "Epoch: 309 | Loss: 0.07582979649305344 | Val. Loss: 0.2262696921825409\n",
            "Epoch: 310 | Loss: 0.24758756160736084 | Val. Loss: 0.22608813643455505\n",
            "Epoch: 311 | Loss: 0.1583145260810852 | Val. Loss: 0.22056308388710022\n",
            "Epoch: 312 | Loss: 0.2289343625307083 | Val. Loss: 0.22586853802204132\n",
            "Epoch: 313 | Loss: 0.16866271197795868 | Val. Loss: 0.2208821326494217\n",
            "Epoch: 314 | Loss: 0.07288800925016403 | Val. Loss: 0.2190266102552414\n",
            "Epoch: 315 | Loss: 0.06367845088243484 | Val. Loss: 0.21992823481559753\n",
            "Epoch: 316 | Loss: 0.09452246874570847 | Val. Loss: 0.22106552124023438\n",
            "Epoch: 317 | Loss: 0.09707294404506683 | Val. Loss: 0.21811799705028534\n",
            "Epoch: 318 | Loss: 0.1091352328658104 | Val. Loss: 0.21842820942401886\n",
            "Epoch: 319 | Loss: 0.10961194336414337 | Val. Loss: 0.22125260531902313\n",
            "Epoch: 320 | Loss: 0.13487876951694489 | Val. Loss: 0.21620619297027588\n",
            "Epoch: 321 | Loss: 0.10176060348749161 | Val. Loss: 0.21697743237018585\n",
            "Epoch: 322 | Loss: 0.1261211335659027 | Val. Loss: 0.2140362560749054\n",
            "Epoch: 323 | Loss: 0.11928573250770569 | Val. Loss: 0.21432587504386902\n",
            "Epoch: 324 | Loss: 0.069419264793396 | Val. Loss: 0.21603897213935852\n",
            "Epoch: 325 | Loss: 0.12152637541294098 | Val. Loss: 0.21535539627075195\n",
            "Epoch: 326 | Loss: 0.12333901971578598 | Val. Loss: 0.2128315269947052\n",
            "Epoch: 327 | Loss: 0.10597828030586243 | Val. Loss: 0.21250030398368835\n",
            "Epoch: 328 | Loss: 0.13776461780071259 | Val. Loss: 0.21023735404014587\n",
            "Epoch: 329 | Loss: 0.1532997190952301 | Val. Loss: 0.21144941449165344\n",
            "Epoch: 330 | Loss: 0.11768460273742676 | Val. Loss: 0.21055376529693604\n",
            "Epoch: 331 | Loss: 0.16727477312088013 | Val. Loss: 0.21224245429039001\n",
            "Epoch: 332 | Loss: 0.11856681108474731 | Val. Loss: 0.20601901412010193\n",
            "Epoch: 333 | Loss: 0.19372165203094482 | Val. Loss: 0.20815470814704895\n",
            "Epoch: 334 | Loss: 0.15834636986255646 | Val. Loss: 0.2089805155992508\n",
            "Epoch: 335 | Loss: 0.07680340856313705 | Val. Loss: 0.20564451813697815\n",
            "Epoch: 336 | Loss: 0.23185819387435913 | Val. Loss: 0.2066657543182373\n",
            "Epoch: 337 | Loss: 0.22455759346485138 | Val. Loss: 0.20796211063861847\n",
            "Epoch: 338 | Loss: 0.22584301233291626 | Val. Loss: 0.20540761947631836\n",
            "Epoch: 339 | Loss: 0.13423681259155273 | Val. Loss: 0.20231838524341583\n",
            "Epoch: 340 | Loss: 0.1915394514799118 | Val. Loss: 0.2049531191587448\n",
            "Epoch: 341 | Loss: 0.13111352920532227 | Val. Loss: 0.20199362933635712\n",
            "Epoch: 342 | Loss: 0.11247367411851883 | Val. Loss: 0.20339587330818176\n",
            "Epoch: 343 | Loss: 0.23100018501281738 | Val. Loss: 0.20574143528938293\n",
            "Epoch: 344 | Loss: 0.22595281898975372 | Val. Loss: 0.20088651776313782\n",
            "Epoch: 345 | Loss: 0.10364784300327301 | Val. Loss: 0.20154082775115967\n",
            "Epoch: 346 | Loss: 0.11105936020612717 | Val. Loss: 0.20133905112743378\n",
            "Epoch: 347 | Loss: 0.1825077384710312 | Val. Loss: 0.20076043903827667\n",
            "Epoch: 348 | Loss: 0.14145424962043762 | Val. Loss: 0.20212583243846893\n",
            "Epoch: 349 | Loss: 0.13746623694896698 | Val. Loss: 0.1984660029411316\n",
            "Epoch: 350 | Loss: 0.07271861284971237 | Val. Loss: 0.19803020358085632\n",
            "Epoch: 351 | Loss: 0.10710673034191132 | Val. Loss: 0.20005469024181366\n",
            "Epoch: 352 | Loss: 0.08529425412416458 | Val. Loss: 0.20025715231895447\n",
            "Epoch: 353 | Loss: 0.15357042849063873 | Val. Loss: 0.1951424926519394\n",
            "Epoch: 354 | Loss: 0.13766570389270782 | Val. Loss: 0.1985914260149002\n",
            "Epoch: 355 | Loss: 0.13120166957378387 | Val. Loss: 0.19683590531349182\n",
            "Epoch: 356 | Loss: 0.1515592634677887 | Val. Loss: 0.19521334767341614\n",
            "Epoch: 357 | Loss: 0.2367209941148758 | Val. Loss: 0.1944447010755539\n",
            "Epoch: 358 | Loss: 0.2657034397125244 | Val. Loss: 0.19564023613929749\n",
            "Epoch: 359 | Loss: 0.060841742902994156 | Val. Loss: 0.1949693262577057\n",
            "Epoch: 360 | Loss: 0.08576369285583496 | Val. Loss: 0.19459207355976105\n",
            "Epoch: 361 | Loss: 0.13438458740711212 | Val. Loss: 0.19264954328536987\n",
            "Epoch: 362 | Loss: 0.15979188680648804 | Val. Loss: 0.19438877701759338\n",
            "Epoch: 363 | Loss: 0.173923522233963 | Val. Loss: 0.1936837136745453\n",
            "Epoch: 364 | Loss: 0.07618460804224014 | Val. Loss: 0.19217172265052795\n",
            "Epoch: 365 | Loss: 0.09545541554689407 | Val. Loss: 0.19522994756698608\n",
            "Epoch: 366 | Loss: 0.19158822298049927 | Val. Loss: 0.19151359796524048\n",
            "Epoch: 367 | Loss: 0.07209731638431549 | Val. Loss: 0.19022798538208008\n",
            "Epoch: 368 | Loss: 0.06975352019071579 | Val. Loss: 0.18979862332344055\n",
            "Epoch: 369 | Loss: 0.08836303651332855 | Val. Loss: 0.19106847047805786\n",
            "Epoch: 370 | Loss: 0.08681043237447739 | Val. Loss: 0.1900714933872223\n",
            "Epoch: 371 | Loss: 0.1718037724494934 | Val. Loss: 0.18960337340831757\n",
            "Epoch: 372 | Loss: 0.08149128407239914 | Val. Loss: 0.19063732028007507\n",
            "Epoch: 373 | Loss: 0.10363084822893143 | Val. Loss: 0.18956783413887024\n",
            "Epoch: 374 | Loss: 0.1195569857954979 | Val. Loss: 0.18805891275405884\n",
            "Epoch: 375 | Loss: 0.14049963653087616 | Val. Loss: 0.18794330954551697\n",
            "Epoch: 376 | Loss: 0.10537834465503693 | Val. Loss: 0.1893366426229477\n",
            "Epoch: 377 | Loss: 0.08411131799221039 | Val. Loss: 0.18902505934238434\n",
            "Epoch: 378 | Loss: 0.1269240379333496 | Val. Loss: 0.18674632906913757\n",
            "Epoch: 379 | Loss: 0.07945679873228073 | Val. Loss: 0.18562133610248566\n",
            "Epoch: 380 | Loss: 0.13800927996635437 | Val. Loss: 0.18886922299861908\n",
            "Epoch: 381 | Loss: 0.16137102246284485 | Val. Loss: 0.18670034408569336\n",
            "Epoch: 382 | Loss: 0.10136104375123978 | Val. Loss: 0.1871831715106964\n",
            "Epoch: 383 | Loss: 0.08375121653079987 | Val. Loss: 0.18498986959457397\n",
            "Epoch: 384 | Loss: 0.14748238027095795 | Val. Loss: 0.18613888323307037\n",
            "Epoch: 385 | Loss: 0.05566738545894623 | Val. Loss: 0.18470902740955353\n",
            "Epoch: 386 | Loss: 0.0884135514497757 | Val. Loss: 0.1847381889820099\n",
            "Epoch: 387 | Loss: 0.17667055130004883 | Val. Loss: 0.18658490478992462\n",
            "Epoch: 388 | Loss: 0.08709891885519028 | Val. Loss: 0.18513229489326477\n",
            "Epoch: 389 | Loss: 0.09906166791915894 | Val. Loss: 0.18261995911598206\n",
            "Epoch: 390 | Loss: 0.09134898334741592 | Val. Loss: 0.18354319036006927\n",
            "Epoch: 391 | Loss: 0.22265475988388062 | Val. Loss: 0.18400238454341888\n",
            "Epoch: 392 | Loss: 0.16325299441814423 | Val. Loss: 0.18268199265003204\n",
            "Epoch: 393 | Loss: 0.07919984310865402 | Val. Loss: 0.18144816160202026\n",
            "Epoch: 394 | Loss: 0.05852356180548668 | Val. Loss: 0.1828901469707489\n",
            "Epoch: 395 | Loss: 0.07967915385961533 | Val. Loss: 0.18313175439834595\n",
            "Epoch: 396 | Loss: 0.12930141389369965 | Val. Loss: 0.1805148422718048\n",
            "Epoch: 397 | Loss: 0.14826489984989166 | Val. Loss: 0.1809234321117401\n",
            "Epoch: 398 | Loss: 0.053722843527793884 | Val. Loss: 0.1816595047712326\n",
            "Epoch: 399 | Loss: 0.12033278495073318 | Val. Loss: 0.1814606934785843\n",
            "Epoch: 400 | Loss: 0.18165001273155212 | Val. Loss: 0.18049070239067078\n",
            "Epoch: 401 | Loss: 0.18051393330097198 | Val. Loss: 0.17944668233394623\n",
            "Epoch: 402 | Loss: 0.14621227979660034 | Val. Loss: 0.1796865463256836\n",
            "Epoch: 403 | Loss: 0.08734500408172607 | Val. Loss: 0.17909657955169678\n",
            "Epoch: 404 | Loss: 0.1178303062915802 | Val. Loss: 0.17790904641151428\n",
            "Epoch: 405 | Loss: 0.09618362784385681 | Val. Loss: 0.17885872721672058\n",
            "Epoch: 406 | Loss: 0.1731521189212799 | Val. Loss: 0.17916825413703918\n",
            "Epoch: 407 | Loss: 0.1472117304801941 | Val. Loss: 0.17834845185279846\n",
            "Epoch: 408 | Loss: 0.1371634155511856 | Val. Loss: 0.1774606555700302\n",
            "Epoch: 409 | Loss: 0.11087044328451157 | Val. Loss: 0.17770910263061523\n",
            "Epoch: 410 | Loss: 0.11798140406608582 | Val. Loss: 0.17690198123455048\n",
            "Epoch: 411 | Loss: 0.1440609246492386 | Val. Loss: 0.177718386054039\n",
            "Epoch: 412 | Loss: 0.036704566329717636 | Val. Loss: 0.1759064644575119\n",
            "Epoch: 413 | Loss: 0.09550011903047562 | Val. Loss: 0.1756175458431244\n",
            "Epoch: 414 | Loss: 0.06867232918739319 | Val. Loss: 0.17854224145412445\n",
            "Epoch: 415 | Loss: 0.07625982165336609 | Val. Loss: 0.17458614706993103\n",
            "Epoch: 416 | Loss: 0.1429079920053482 | Val. Loss: 0.17427876591682434\n",
            "Epoch: 417 | Loss: 0.04754704236984253 | Val. Loss: 0.17485545575618744\n",
            "Epoch: 418 | Loss: 0.14666391909122467 | Val. Loss: 0.17540627717971802\n",
            "Epoch: 419 | Loss: 0.20164227485656738 | Val. Loss: 0.17413979768753052\n",
            "Epoch: 420 | Loss: 0.11471811681985855 | Val. Loss: 0.17411869764328003\n",
            "Epoch: 421 | Loss: 0.1303596794605255 | Val. Loss: 0.1722024381160736\n",
            "Epoch: 422 | Loss: 0.060623884201049805 | Val. Loss: 0.1737145334482193\n",
            "Epoch: 423 | Loss: 0.07223091274499893 | Val. Loss: 0.173478364944458\n",
            "Epoch: 424 | Loss: 0.18214116990566254 | Val. Loss: 0.1722065508365631\n",
            "Epoch: 425 | Loss: 0.12775000929832458 | Val. Loss: 0.1741950362920761\n",
            "Epoch: 426 | Loss: 0.15054886043071747 | Val. Loss: 0.17249718308448792\n",
            "Epoch: 427 | Loss: 0.0723050981760025 | Val. Loss: 0.17163203656673431\n",
            "Epoch: 428 | Loss: 0.07116596400737762 | Val. Loss: 0.17127546668052673\n",
            "Epoch: 429 | Loss: 0.1191696971654892 | Val. Loss: 0.1711445152759552\n",
            "Epoch: 430 | Loss: 0.08409988880157471 | Val. Loss: 0.16990703344345093\n",
            "Epoch: 431 | Loss: 0.1764344573020935 | Val. Loss: 0.1710815727710724\n",
            "Epoch: 432 | Loss: 0.10187777876853943 | Val. Loss: 0.17132946848869324\n",
            "Epoch: 433 | Loss: 0.05141260474920273 | Val. Loss: 0.17009314894676208\n",
            "Epoch: 434 | Loss: 0.07616608589887619 | Val. Loss: 0.16863206028938293\n",
            "Epoch: 435 | Loss: 0.05705210566520691 | Val. Loss: 0.1711246818304062\n",
            "Epoch: 436 | Loss: 0.07189477980136871 | Val. Loss: 0.16975495219230652\n",
            "Epoch: 437 | Loss: 0.21024572849273682 | Val. Loss: 0.16950109601020813\n",
            "Epoch: 438 | Loss: 0.10735010355710983 | Val. Loss: 0.1682126224040985\n",
            "Epoch: 439 | Loss: 0.23393401503562927 | Val. Loss: 0.1683204174041748\n",
            "Epoch: 440 | Loss: 0.10732322931289673 | Val. Loss: 0.1682337373495102\n",
            "Epoch: 441 | Loss: 0.07313043624162674 | Val. Loss: 0.1666449010372162\n",
            "Epoch: 442 | Loss: 0.08528078347444534 | Val. Loss: 0.16736693680286407\n",
            "Epoch: 443 | Loss: 0.12747123837471008 | Val. Loss: 0.1668395698070526\n",
            "Epoch: 444 | Loss: 0.12392949312925339 | Val. Loss: 0.16705170273780823\n",
            "Epoch: 445 | Loss: 0.0555790513753891 | Val. Loss: 0.1662219613790512\n",
            "Epoch: 446 | Loss: 0.12587356567382812 | Val. Loss: 0.1666354238986969\n",
            "Epoch: 447 | Loss: 0.07597729563713074 | Val. Loss: 0.16570068895816803\n",
            "Epoch: 448 | Loss: 0.10402332246303558 | Val. Loss: 0.16635587811470032\n",
            "Epoch: 449 | Loss: 0.05645814538002014 | Val. Loss: 0.16522708535194397\n",
            "Epoch: 450 | Loss: 0.09314820915460587 | Val. Loss: 0.16440337896347046\n",
            "Epoch: 451 | Loss: 0.1426418572664261 | Val. Loss: 0.16630315780639648\n",
            "Epoch: 452 | Loss: 0.13391044735908508 | Val. Loss: 0.16584859788417816\n",
            "Epoch: 453 | Loss: 0.17895255982875824 | Val. Loss: 0.16263560950756073\n",
            "Epoch: 454 | Loss: 0.0350201278924942 | Val. Loss: 0.164068803191185\n",
            "Epoch: 455 | Loss: 0.13930146396160126 | Val. Loss: 0.16455335915088654\n",
            "Epoch: 456 | Loss: 0.10793069750070572 | Val. Loss: 0.16324521601200104\n",
            "Epoch: 457 | Loss: 0.12009114772081375 | Val. Loss: 0.16309770941734314\n",
            "Epoch: 458 | Loss: 0.06854569166898727 | Val. Loss: 0.16509132087230682\n",
            "Epoch: 459 | Loss: 0.12463276088237762 | Val. Loss: 0.16453972458839417\n",
            "Epoch: 460 | Loss: 0.08212624490261078 | Val. Loss: 0.16189377009868622\n",
            "Epoch: 461 | Loss: 0.18449392914772034 | Val. Loss: 0.16297128796577454\n",
            "Epoch: 462 | Loss: 0.10176711529493332 | Val. Loss: 0.16298052668571472\n",
            "Epoch: 463 | Loss: 0.09136288613080978 | Val. Loss: 0.16107624769210815\n",
            "Epoch: 464 | Loss: 0.08583668619394302 | Val. Loss: 0.16256003081798553\n",
            "Epoch: 465 | Loss: 0.06275017559528351 | Val. Loss: 0.16115348041057587\n",
            "Epoch: 466 | Loss: 0.04920695722103119 | Val. Loss: 0.161030575633049\n",
            "Epoch: 467 | Loss: 0.14710327982902527 | Val. Loss: 0.1614225208759308\n",
            "Epoch: 468 | Loss: 0.10479538142681122 | Val. Loss: 0.1617375612258911\n",
            "Epoch: 469 | Loss: 0.049061499536037445 | Val. Loss: 0.15929198265075684\n",
            "Epoch: 470 | Loss: 0.09686712920665741 | Val. Loss: 0.16127876937389374\n",
            "Epoch: 471 | Loss: 0.12793688476085663 | Val. Loss: 0.16106048226356506\n",
            "Epoch: 472 | Loss: 0.12738348543643951 | Val. Loss: 0.15992555022239685\n",
            "Epoch: 473 | Loss: 0.13882634043693542 | Val. Loss: 0.1598174124956131\n",
            "Epoch: 474 | Loss: 0.1612384021282196 | Val. Loss: 0.16029788553714752\n",
            "Epoch: 475 | Loss: 0.1365853250026703 | Val. Loss: 0.15930745005607605\n",
            "Epoch: 476 | Loss: 0.08571120351552963 | Val. Loss: 0.15856316685676575\n",
            "Epoch: 477 | Loss: 0.20553864538669586 | Val. Loss: 0.1589205414056778\n",
            "Epoch: 478 | Loss: 0.10162201523780823 | Val. Loss: 0.15979807078838348\n",
            "Epoch: 479 | Loss: 0.1777653694152832 | Val. Loss: 0.15768496692180634\n",
            "Epoch: 480 | Loss: 0.07585489004850388 | Val. Loss: 0.15848343074321747\n",
            "Epoch: 481 | Loss: 0.08706196397542953 | Val. Loss: 0.15762022137641907\n",
            "Epoch: 482 | Loss: 0.08987659960985184 | Val. Loss: 0.1572847068309784\n",
            "Epoch: 483 | Loss: 0.1544237732887268 | Val. Loss: 0.15777845680713654\n",
            "Epoch: 484 | Loss: 0.06987825036048889 | Val. Loss: 0.15887635946273804\n",
            "Epoch: 485 | Loss: 0.14137473702430725 | Val. Loss: 0.15773215889930725\n",
            "Epoch: 486 | Loss: 0.03258995711803436 | Val. Loss: 0.1575591266155243\n",
            "Epoch: 487 | Loss: 0.17862942814826965 | Val. Loss: 0.15794825553894043\n",
            "Epoch: 488 | Loss: 0.08436337858438492 | Val. Loss: 0.15890225768089294\n",
            "Epoch: 489 | Loss: 0.07923627644777298 | Val. Loss: 0.15736421942710876\n",
            "Epoch: 490 | Loss: 0.03188376501202583 | Val. Loss: 0.15698674321174622\n",
            "Epoch: 491 | Loss: 0.12307441979646683 | Val. Loss: 0.1574709415435791\n",
            "Epoch: 492 | Loss: 0.13589434325695038 | Val. Loss: 0.15649035573005676\n",
            "Epoch: 493 | Loss: 0.04523003101348877 | Val. Loss: 0.15735307335853577\n",
            "Epoch: 494 | Loss: 0.09365459531545639 | Val. Loss: 0.15594376623630524\n",
            "Epoch: 495 | Loss: 0.040759164839982986 | Val. Loss: 0.15683892369270325\n",
            "Epoch: 496 | Loss: 0.06627960503101349 | Val. Loss: 0.15532568097114563\n",
            "Epoch: 497 | Loss: 0.10971485823392868 | Val. Loss: 0.15765860676765442\n",
            "Epoch: 498 | Loss: 0.09859837591648102 | Val. Loss: 0.15617461502552032\n",
            "Epoch: 499 | Loss: 0.0820966437458992 | Val. Loss: 0.15457868576049805\n",
            "Epoch: 500 | Loss: 0.11700655519962311 | Val. Loss: 0.1571236103773117\n",
            "Epoch: 501 | Loss: 0.0875929743051529 | Val. Loss: 0.1547805815935135\n",
            "Epoch: 502 | Loss: 0.07652343809604645 | Val. Loss: 0.15691527724266052\n",
            "Epoch: 503 | Loss: 0.05544276162981987 | Val. Loss: 0.15581002831459045\n",
            "Epoch: 504 | Loss: 0.054217372089624405 | Val. Loss: 0.15514646470546722\n",
            "Epoch: 505 | Loss: 0.11288642883300781 | Val. Loss: 0.15387506783008575\n",
            "Epoch: 506 | Loss: 0.0591866597533226 | Val. Loss: 0.15527205169200897\n",
            "Epoch: 507 | Loss: 0.16179102659225464 | Val. Loss: 0.1551072597503662\n",
            "Epoch: 508 | Loss: 0.10998386144638062 | Val. Loss: 0.1555788666009903\n",
            "Epoch: 509 | Loss: 0.061213765293359756 | Val. Loss: 0.15383349359035492\n",
            "Epoch: 510 | Loss: 0.1253746747970581 | Val. Loss: 0.15408720076084137\n",
            "Epoch: 511 | Loss: 0.11780686676502228 | Val. Loss: 0.15471288561820984\n",
            "Epoch: 512 | Loss: 0.11017593741416931 | Val. Loss: 0.15402624011039734\n",
            "Epoch: 513 | Loss: 0.14447662234306335 | Val. Loss: 0.15456035733222961\n",
            "Epoch: 514 | Loss: 0.09939154982566833 | Val. Loss: 0.15260890126228333\n",
            "Epoch: 515 | Loss: 0.09244512021541595 | Val. Loss: 0.15281814336776733\n",
            "Epoch: 516 | Loss: 0.1148601621389389 | Val. Loss: 0.15245439112186432\n",
            "Epoch: 517 | Loss: 0.06451049447059631 | Val. Loss: 0.1526581346988678\n",
            "Epoch: 518 | Loss: 0.050680190324783325 | Val. Loss: 0.15272486209869385\n",
            "Epoch: 519 | Loss: 0.07758025825023651 | Val. Loss: 0.15144190192222595\n",
            "Epoch: 520 | Loss: 0.12822213768959045 | Val. Loss: 0.15253940224647522\n",
            "Epoch: 521 | Loss: 0.09840825200080872 | Val. Loss: 0.15168152749538422\n",
            "Epoch: 522 | Loss: 0.17871730029582977 | Val. Loss: 0.15263812243938446\n",
            "Epoch: 523 | Loss: 0.1331278383731842 | Val. Loss: 0.1509796380996704\n",
            "Epoch: 524 | Loss: 0.0623367615044117 | Val. Loss: 0.15227563679218292\n",
            "Epoch: 525 | Loss: 0.11987880617380142 | Val. Loss: 0.15238869190216064\n",
            "Epoch: 526 | Loss: 0.06627833843231201 | Val. Loss: 0.15121674537658691\n",
            "Epoch: 527 | Loss: 0.07232946157455444 | Val. Loss: 0.14996623992919922\n",
            "Epoch: 528 | Loss: 0.14753638207912445 | Val. Loss: 0.15271471440792084\n",
            "Epoch: 529 | Loss: 0.07327835261821747 | Val. Loss: 0.15163694322109222\n",
            "Epoch: 530 | Loss: 0.09899454563856125 | Val. Loss: 0.1520034372806549\n",
            "Epoch: 531 | Loss: 0.11317178606987 | Val. Loss: 0.15016598999500275\n",
            "Epoch: 532 | Loss: 0.21382799744606018 | Val. Loss: 0.1493978351354599\n",
            "Epoch: 533 | Loss: 0.1467648148536682 | Val. Loss: 0.1508600413799286\n",
            "Epoch: 534 | Loss: 0.04783817008137703 | Val. Loss: 0.14955228567123413\n",
            "Epoch: 535 | Loss: 0.1184990182518959 | Val. Loss: 0.1502244770526886\n",
            "Epoch: 536 | Loss: 0.07313378900289536 | Val. Loss: 0.15049339830875397\n",
            "Epoch: 537 | Loss: 0.08441396057605743 | Val. Loss: 0.1500580608844757\n",
            "Epoch: 538 | Loss: 0.08629995584487915 | Val. Loss: 0.15022055804729462\n",
            "Epoch: 539 | Loss: 0.09952615201473236 | Val. Loss: 0.14948438107967377\n",
            "Epoch: 540 | Loss: 0.12995117902755737 | Val. Loss: 0.14947432279586792\n",
            "Epoch: 541 | Loss: 0.07944068312644958 | Val. Loss: 0.1497534215450287\n",
            "Epoch: 542 | Loss: 0.10941808670759201 | Val. Loss: 0.1485484093427658\n",
            "Epoch: 543 | Loss: 0.1775241643190384 | Val. Loss: 0.15004947781562805\n",
            "Epoch: 544 | Loss: 0.06078971177339554 | Val. Loss: 0.1483384519815445\n",
            "Epoch: 545 | Loss: 0.0808018371462822 | Val. Loss: 0.14882151782512665\n",
            "Epoch: 546 | Loss: 0.16239742934703827 | Val. Loss: 0.14903689920902252\n",
            "Epoch: 547 | Loss: 0.10577018558979034 | Val. Loss: 0.14842386543750763\n",
            "Epoch: 548 | Loss: 0.03339900076389313 | Val. Loss: 0.14865742623806\n",
            "Epoch: 549 | Loss: 0.04168880358338356 | Val. Loss: 0.1490948349237442\n",
            "Epoch: 550 | Loss: 0.10326599329710007 | Val. Loss: 0.1478516310453415\n",
            "Epoch: 551 | Loss: 0.15826144814491272 | Val. Loss: 0.1480553150177002\n",
            "Epoch: 552 | Loss: 0.03598588705062866 | Val. Loss: 0.14751727879047394\n",
            "Epoch: 553 | Loss: 0.12376488000154495 | Val. Loss: 0.14917507767677307\n",
            "Epoch: 554 | Loss: 0.10645681619644165 | Val. Loss: 0.14830951392650604\n",
            "Epoch: 555 | Loss: 0.05547870323061943 | Val. Loss: 0.148765429854393\n",
            "Epoch: 556 | Loss: 0.04638750106096268 | Val. Loss: 0.14996370673179626\n",
            "Epoch: 557 | Loss: 0.09019577503204346 | Val. Loss: 0.1477106511592865\n",
            "Epoch: 558 | Loss: 0.0346287302672863 | Val. Loss: 0.1475999653339386\n",
            "Epoch: 559 | Loss: 0.08944379538297653 | Val. Loss: 0.1470373570919037\n",
            "Epoch: 560 | Loss: 0.16920205950737 | Val. Loss: 0.148803249001503\n",
            "Epoch: 561 | Loss: 0.16491670906543732 | Val. Loss: 0.14743250608444214\n",
            "Epoch: 562 | Loss: 0.04165269806981087 | Val. Loss: 0.14778777956962585\n",
            "Epoch: 563 | Loss: 0.1001436784863472 | Val. Loss: 0.14746461808681488\n",
            "Epoch: 564 | Loss: 0.090501569211483 | Val. Loss: 0.14842180907726288\n",
            "Epoch: 565 | Loss: 0.06306476891040802 | Val. Loss: 0.14713962376117706\n",
            "Epoch: 566 | Loss: 0.03197836875915527 | Val. Loss: 0.14766769111156464\n",
            "Epoch: 567 | Loss: 0.03732232749462128 | Val. Loss: 0.14794591069221497\n",
            "Epoch: 568 | Loss: 0.021387457847595215 | Val. Loss: 0.1491488814353943\n",
            "Epoch: 569 | Loss: 0.07044637203216553 | Val. Loss: 0.14770051836967468\n",
            "Epoch: 570 | Loss: 0.046443380415439606 | Val. Loss: 0.1486685574054718\n",
            "Epoch: 571 | Loss: 0.05316099151968956 | Val. Loss: 0.1487889587879181\n",
            "Epoch: 572 | Loss: 0.04645153880119324 | Val. Loss: 0.14863252639770508\n",
            "Epoch: 573 | Loss: 0.09124596416950226 | Val. Loss: 0.1499515175819397\n",
            "Epoch: 574 | Loss: 0.1328817903995514 | Val. Loss: 0.1500680148601532\n",
            "Epoch: 575 | Loss: 0.08287239819765091 | Val. Loss: 0.14988741278648376\n",
            "Epoch: 576 | Loss: 0.05718536674976349 | Val. Loss: 0.15052485466003418\n",
            "Epoch: 577 | Loss: 0.0725976750254631 | Val. Loss: 0.1515323519706726\n",
            "Epoch: 578 | Loss: 0.17292889952659607 | Val. Loss: 0.1501345932483673\n",
            "Epoch: 579 | Loss: 0.05161150544881821 | Val. Loss: 0.15164266526699066\n",
            "Epoch: 580 | Loss: 0.11119373142719269 | Val. Loss: 0.15058013796806335\n",
            "Epoch: 581 | Loss: 0.08628520369529724 | Val. Loss: 0.14980916678905487\n",
            "Epoch: 582 | Loss: 0.06869655847549438 | Val. Loss: 0.14901436865329742\n",
            "Epoch: 583 | Loss: 0.040228575468063354 | Val. Loss: 0.14889749884605408\n",
            "Epoch: 584 | Loss: 0.05819747596979141 | Val. Loss: 0.15011517703533173\n",
            "Epoch: 585 | Loss: 0.09202250093221664 | Val. Loss: 0.1496516317129135\n",
            "Epoch: 586 | Loss: 0.1032605841755867 | Val. Loss: 0.14861083030700684\n",
            "Epoch: 587 | Loss: 0.03893532231450081 | Val. Loss: 0.14806917309761047\n",
            "Epoch: 588 | Loss: 0.05439671874046326 | Val. Loss: 0.14990945160388947\n",
            "Epoch: 589 | Loss: 0.0492694266140461 | Val. Loss: 0.14876626431941986\n",
            "Epoch: 590 | Loss: 0.07729747146368027 | Val. Loss: 0.15041273832321167\n",
            "Epoch: 591 | Loss: 0.09380504488945007 | Val. Loss: 0.15064910054206848\n",
            "Epoch: 592 | Loss: 0.08541391044855118 | Val. Loss: 0.1507771760225296\n",
            "Epoch: 593 | Loss: 0.06861534714698792 | Val. Loss: 0.14931432902812958\n",
            "Epoch: 594 | Loss: 0.1232980340719223 | Val. Loss: 0.1496666669845581\n",
            "Epoch: 595 | Loss: 0.0371943898499012 | Val. Loss: 0.15005216002464294\n",
            "Epoch: 596 | Loss: 0.07450556010007858 | Val. Loss: 0.15156832337379456\n",
            "Epoch: 597 | Loss: 0.09046849608421326 | Val. Loss: 0.1501782089471817\n",
            "Epoch: 598 | Loss: 0.08722957968711853 | Val. Loss: 0.15101388096809387\n",
            "Epoch: 599 | Loss: 0.07353927940130234 | Val. Loss: 0.1511310487985611\n",
            "Epoch: 600 | Loss: 0.10788939148187637 | Val. Loss: 0.15118464827537537\n",
            "Epoch: 601 | Loss: 0.09423534572124481 | Val. Loss: 0.15057523548603058\n",
            "Epoch: 602 | Loss: 0.05527491495013237 | Val. Loss: 0.1512080729007721\n",
            "Epoch: 603 | Loss: 0.07061706483364105 | Val. Loss: 0.15056316554546356\n",
            "Epoch: 604 | Loss: 0.05561143159866333 | Val. Loss: 0.15131409466266632\n",
            "Epoch: 605 | Loss: 0.07699223607778549 | Val. Loss: 0.15028615295886993\n",
            "Epoch: 606 | Loss: 0.019689461216330528 | Val. Loss: 0.15266717970371246\n",
            "Epoch: 607 | Loss: 0.1663152128458023 | Val. Loss: 0.14999143779277802\n",
            "Epoch: 608 | Loss: 0.1414608657360077 | Val. Loss: 0.15134620666503906\n",
            "Epoch: 609 | Loss: 0.1403081864118576 | Val. Loss: 0.1515454798936844\n",
            "Epoch: 610 | Loss: 0.06252001971006393 | Val. Loss: 0.1502995640039444\n",
            "Epoch: 611 | Loss: 0.024289991706609726 | Val. Loss: 0.15130119025707245\n",
            "Epoch: 612 | Loss: 0.08896923065185547 | Val. Loss: 0.15177574753761292\n",
            "Epoch: 613 | Loss: 0.07271302491426468 | Val. Loss: 0.14946283400058746\n",
            "Epoch: 614 | Loss: 0.04860050603747368 | Val. Loss: 0.1527053564786911\n",
            "Epoch: 615 | Loss: 0.07630550861358643 | Val. Loss: 0.1515509933233261\n",
            "Epoch: 616 | Loss: 0.03495961055159569 | Val. Loss: 0.15329714119434357\n",
            "Epoch: 617 | Loss: 0.042859967797994614 | Val. Loss: 0.15215101838111877\n",
            "Epoch: 618 | Loss: 0.08346479386091232 | Val. Loss: 0.15273532271385193\n",
            "Epoch: 619 | Loss: 0.06717327237129211 | Val. Loss: 0.15185189247131348\n",
            "Epoch: 620 | Loss: 0.061329856514930725 | Val. Loss: 0.15165777504444122\n",
            "Epoch: 621 | Loss: 0.07248149812221527 | Val. Loss: 0.15316350758075714\n",
            "Epoch: 622 | Loss: 0.09404463320970535 | Val. Loss: 0.15219902992248535\n",
            "Epoch: 623 | Loss: 0.05421215295791626 | Val. Loss: 0.15214788913726807\n",
            "Epoch: 624 | Loss: 0.04321068897843361 | Val. Loss: 0.15245592594146729\n",
            "Epoch: 625 | Loss: 0.08289405703544617 | Val. Loss: 0.15464168787002563\n",
            "Epoch: 626 | Loss: 0.06005994230508804 | Val. Loss: 0.1536470353603363\n",
            "Epoch: 627 | Loss: 0.032268498092889786 | Val. Loss: 0.1529611051082611\n",
            "Epoch: 628 | Loss: 0.0522497333586216 | Val. Loss: 0.15209311246871948\n",
            "Epoch: 629 | Loss: 0.09380396455526352 | Val. Loss: 0.15432527661323547\n",
            "Epoch: 630 | Loss: 0.18260565400123596 | Val. Loss: 0.15425996482372284\n",
            "Epoch: 631 | Loss: 0.04505400359630585 | Val. Loss: 0.15247905254364014\n",
            "Epoch: 632 | Loss: 0.08199761807918549 | Val. Loss: 0.15380077064037323\n",
            "Epoch: 633 | Loss: 0.10022446513175964 | Val. Loss: 0.152911975979805\n",
            "Epoch: 634 | Loss: 0.07757338136434555 | Val. Loss: 0.15393948554992676\n",
            "Epoch: 635 | Loss: 0.049466367810964584 | Val. Loss: 0.15464197099208832\n",
            "Epoch: 636 | Loss: 0.05218271538615227 | Val. Loss: 0.15442411601543427\n",
            "Epoch: 637 | Loss: 0.05569380521774292 | Val. Loss: 0.15386763215065002\n",
            "Epoch: 638 | Loss: 0.03647180274128914 | Val. Loss: 0.1548561155796051\n",
            "Epoch: 639 | Loss: 0.0769830197095871 | Val. Loss: 0.15421226620674133\n",
            "Epoch: 640 | Loss: 0.06052564084529877 | Val. Loss: 0.1539265215396881\n",
            "Epoch: 641 | Loss: 0.04885570704936981 | Val. Loss: 0.15593047440052032\n",
            "Epoch: 642 | Loss: 0.04680532589554787 | Val. Loss: 0.1547325849533081\n",
            "Epoch: 643 | Loss: 0.11427108943462372 | Val. Loss: 0.15562614798545837\n",
            "Epoch: 644 | Loss: 0.08481606841087341 | Val. Loss: 0.1565905213356018\n",
            "Epoch: 645 | Loss: 0.028160838410258293 | Val. Loss: 0.15499326586723328\n",
            "Epoch: 646 | Loss: 0.06755131483078003 | Val. Loss: 0.15697412192821503\n",
            "Epoch: 647 | Loss: 0.07923019677400589 | Val. Loss: 0.15677401423454285\n",
            "Epoch: 648 | Loss: 0.040414877235889435 | Val. Loss: 0.15518733859062195\n",
            "Epoch: 649 | Loss: 0.07080519199371338 | Val. Loss: 0.15635837614536285\n",
            "Epoch: 650 | Loss: 0.04855557903647423 | Val. Loss: 0.15701809525489807\n",
            "Epoch: 651 | Loss: 0.06707906723022461 | Val. Loss: 0.15722380578517914\n",
            "Epoch: 652 | Loss: 0.07116885483264923 | Val. Loss: 0.15821108222007751\n",
            "Epoch: 653 | Loss: 0.09975024312734604 | Val. Loss: 0.15887080132961273\n",
            "Epoch: 654 | Loss: 0.07517017424106598 | Val. Loss: 0.15816278755664825\n",
            "Epoch: 655 | Loss: 0.038837533444166183 | Val. Loss: 0.15914466977119446\n",
            "Epoch: 656 | Loss: 0.042646441608667374 | Val. Loss: 0.15813592076301575\n",
            "Epoch: 657 | Loss: 0.08003465831279755 | Val. Loss: 0.15802976489067078\n",
            "Epoch: 658 | Loss: 0.03670340031385422 | Val. Loss: 0.1599608212709427\n",
            "Epoch: 659 | Loss: 0.13426941633224487 | Val. Loss: 0.1577167809009552\n",
            "Epoch: 660 | Loss: 0.06036260724067688 | Val. Loss: 0.15894314646720886\n",
            "Epoch: 661 | Loss: 0.03515341877937317 | Val. Loss: 0.15872997045516968\n",
            "Epoch: 662 | Loss: 0.07964534312486649 | Val. Loss: 0.16276638209819794\n",
            "Epoch: 663 | Loss: 0.03760378062725067 | Val. Loss: 0.16050660610198975\n",
            "Epoch: 664 | Loss: 0.026092365384101868 | Val. Loss: 0.15962377190589905\n",
            "Epoch: 665 | Loss: 0.039057228714227676 | Val. Loss: 0.16133472323417664\n",
            "Epoch: 666 | Loss: 0.04313017427921295 | Val. Loss: 0.15885528922080994\n",
            "Epoch: 667 | Loss: 0.052206914871931076 | Val. Loss: 0.15962561964988708\n",
            "Epoch: 668 | Loss: 0.08320179581642151 | Val. Loss: 0.16033856570720673\n",
            "Epoch: 669 | Loss: 0.06444818526506424 | Val. Loss: 0.1618843972682953\n",
            "Epoch: 670 | Loss: 0.08134523779153824 | Val. Loss: 0.16142557561397552\n",
            "Epoch: 671 | Loss: 0.028819222003221512 | Val. Loss: 0.163523867726326\n",
            "Epoch: 672 | Loss: 0.08936642110347748 | Val. Loss: 0.161655455827713\n",
            "Epoch: 673 | Loss: 0.059951066970825195 | Val. Loss: 0.1637236773967743\n",
            "Epoch: 674 | Loss: 0.08159300684928894 | Val. Loss: 0.1635853350162506\n",
            "Epoch: 675 | Loss: 0.0855678841471672 | Val. Loss: 0.16309715807437897\n",
            "Epoch: 676 | Loss: 0.07514850795269012 | Val. Loss: 0.16353121399879456\n",
            "Epoch: 677 | Loss: 0.11519540846347809 | Val. Loss: 0.16235637664794922\n",
            "Epoch: 678 | Loss: 0.019216809421777725 | Val. Loss: 0.16344687342643738\n",
            "Epoch: 679 | Loss: 0.03724116086959839 | Val. Loss: 0.16367164254188538\n",
            "Epoch: 680 | Loss: 0.05389898270368576 | Val. Loss: 0.16483771800994873\n",
            "Epoch: 681 | Loss: 0.079670250415802 | Val. Loss: 0.16390962898731232\n",
            "Epoch: 682 | Loss: 0.057646095752716064 | Val. Loss: 0.16285942494869232\n",
            "Epoch: 683 | Loss: 0.07670139521360397 | Val. Loss: 0.1669367253780365\n",
            "Epoch: 684 | Loss: 0.08061648160219193 | Val. Loss: 0.16573917865753174\n",
            "Epoch: 685 | Loss: 0.024414725601673126 | Val. Loss: 0.16703550517559052\n",
            "Epoch: 686 | Loss: 0.10280023515224457 | Val. Loss: 0.16644935309886932\n",
            "Epoch: 687 | Loss: 0.07567518949508667 | Val. Loss: 0.16513636708259583\n",
            "Epoch: 688 | Loss: 0.03427492082118988 | Val. Loss: 0.16771455109119415\n",
            "Epoch: 689 | Loss: 0.03115249238908291 | Val. Loss: 0.16659478843212128\n",
            "Epoch: 690 | Loss: 0.08653616905212402 | Val. Loss: 0.16652558743953705\n",
            "Epoch: 691 | Loss: 0.12175463885068893 | Val. Loss: 0.16905565559864044\n",
            "Epoch: 692 | Loss: 0.09679128974676132 | Val. Loss: 0.1685432344675064\n",
            "Epoch: 693 | Loss: 0.10014823079109192 | Val. Loss: 0.1675245761871338\n",
            "Epoch: 694 | Loss: 0.04308519884943962 | Val. Loss: 0.1660986840724945\n",
            "Epoch: 695 | Loss: 0.054098013788461685 | Val. Loss: 0.16929510235786438\n",
            "Epoch: 696 | Loss: 0.07430504262447357 | Val. Loss: 0.16809310019016266\n",
            "Epoch: 697 | Loss: 0.06145038455724716 | Val. Loss: 0.16977784037590027\n",
            "Epoch: 698 | Loss: 0.07764016091823578 | Val. Loss: 0.16951802372932434\n",
            "Epoch: 699 | Loss: 0.03428911790251732 | Val. Loss: 0.1696234941482544\n",
            "Epoch: 700 | Loss: 0.03583132475614548 | Val. Loss: 0.1685151606798172\n",
            "Epoch: 701 | Loss: 0.08673617988824844 | Val. Loss: 0.1690995842218399\n",
            "Epoch: 702 | Loss: 0.04740691930055618 | Val. Loss: 0.17054806649684906\n",
            "Epoch: 703 | Loss: 0.06099085882306099 | Val. Loss: 0.17062194645404816\n",
            "Epoch: 704 | Loss: 0.028780890628695488 | Val. Loss: 0.17303933203220367\n",
            "Epoch: 705 | Loss: 0.07210111618041992 | Val. Loss: 0.17073532938957214\n",
            "Epoch: 706 | Loss: 0.09901146590709686 | Val. Loss: 0.17416813969612122\n",
            "Epoch: 707 | Loss: 0.04979635775089264 | Val. Loss: 0.17224422097206116\n",
            "Epoch: 708 | Loss: 0.09719149023294449 | Val. Loss: 0.1713939905166626\n",
            "Epoch: 709 | Loss: 0.03502057120203972 | Val. Loss: 0.17405842244625092\n",
            "Epoch: 710 | Loss: 0.05359715223312378 | Val. Loss: 0.17291738092899323\n",
            "Epoch: 711 | Loss: 0.022202972322702408 | Val. Loss: 0.17312294244766235\n",
            "Epoch: 712 | Loss: 0.057679951190948486 | Val. Loss: 0.17447499930858612\n",
            "Epoch: 713 | Loss: 0.032329317182302475 | Val. Loss: 0.1752023994922638\n",
            "Epoch: 714 | Loss: 0.10116851329803467 | Val. Loss: 0.1745978593826294\n",
            "Epoch: 715 | Loss: 0.11755236238241196 | Val. Loss: 0.17557920515537262\n",
            "Epoch: 716 | Loss: 0.07803556323051453 | Val. Loss: 0.17784717679023743\n",
            "Epoch: 717 | Loss: 0.026228677481412888 | Val. Loss: 0.17768998444080353\n",
            "Epoch: 718 | Loss: 0.030335407704114914 | Val. Loss: 0.17780092358589172\n",
            "Epoch: 719 | Loss: 0.05264514684677124 | Val. Loss: 0.176304891705513\n",
            "Epoch: 720 | Loss: 0.07387952506542206 | Val. Loss: 0.17720669507980347\n",
            "Epoch: 721 | Loss: 0.03581902012228966 | Val. Loss: 0.17886511981487274\n",
            "Epoch: 722 | Loss: 0.10694508999586105 | Val. Loss: 0.17750215530395508\n",
            "Epoch: 723 | Loss: 0.10442804545164108 | Val. Loss: 0.17908811569213867\n",
            "Epoch: 724 | Loss: 0.04596336930990219 | Val. Loss: 0.17986123263835907\n",
            "Epoch: 725 | Loss: 0.02688189409673214 | Val. Loss: 0.17977020144462585\n",
            "Epoch: 726 | Loss: 0.028180740773677826 | Val. Loss: 0.18016153573989868\n",
            "Epoch: 727 | Loss: 0.06513428688049316 | Val. Loss: 0.17889471352100372\n",
            "Epoch: 728 | Loss: 0.018356721848249435 | Val. Loss: 0.18023498356342316\n",
            "Epoch: 729 | Loss: 0.03783693537116051 | Val. Loss: 0.18164889514446259\n",
            "Epoch: 730 | Loss: 0.057255059480667114 | Val. Loss: 0.18090876936912537\n",
            "Epoch: 731 | Loss: 0.06379104405641556 | Val. Loss: 0.18145860731601715\n",
            "Epoch: 732 | Loss: 0.07675576210021973 | Val. Loss: 0.1832490712404251\n",
            "Epoch: 733 | Loss: 0.023729436099529266 | Val. Loss: 0.18315955996513367\n",
            "Epoch: 734 | Loss: 0.0911678895354271 | Val. Loss: 0.18273402750492096\n",
            "Epoch: 735 | Loss: 0.04435218870639801 | Val. Loss: 0.18351884186267853\n",
            "Epoch: 736 | Loss: 0.08504442870616913 | Val. Loss: 0.18470196425914764\n",
            "Epoch: 737 | Loss: 0.03593431040644646 | Val. Loss: 0.18361631035804749\n",
            "Epoch: 738 | Loss: 0.02938491478562355 | Val. Loss: 0.18475325405597687\n",
            "Epoch: 739 | Loss: 0.042825013399124146 | Val. Loss: 0.18589776754379272\n",
            "Epoch: 740 | Loss: 0.09627179056406021 | Val. Loss: 0.1870662271976471\n",
            "Epoch: 741 | Loss: 0.1132056713104248 | Val. Loss: 0.18462681770324707\n",
            "Epoch: 742 | Loss: 0.03324222192168236 | Val. Loss: 0.18560676276683807\n",
            "Epoch: 743 | Loss: 0.037266943603754044 | Val. Loss: 0.18797218799591064\n",
            "Epoch: 744 | Loss: 0.04161493480205536 | Val. Loss: 0.1865268051624298\n",
            "Epoch: 745 | Loss: 0.0669071152806282 | Val. Loss: 0.1868167668581009\n",
            "Epoch: 746 | Loss: 0.07084470987319946 | Val. Loss: 0.1876751184463501\n",
            "Epoch: 747 | Loss: 0.10146994143724442 | Val. Loss: 0.18722791969776154\n",
            "Epoch: 748 | Loss: 0.0645718201994896 | Val. Loss: 0.18828734755516052\n",
            "Epoch: 749 | Loss: 0.05014698952436447 | Val. Loss: 0.1899086833000183\n",
            "Epoch: 750 | Loss: 0.02242286317050457 | Val. Loss: 0.18871381878852844\n",
            "Epoch: 751 | Loss: 0.0327458493411541 | Val. Loss: 0.18928630650043488\n",
            "Epoch: 752 | Loss: 0.03539280593395233 | Val. Loss: 0.1885073184967041\n",
            "Epoch: 753 | Loss: 0.03933437541127205 | Val. Loss: 0.18968918919563293\n",
            "Epoch: 754 | Loss: 0.07212024927139282 | Val. Loss: 0.1901743859052658\n",
            "Epoch: 755 | Loss: 0.01882638968527317 | Val. Loss: 0.1901315450668335\n",
            "Epoch: 756 | Loss: 0.025830954313278198 | Val. Loss: 0.1906168907880783\n",
            "Epoch: 757 | Loss: 0.04073537886142731 | Val. Loss: 0.19106745719909668\n",
            "Epoch: 758 | Loss: 0.07405424118041992 | Val. Loss: 0.19192099571228027\n",
            "Epoch: 759 | Loss: 0.04767109453678131 | Val. Loss: 0.19011838734149933\n",
            "Epoch: 760 | Loss: 0.09964586049318314 | Val. Loss: 0.19245271384716034\n",
            "Epoch: 761 | Loss: 0.06807747483253479 | Val. Loss: 0.19127517938613892\n",
            "Epoch: 762 | Loss: 0.03386300802230835 | Val. Loss: 0.19063949584960938\n",
            "Epoch: 763 | Loss: 0.05607064813375473 | Val. Loss: 0.19328360259532928\n",
            "Epoch: 764 | Loss: 0.03486086055636406 | Val. Loss: 0.19328081607818604\n",
            "Epoch: 765 | Loss: 0.09601788967847824 | Val. Loss: 0.1914507895708084\n",
            "Epoch: 766 | Loss: 0.057846419513225555 | Val. Loss: 0.19246475398540497\n",
            "Epoch: 767 | Loss: 0.061499182134866714 | Val. Loss: 0.1931867152452469\n",
            "Epoch: 768 | Loss: 0.06130072474479675 | Val. Loss: 0.19184410572052002\n",
            "Epoch: 769 | Loss: 0.04390757903456688 | Val. Loss: 0.19494374096393585\n",
            "Epoch: 770 | Loss: 0.07296054065227509 | Val. Loss: 0.1945185363292694\n",
            "Epoch: 771 | Loss: 0.12197887152433395 | Val. Loss: 0.19445860385894775\n",
            "Epoch: 772 | Loss: 0.03353806585073471 | Val. Loss: 0.19388632476329803\n",
            "Epoch: 773 | Loss: 0.01951226033270359 | Val. Loss: 0.19527201354503632\n",
            "Epoch: 774 | Loss: 0.035921089351177216 | Val. Loss: 0.19365441799163818\n",
            "Epoch: 775 | Loss: 0.07091624289751053 | Val. Loss: 0.19419077038764954\n",
            "Epoch: 776 | Loss: 0.040116455405950546 | Val. Loss: 0.19614876806735992\n",
            "Epoch: 777 | Loss: 0.06535124778747559 | Val. Loss: 0.1956217885017395\n",
            "Epoch: 778 | Loss: 0.03405820205807686 | Val. Loss: 0.19588196277618408\n",
            "Epoch: 779 | Loss: 0.04082703962922096 | Val. Loss: 0.19674387574195862\n",
            "Epoch: 780 | Loss: 0.02576882392168045 | Val. Loss: 0.19921278953552246\n",
            "Epoch: 781 | Loss: 0.08919193595647812 | Val. Loss: 0.1989246904850006\n",
            "Epoch: 782 | Loss: 0.02581772767007351 | Val. Loss: 0.19778120517730713\n",
            "Epoch: 783 | Loss: 0.04617089033126831 | Val. Loss: 0.19932188093662262\n",
            "Epoch: 784 | Loss: 0.028222473338246346 | Val. Loss: 0.19789960980415344\n",
            "Epoch: 785 | Loss: 0.059690456837415695 | Val. Loss: 0.19870057702064514\n",
            "Epoch: 786 | Loss: 0.02541349269449711 | Val. Loss: 0.19810505211353302\n",
            "Epoch: 787 | Loss: 0.03556923568248749 | Val. Loss: 0.1991543471813202\n",
            "Epoch: 788 | Loss: 0.027480393648147583 | Val. Loss: 0.19897451996803284\n",
            "Epoch: 789 | Loss: 0.03905743360519409 | Val. Loss: 0.20197971165180206\n",
            "Epoch: 790 | Loss: 0.015416274778544903 | Val. Loss: 0.20130205154418945\n",
            "Epoch: 791 | Loss: 0.044192951172590256 | Val. Loss: 0.20041179656982422\n",
            "Epoch: 792 | Loss: 0.10332771390676498 | Val. Loss: 0.1989986151456833\n",
            "Epoch: 793 | Loss: 0.018496738746762276 | Val. Loss: 0.20285794138908386\n",
            "Epoch: 794 | Loss: 0.09825357049703598 | Val. Loss: 0.20403678715229034\n",
            "Epoch: 795 | Loss: 0.032676104456186295 | Val. Loss: 0.204178124666214\n",
            "Epoch: 796 | Loss: 0.02691972814500332 | Val. Loss: 0.20299942791461945\n",
            "Epoch: 797 | Loss: 0.021978920325636864 | Val. Loss: 0.20282165706157684\n",
            "Epoch: 798 | Loss: 0.023603081703186035 | Val. Loss: 0.20349808037281036\n",
            "Epoch: 799 | Loss: 0.03798811137676239 | Val. Loss: 0.20507800579071045\n",
            "Epoch: 800 | Loss: 0.09181826561689377 | Val. Loss: 0.20624391734600067\n",
            "Epoch: 801 | Loss: 0.03616316616535187 | Val. Loss: 0.20537742972373962\n",
            "Epoch: 802 | Loss: 0.036324478685855865 | Val. Loss: 0.20583999156951904\n",
            "Epoch: 803 | Loss: 0.026173552498221397 | Val. Loss: 0.20646452903747559\n",
            "Epoch: 804 | Loss: 0.03603862598538399 | Val. Loss: 0.20819763839244843\n",
            "Epoch: 805 | Loss: 0.03708946704864502 | Val. Loss: 0.2063048779964447\n",
            "Epoch: 806 | Loss: 0.017207693308591843 | Val. Loss: 0.20742221176624298\n",
            "Epoch: 807 | Loss: 0.025188306346535683 | Val. Loss: 0.206500843167305\n",
            "Epoch: 808 | Loss: 0.04734499379992485 | Val. Loss: 0.20742300152778625\n",
            "Epoch: 809 | Loss: 0.028395796194672585 | Val. Loss: 0.20946958661079407\n",
            "Epoch: 810 | Loss: 0.05325069651007652 | Val. Loss: 0.20715327560901642\n",
            "Epoch: 811 | Loss: 0.02109270542860031 | Val. Loss: 0.21062573790550232\n",
            "Epoch: 812 | Loss: 0.049984004348516464 | Val. Loss: 0.20655135810375214\n",
            "Epoch: 813 | Loss: 0.021023821085691452 | Val. Loss: 0.2090781033039093\n",
            "Epoch: 814 | Loss: 0.04626410827040672 | Val. Loss: 0.20944508910179138\n",
            "Epoch: 815 | Loss: 0.03449741005897522 | Val. Loss: 0.20420770347118378\n",
            "Epoch: 816 | Loss: 0.07335522025823593 | Val. Loss: 0.2044692486524582\n",
            "Epoch: 817 | Loss: 0.05142150819301605 | Val. Loss: 0.208377867937088\n",
            "Epoch: 818 | Loss: 0.07200895249843597 | Val. Loss: 0.20931191742420197\n",
            "Epoch: 819 | Loss: 0.052704453468322754 | Val. Loss: 0.20408764481544495\n",
            "Epoch: 820 | Loss: 0.0653909221291542 | Val. Loss: 0.2027892768383026\n",
            "Epoch: 821 | Loss: 0.02232690155506134 | Val. Loss: 0.20401163399219513\n",
            "Epoch: 822 | Loss: 0.02777462638914585 | Val. Loss: 0.2034279853105545\n",
            "Epoch: 823 | Loss: 0.017037630081176758 | Val. Loss: 0.20929773151874542\n",
            "Epoch: 824 | Loss: 0.05702003091573715 | Val. Loss: 0.20399002730846405\n",
            "Epoch: 825 | Loss: 0.07956308126449585 | Val. Loss: 0.2042851448059082\n",
            "Epoch: 826 | Loss: 0.016903087496757507 | Val. Loss: 0.20441070199012756\n",
            "Epoch: 827 | Loss: 0.06572859734296799 | Val. Loss: 0.2066098004579544\n",
            "Epoch: 828 | Loss: 0.07928101718425751 | Val. Loss: 0.20325057208538055\n",
            "Epoch: 829 | Loss: 0.05720801651477814 | Val. Loss: 0.20169885456562042\n",
            "Epoch: 830 | Loss: 0.05693308264017105 | Val. Loss: 0.20420849323272705\n",
            "Epoch: 831 | Loss: 0.015206716023385525 | Val. Loss: 0.19688989222049713\n",
            "Epoch: 832 | Loss: 0.06484959274530411 | Val. Loss: 0.19874146580696106\n",
            "Epoch: 833 | Loss: 0.024345319718122482 | Val. Loss: 0.19754882156848907\n",
            "Epoch: 834 | Loss: 0.03613412380218506 | Val. Loss: 0.19859889149665833\n",
            "Epoch: 835 | Loss: 0.036068543791770935 | Val. Loss: 0.2017407864332199\n",
            "Epoch: 836 | Loss: 0.05811098590493202 | Val. Loss: 0.19683986902236938\n",
            "Epoch: 837 | Loss: 0.026539579033851624 | Val. Loss: 0.19991134107112885\n",
            "Epoch: 838 | Loss: 0.01941366121172905 | Val. Loss: 0.20112097263336182\n",
            "Epoch: 839 | Loss: 0.061642251908779144 | Val. Loss: 0.1996322125196457\n",
            "Epoch: 840 | Loss: 0.029218917712569237 | Val. Loss: 0.1986776888370514\n",
            "Epoch: 841 | Loss: 0.049323298037052155 | Val. Loss: 0.19679084420204163\n",
            "Epoch: 842 | Loss: 0.058920469135046005 | Val. Loss: 0.20162823796272278\n",
            "Epoch: 843 | Loss: 0.06411097943782806 | Val. Loss: 0.1950853168964386\n",
            "Epoch: 844 | Loss: 0.05952014401555061 | Val. Loss: 0.19853191077709198\n",
            "Epoch: 845 | Loss: 0.021267613396048546 | Val. Loss: 0.1981939822435379\n",
            "Epoch: 846 | Loss: 0.029665127396583557 | Val. Loss: 0.19282355904579163\n",
            "Epoch: 847 | Loss: 0.08054254204034805 | Val. Loss: 0.19619090855121613\n",
            "Epoch: 848 | Loss: 0.05639499053359032 | Val. Loss: 0.19784238934516907\n",
            "Epoch: 849 | Loss: 0.03718879073858261 | Val. Loss: 0.19739189743995667\n",
            "Epoch: 850 | Loss: 0.008945164270699024 | Val. Loss: 0.1962481439113617\n",
            "Epoch: 851 | Loss: 0.0545901358127594 | Val. Loss: 0.19361186027526855\n",
            "Epoch: 852 | Loss: 0.03369901329278946 | Val. Loss: 0.19251349568367004\n",
            "Epoch: 853 | Loss: 0.017525549978017807 | Val. Loss: 0.19106735289096832\n",
            "Epoch: 854 | Loss: 0.05273602157831192 | Val. Loss: 0.19561149179935455\n",
            "Epoch: 855 | Loss: 0.09837956726551056 | Val. Loss: 0.19379928708076477\n",
            "Epoch: 856 | Loss: 0.04417707771062851 | Val. Loss: 0.196274071931839\n",
            "Epoch: 857 | Loss: 0.04594108834862709 | Val. Loss: 0.19341795146465302\n",
            "Epoch: 858 | Loss: 0.014905435033142567 | Val. Loss: 0.19215285778045654\n",
            "Epoch: 859 | Loss: 0.04362406209111214 | Val. Loss: 0.18915565311908722\n",
            "Epoch: 860 | Loss: 0.026547865942120552 | Val. Loss: 0.19292423129081726\n",
            "Epoch: 861 | Loss: 0.06007221341133118 | Val. Loss: 0.19340509176254272\n",
            "Epoch: 862 | Loss: 0.030680090188980103 | Val. Loss: 0.19210810959339142\n",
            "Epoch: 863 | Loss: 0.06790269166231155 | Val. Loss: 0.18950167298316956\n",
            "Epoch: 864 | Loss: 0.022634677588939667 | Val. Loss: 0.1903967559337616\n",
            "Epoch: 865 | Loss: 0.029632538557052612 | Val. Loss: 0.1927298754453659\n",
            "Epoch: 866 | Loss: 0.054429519921541214 | Val. Loss: 0.1917511224746704\n",
            "Epoch: 867 | Loss: 0.05599072575569153 | Val. Loss: 0.18804606795310974\n",
            "Epoch: 868 | Loss: 0.018408773466944695 | Val. Loss: 0.19007067382335663\n",
            "Epoch: 869 | Loss: 0.03832902014255524 | Val. Loss: 0.18777409195899963\n",
            "Epoch: 870 | Loss: 0.07053280621767044 | Val. Loss: 0.18911510705947876\n",
            "Epoch: 871 | Loss: 0.06546450406312943 | Val. Loss: 0.18828792870044708\n",
            "Epoch: 872 | Loss: 0.018888304010033607 | Val. Loss: 0.1891949474811554\n",
            "Epoch: 873 | Loss: 0.05644107609987259 | Val. Loss: 0.18854984641075134\n",
            "Epoch: 874 | Loss: 0.03495329990983009 | Val. Loss: 0.18939955532550812\n",
            "Epoch: 875 | Loss: 0.07183265686035156 | Val. Loss: 0.18168194591999054\n",
            "Epoch: 876 | Loss: 0.04314997047185898 | Val. Loss: 0.18218591809272766\n",
            "Epoch: 877 | Loss: 0.04255037382245064 | Val. Loss: 0.18847617506980896\n",
            "Epoch: 878 | Loss: 0.06682905554771423 | Val. Loss: 0.18855340778827667\n",
            "Epoch: 879 | Loss: 0.03231760114431381 | Val. Loss: 0.1844038963317871\n",
            "Epoch: 880 | Loss: 0.05713025853037834 | Val. Loss: 0.18343350291252136\n",
            "Epoch: 881 | Loss: 0.02790815755724907 | Val. Loss: 0.1817944496870041\n",
            "Epoch: 882 | Loss: 0.02998603694140911 | Val. Loss: 0.18608899414539337\n",
            "Epoch: 883 | Loss: 0.064220130443573 | Val. Loss: 0.1810540109872818\n",
            "Epoch: 884 | Loss: 0.02666885405778885 | Val. Loss: 0.18150028586387634\n",
            "Epoch: 885 | Loss: 0.037953805178403854 | Val. Loss: 0.18151475489139557\n",
            "Epoch: 886 | Loss: 0.03432139754295349 | Val. Loss: 0.18442730605602264\n",
            "Epoch: 887 | Loss: 0.045071691274642944 | Val. Loss: 0.18174639344215393\n",
            "Epoch: 888 | Loss: 0.08573225885629654 | Val. Loss: 0.18015022575855255\n",
            "Epoch: 889 | Loss: 0.09328683465719223 | Val. Loss: 0.18197007477283478\n",
            "Epoch: 890 | Loss: 0.03709256649017334 | Val. Loss: 0.1779203712940216\n",
            "Epoch: 891 | Loss: 0.060894675552845 | Val. Loss: 0.1802913397550583\n",
            "Epoch: 892 | Loss: 0.025508686900138855 | Val. Loss: 0.18143460154533386\n",
            "Epoch: 893 | Loss: 0.0815289169549942 | Val. Loss: 0.18023666739463806\n",
            "Epoch: 894 | Loss: 0.042864058166742325 | Val. Loss: 0.17695878446102142\n",
            "Epoch: 895 | Loss: 0.07476646453142166 | Val. Loss: 0.17996449768543243\n",
            "Epoch: 896 | Loss: 0.021923532709479332 | Val. Loss: 0.18247336149215698\n",
            "Epoch: 897 | Loss: 0.019966142252087593 | Val. Loss: 0.18143440783023834\n",
            "Epoch: 898 | Loss: 0.035916768014431 | Val. Loss: 0.1769162118434906\n",
            "Epoch: 899 | Loss: 0.03345555067062378 | Val. Loss: 0.17700108885765076\n",
            "Epoch: 900 | Loss: 0.028863519430160522 | Val. Loss: 0.17548325657844543\n",
            "Epoch: 901 | Loss: 0.06100590527057648 | Val. Loss: 0.17900434136390686\n",
            "Epoch: 902 | Loss: 0.042944543063640594 | Val. Loss: 0.1742216944694519\n",
            "Epoch: 903 | Loss: 0.0356619693338871 | Val. Loss: 0.1801077127456665\n",
            "Epoch: 904 | Loss: 0.03360530361533165 | Val. Loss: 0.17867937684059143\n",
            "Epoch: 905 | Loss: 0.023873532190918922 | Val. Loss: 0.17766614258289337\n",
            "Epoch: 906 | Loss: 0.024954112246632576 | Val. Loss: 0.17517203092575073\n",
            "Epoch: 907 | Loss: 0.014469597488641739 | Val. Loss: 0.1743326187133789\n",
            "Epoch: 908 | Loss: 0.018440546467900276 | Val. Loss: 0.17667408287525177\n",
            "Epoch: 909 | Loss: 0.02064480446279049 | Val. Loss: 0.17567729949951172\n",
            "Epoch: 910 | Loss: 0.01795651763677597 | Val. Loss: 0.17305870354175568\n",
            "Epoch: 911 | Loss: 0.04458457976579666 | Val. Loss: 0.16971251368522644\n",
            "Epoch: 912 | Loss: 0.041491568088531494 | Val. Loss: 0.1710996925830841\n",
            "Epoch: 913 | Loss: 0.023293964564800262 | Val. Loss: 0.17287783324718475\n",
            "Epoch: 914 | Loss: 0.014475866220891476 | Val. Loss: 0.17187266051769257\n",
            "Epoch: 915 | Loss: 0.04711247235536575 | Val. Loss: 0.17337338626384735\n",
            "Epoch: 916 | Loss: 0.02541167289018631 | Val. Loss: 0.1741342544555664\n",
            "Epoch: 917 | Loss: 0.015281478874385357 | Val. Loss: 0.1721223145723343\n",
            "Epoch: 918 | Loss: 0.07055811583995819 | Val. Loss: 0.17383460700511932\n",
            "Epoch: 919 | Loss: 0.06623043119907379 | Val. Loss: 0.16602802276611328\n",
            "Epoch: 920 | Loss: 0.012812844477593899 | Val. Loss: 0.17311878502368927\n",
            "Epoch: 921 | Loss: 0.017462097108364105 | Val. Loss: 0.17683139443397522\n",
            "Epoch: 922 | Loss: 0.057303715497255325 | Val. Loss: 0.1654907763004303\n",
            "Epoch: 923 | Loss: 0.03819088637828827 | Val. Loss: 0.17266765236854553\n",
            "Epoch: 924 | Loss: 0.03807752579450607 | Val. Loss: 0.17341193556785583\n",
            "Epoch: 925 | Loss: 0.031910430639982224 | Val. Loss: 0.1722545325756073\n",
            "Epoch: 926 | Loss: 0.04103120416402817 | Val. Loss: 0.1703726351261139\n",
            "Epoch: 927 | Loss: 0.029157646000385284 | Val. Loss: 0.16973741352558136\n",
            "Epoch: 928 | Loss: 0.04421817883849144 | Val. Loss: 0.16787388920783997\n",
            "Epoch: 929 | Loss: 0.08061213791370392 | Val. Loss: 0.17238903045654297\n",
            "Epoch: 930 | Loss: 0.015034896321594715 | Val. Loss: 0.1684175431728363\n",
            "Epoch: 931 | Loss: 0.01424340158700943 | Val. Loss: 0.16639676690101624\n",
            "Epoch: 932 | Loss: 0.014782377518713474 | Val. Loss: 0.16824069619178772\n",
            "Epoch: 933 | Loss: 0.027423512190580368 | Val. Loss: 0.17072728276252747\n",
            "Epoch: 934 | Loss: 0.02005886100232601 | Val. Loss: 0.1691937893629074\n",
            "Epoch: 935 | Loss: 0.020371634513139725 | Val. Loss: 0.16291669011116028\n",
            "Epoch: 936 | Loss: 0.01406653318554163 | Val. Loss: 0.16600100696086884\n",
            "Epoch: 937 | Loss: 0.023218272253870964 | Val. Loss: 0.1644703447818756\n",
            "Epoch: 938 | Loss: 0.048022277653217316 | Val. Loss: 0.16738052666187286\n",
            "Epoch: 939 | Loss: 0.04162975400686264 | Val. Loss: 0.16231004893779755\n",
            "Epoch: 940 | Loss: 0.009625427424907684 | Val. Loss: 0.1634652018547058\n",
            "Epoch: 941 | Loss: 0.04413481801748276 | Val. Loss: 0.16829688847064972\n",
            "Epoch: 942 | Loss: 0.07179190218448639 | Val. Loss: 0.1656787097454071\n",
            "Epoch: 943 | Loss: 0.04803094640374184 | Val. Loss: 0.15930438041687012\n",
            "Epoch: 944 | Loss: 0.01601131446659565 | Val. Loss: 0.16370776295661926\n",
            "Epoch: 945 | Loss: 0.03981982171535492 | Val. Loss: 0.15613408386707306\n",
            "Epoch: 946 | Loss: 0.037255190312862396 | Val. Loss: 0.1624910831451416\n",
            "Epoch: 947 | Loss: 0.03329547122120857 | Val. Loss: 0.16246755421161652\n",
            "Epoch: 948 | Loss: 0.007771327160298824 | Val. Loss: 0.16534768044948578\n",
            "Epoch: 949 | Loss: 0.02797149308025837 | Val. Loss: 0.1617402732372284\n",
            "Epoch: 950 | Loss: 0.016234729439020157 | Val. Loss: 0.1632630079984665\n",
            "Epoch: 951 | Loss: 0.009089753963053226 | Val. Loss: 0.15859366953372955\n",
            "Epoch: 952 | Loss: 0.02800619788467884 | Val. Loss: 0.16147266328334808\n",
            "Epoch: 953 | Loss: 0.04379641264677048 | Val. Loss: 0.16154715418815613\n",
            "Epoch: 954 | Loss: 0.015449536964297295 | Val. Loss: 0.16245099902153015\n",
            "Epoch: 955 | Loss: 0.014399849809706211 | Val. Loss: 0.16308417916297913\n",
            "Epoch: 956 | Loss: 0.03477191925048828 | Val. Loss: 0.15916340053081512\n",
            "Epoch: 957 | Loss: 0.03418724238872528 | Val. Loss: 0.16042742133140564\n",
            "Epoch: 958 | Loss: 0.019479360431432724 | Val. Loss: 0.15938708186149597\n",
            "Epoch: 959 | Loss: 0.029058370739221573 | Val. Loss: 0.15734054148197174\n",
            "Epoch: 960 | Loss: 0.029305897653102875 | Val. Loss: 0.1593853235244751\n",
            "Epoch: 961 | Loss: 0.04000747576355934 | Val. Loss: 0.1594371348619461\n",
            "Epoch: 962 | Loss: 0.04921871796250343 | Val. Loss: 0.1565704643726349\n",
            "Epoch: 963 | Loss: 0.0644456148147583 | Val. Loss: 0.15863189101219177\n",
            "Epoch: 964 | Loss: 0.05842792987823486 | Val. Loss: 0.1581363081932068\n",
            "Epoch: 965 | Loss: 0.023649627342820168 | Val. Loss: 0.1585148721933365\n",
            "Epoch: 966 | Loss: 0.009926874190568924 | Val. Loss: 0.1573304533958435\n",
            "Epoch: 967 | Loss: 0.013630314730107784 | Val. Loss: 0.16021019220352173\n",
            "Epoch: 968 | Loss: 0.03959034010767937 | Val. Loss: 0.15655191242694855\n",
            "Epoch: 969 | Loss: 0.054500024765729904 | Val. Loss: 0.1585143804550171\n",
            "Epoch: 970 | Loss: 0.025577597320079803 | Val. Loss: 0.15281328558921814\n",
            "Epoch: 971 | Loss: 0.03556159511208534 | Val. Loss: 0.15781082212924957\n",
            "Epoch: 972 | Loss: 0.03186663240194321 | Val. Loss: 0.15626800060272217\n",
            "Epoch: 973 | Loss: 0.01574542373418808 | Val. Loss: 0.15690116584300995\n",
            "Epoch: 974 | Loss: 0.04181473329663277 | Val. Loss: 0.15395058691501617\n",
            "Epoch: 975 | Loss: 0.0274085383862257 | Val. Loss: 0.15183000266551971\n",
            "Epoch: 976 | Loss: 0.04598236456513405 | Val. Loss: 0.15406541526317596\n",
            "Epoch: 977 | Loss: 0.02047966606914997 | Val. Loss: 0.1574234515428543\n",
            "Epoch: 978 | Loss: 0.007926980964839458 | Val. Loss: 0.15839643776416779\n",
            "Epoch: 979 | Loss: 0.008684259839355946 | Val. Loss: 0.15565617382526398\n",
            "Epoch: 980 | Loss: 0.04269586130976677 | Val. Loss: 0.15055187046527863\n",
            "Epoch: 981 | Loss: 0.030701670795679092 | Val. Loss: 0.15473893284797668\n",
            "Epoch: 982 | Loss: 0.024005437269806862 | Val. Loss: 0.1531621515750885\n",
            "Epoch: 983 | Loss: 0.031749434769153595 | Val. Loss: 0.14891402423381805\n",
            "Epoch: 984 | Loss: 0.017044980078935623 | Val. Loss: 0.15014182031154633\n",
            "Epoch: 985 | Loss: 0.012561731040477753 | Val. Loss: 0.15086126327514648\n",
            "Epoch: 986 | Loss: 0.007396564818918705 | Val. Loss: 0.15002581477165222\n",
            "Epoch: 987 | Loss: 0.02365744858980179 | Val. Loss: 0.14708872139453888\n",
            "Epoch: 988 | Loss: 0.017453540116548538 | Val. Loss: 0.15044161677360535\n",
            "Epoch: 989 | Loss: 0.02737901546061039 | Val. Loss: 0.14914196729660034\n",
            "Epoch: 990 | Loss: 0.033085133880376816 | Val. Loss: 0.15025684237480164\n",
            "Epoch: 991 | Loss: 0.02257535047829151 | Val. Loss: 0.151895672082901\n",
            "Epoch: 992 | Loss: 0.009086202830076218 | Val. Loss: 0.14724813401699066\n",
            "Epoch: 993 | Loss: 0.032729413360357285 | Val. Loss: 0.15092454850673676\n",
            "Epoch: 994 | Loss: 0.023115722462534904 | Val. Loss: 0.15228882431983948\n",
            "Epoch: 995 | Loss: 0.04868779331445694 | Val. Loss: 0.1501867175102234\n",
            "Epoch: 996 | Loss: 0.0761084109544754 | Val. Loss: 0.153535395860672\n",
            "Epoch: 997 | Loss: 0.018994679674506187 | Val. Loss: 0.14879044890403748\n",
            "Epoch: 998 | Loss: 0.02218594215810299 | Val. Loss: 0.14188513159751892\n",
            "Epoch: 999 | Loss: 0.022994372993707657 | Val. Loss: 0.14845553040504456\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Loss')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testando o modelo\n",
        "\n",
        "# Converte os dados de teste em arrays numpy\n",
        "# x_teste_np = data_test.iloc[:, :-1].to_numpy()\n",
        "# d_teste_np = data_test.iloc[:, -1].to_numpy()\n",
        "\n",
        "# Aplica a normalização no conjunto de teste\n",
        "# x_teste_np_norm = (x_teste_np - media_treino) / desvio_padrao_treino\n",
        "\n",
        "# Projeta o teste\n",
        "x_teste_pca = (U @ x_teste_np_norm.T)\n",
        "x_teste_pca = x_teste_pca.T\n",
        "\n",
        "# Converte os arrays numpy em tensores PyTorch\n",
        "x_teste_pca = torch.tensor(x_teste_pca, dtype=torch.float32).to(device=device)\n",
        "d_teste_tensor = torch.tensor(d_teste_np, dtype=torch.long).to(device=device)\n",
        "\n",
        "# Testa o modelo com os dados de teste\n",
        "y_teste_tensor_pca = model_pca(x_teste_pca)\n",
        "y_teste_np = y_teste_tensor_pca.cpu().detach().numpy()\n",
        "\n",
        "predicoes_pca = np.argmax(y_teste_np, axis=1)\n",
        "acuracia_pca = np.mean(predicoes_pca == d_teste_np)\n",
        "\n",
        "# Taxa de erros\n",
        "\n",
        "Taxa_de_erro_pca = (1 - acuracia_pca) * 100\n",
        "\n",
        "print(f\"Acurácia: {acuracia_pca*100:.2f}%\")\n",
        "print(f\"Taxa de erro: {Taxa_de_erro_pca:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoV-OR1K4b7R",
        "outputId": "a5b5780f-5409-4699-d3b6-ddb1e86d7b22"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia: 96.89%\n",
            "Taxa de erro: 3.11%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQbeQopQ3K_j"
      },
      "source": [
        "# Exercício 4\n",
        "\n",
        "Repita os exercícios 1, 2 e 3, considerando a transformação dos dados usando o LDA no lugar do PCA. Use como referência o exemplo mostrado [neste Jupyter Notebook](./LDA_IRIS.ipynb)\n",
        "\n",
        "## Resolução"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "TbWImeSV3K_j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf99e97b-b394-49c7-c463-27930ed789ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1.23286021e+00  5.77883828e-17  2.19177081e-17  9.13183191e-18\n",
            "  9.13183191e-18  5.36556955e-19  5.36556955e-19  0.00000000e+00\n",
            " -8.74836566e-18 -8.74836566e-18 -3.34834632e-17 -8.13863938e-17\n",
            " -2.13458097e-16]\n"
          ]
        }
      ],
      "source": [
        "# yn = W.T @ X\n",
        "\n",
        "X_raw = x_treino_np\n",
        "d_raw = d_treino_np\n",
        "N, D = X_raw.shape\n",
        "\n",
        "# Média global\n",
        "# m = (1/N) * sum(x_n)\n",
        "m_global = np.mean(X_raw, axis=0).reshape(-1, 1)\n",
        "\n",
        "\n",
        "# Matrizes de dispersão\n",
        "Sw = np.zeros((D, D)) # Dentro das classes\n",
        "Sb = np.zeros((D, D)) # Entre classes\n",
        "\n",
        "classes = np.unique(d_raw);\n",
        "\n",
        "for k in classes:\n",
        "  X_k = X_raw[d_raw == k]\n",
        "  N_k = X_k.shape[0]\n",
        "\n",
        "  # Média da classe k\n",
        "  m_k = np.mean(X_k, axis=0).reshape(-1, 1)\n",
        "\n",
        "  # Sw = (X_k - m_k) @ (X_k - m_k).T\n",
        "  S_k = (X_k - m_k.T).T @ (X_k - m_k.T) # Somatório interno (em D)\n",
        "  Sw += S_k # Somatório externo (k=1 até K)\n",
        "\n",
        "  # Sb\n",
        "  Sb_k = N_k * ((m_k - m_global) @ (m_k - m_global).T)\n",
        "  Sb += Sb_k\n",
        "\n",
        "# Sw^-1 * Sb * W = lambda * w\n",
        "# Av = lambda * v (forma de autovalor-autovetor)\n",
        "# Vemos que W é o autovetor associado ao autovalor não nulo lambda\n",
        "Sw_inversa = np.linalg.inv(Sw)\n",
        "A = Sw_inversa @ Sb\n",
        "\n",
        "autovalores, autovetores = np.linalg.eig(A)\n",
        "\n",
        "autovalores = autovalores.real\n",
        "autovetores = autovetores.real\n",
        "\n",
        "indices_ordenados = np.argsort(autovalores)[::-1]\n",
        "autovalores = autovalores[indices_ordenados]\n",
        "autovetores = autovetores[:, indices_ordenados]\n",
        "\n",
        "# Somente um autovalor é não-nulo\n",
        "print(autovalores)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W = autovetores[:, :1]\n",
        "\n",
        "# Agora que temos o vetor W, vamos aplicar a transformação\n",
        "# yn = W.T @ Xn\n",
        "\n",
        "y_treino = X_raw @ W\n",
        "y_val = x_val_np @ W\n",
        "y_teste = x_teste_np @ W\n",
        "\n",
        "x_treino_lda = torch.tensor(y_treino, dtype=torch.float32)\n",
        "d_treino_tensor = torch.tensor(d_treino_np, dtype=torch.long)\n",
        "\n",
        "x_val_lda = torch.tensor(y_val, dtype=torch.float32)\n",
        "d_val_tensor = torch.tensor(d_val_np, dtype=torch.long)\n",
        "\n",
        "x_teste_lda = torch.tensor(y_teste, dtype=torch.float32)\n",
        "d_teste_tensor = torch.tensor(d_teste_np, dtype=torch.long)\n"
      ],
      "metadata": {
        "id": "y52SbmHpWdNk"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelLDA(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.model = nn.Sequential(\n",
        "        nn.Linear(1, 8),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.Linear(8, 4),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.Linear(4, 4),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.Linear(4, 2),\n",
        "    )\n",
        "\n",
        "    self._init_weights()\n",
        "\n",
        "  def _init_weights(self):\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Linear):\n",
        "        nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
        "\n",
        "        # Inicializa os bias com zero\n",
        "        if m.bias is not None:\n",
        "          nn.init.constant_(m.bias, 0)\n",
        "\n",
        "  def forward(self, x):\n",
        "    output = self.model(x)\n",
        "    return output"
      ],
      "metadata": {
        "id": "b9oAUoIShtgP"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Aqui muda\n",
        "model_lda = ModelLDA().to(device=device)\n",
        "\n",
        "# Taxa de aprendizado\n",
        "eta = 0.001\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_lda.parameters(), lr=eta)\n",
        "\n",
        "Nb = 64 # Tamanho do mini-batch\n",
        "Ne = 1000 # Número de épocas\n"
      ],
      "metadata": {
        "id": "49WVuSNMi4Xo"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_set_lda = TensorDataset(x_treino_lda, d_treino_tensor)\n",
        "train_loader_lda = torch.utils.data.DataLoader(train_set_lda, batch_size=Nb, shuffle=True)"
      ],
      "metadata": {
        "id": "WiGcFSy5i88d"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Treinamento\n",
        "losses = []\n",
        "val_losses = []\n",
        "\n",
        "x_val_lda = x_val_lda.to(device=device)\n",
        "d_val_tensor = d_val_tensor.to(device=device)\n",
        "\n",
        "for epoch in range(Ne):\n",
        "  for n, (X, d) in enumerate(train_loader_lda):\n",
        "\n",
        "    X = X.to(device=device)\n",
        "    d = d.to(device=device)\n",
        "\n",
        "    # Treinamento\n",
        "    model_lda.train()\n",
        "    model_lda.zero_grad()\n",
        "    y = model_lda(X)\n",
        "    loss = loss_function(y, d)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Validação\n",
        "    model_lda.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      y_val = model_lda(x_val_lda)\n",
        "      val_loss = loss_function(y_val, d_val_tensor)\n",
        "\n",
        "    losses.append(loss.item())\n",
        "    val_losses.append(val_loss.item())\n",
        "\n",
        "    if epoch % 1 == 0 and n == x_treino_lda.shape[0]//Nb - 1:\n",
        "      print(f\"Epoch: {epoch} | Loss: {loss} | Val. Loss: {val_loss}\")\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(losses)\n",
        "plt.plot(val_losses, alpha=0.8)\n",
        "plt.legend([\"Loss\", \"Val. Loss\"])\n",
        "plt.xlabel(\"Batch\")\n",
        "plt.ylabel(\"Loss\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "K0hHNJs9jC_i",
        "outputId": "cf11b504-7734-4be5-c97d-2f8f5f9ef33d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | Loss: 0.7888780236244202 | Val. Loss: 0.9000808000564575\n",
            "Epoch: 1 | Loss: 0.7823255658149719 | Val. Loss: 0.8310191035270691\n",
            "Epoch: 2 | Loss: 0.7842394113540649 | Val. Loss: 0.7755982875823975\n",
            "Epoch: 3 | Loss: 0.7573270797729492 | Val. Loss: 0.731992244720459\n",
            "Epoch: 4 | Loss: 0.7155287861824036 | Val. Loss: 0.6985521912574768\n",
            "Epoch: 5 | Loss: 0.6654696464538574 | Val. Loss: 0.6719021201133728\n",
            "Epoch: 6 | Loss: 0.6633478403091431 | Val. Loss: 0.6499989628791809\n",
            "Epoch: 7 | Loss: 0.6159043908119202 | Val. Loss: 0.633156955242157\n",
            "Epoch: 8 | Loss: 0.6263265609741211 | Val. Loss: 0.6211758852005005\n",
            "Epoch: 9 | Loss: 0.5996385216712952 | Val. Loss: 0.6101387739181519\n",
            "Epoch: 10 | Loss: 0.5482754111289978 | Val. Loss: 0.5993477702140808\n",
            "Epoch: 11 | Loss: 0.582712709903717 | Val. Loss: 0.589074969291687\n",
            "Epoch: 12 | Loss: 0.5098328590393066 | Val. Loss: 0.5793178081512451\n",
            "Epoch: 13 | Loss: 0.5244101285934448 | Val. Loss: 0.5699729919433594\n",
            "Epoch: 14 | Loss: 0.5759801864624023 | Val. Loss: 0.5607138872146606\n",
            "Epoch: 15 | Loss: 0.580255389213562 | Val. Loss: 0.5517525672912598\n",
            "Epoch: 16 | Loss: 0.505505383014679 | Val. Loss: 0.5432151556015015\n",
            "Epoch: 17 | Loss: 0.5096849203109741 | Val. Loss: 0.5349506139755249\n",
            "Epoch: 18 | Loss: 0.5872378349304199 | Val. Loss: 0.526842474937439\n",
            "Epoch: 19 | Loss: 0.49591630697250366 | Val. Loss: 0.5191174149513245\n",
            "Epoch: 20 | Loss: 0.4780026972293854 | Val. Loss: 0.511610746383667\n",
            "Epoch: 21 | Loss: 0.46195676922798157 | Val. Loss: 0.5043684840202332\n",
            "Epoch: 22 | Loss: 0.5298485159873962 | Val. Loss: 0.4974035322666168\n",
            "Epoch: 23 | Loss: 0.5044765472412109 | Val. Loss: 0.4905766546726227\n",
            "Epoch: 24 | Loss: 0.4778555631637573 | Val. Loss: 0.48396021127700806\n",
            "Epoch: 25 | Loss: 0.46446356177330017 | Val. Loss: 0.4776723384857178\n",
            "Epoch: 26 | Loss: 0.40822312235832214 | Val. Loss: 0.47156357765197754\n",
            "Epoch: 27 | Loss: 0.45367148518562317 | Val. Loss: 0.4657011926174164\n",
            "Epoch: 28 | Loss: 0.42052099108695984 | Val. Loss: 0.45990896224975586\n",
            "Epoch: 29 | Loss: 0.4319465756416321 | Val. Loss: 0.4544433057308197\n",
            "Epoch: 30 | Loss: 0.44477784633636475 | Val. Loss: 0.44923901557922363\n",
            "Epoch: 31 | Loss: 0.3504858911037445 | Val. Loss: 0.44433385133743286\n",
            "Epoch: 32 | Loss: 0.39170345664024353 | Val. Loss: 0.4391709268093109\n",
            "Epoch: 33 | Loss: 0.40180787444114685 | Val. Loss: 0.4345645308494568\n",
            "Epoch: 34 | Loss: 0.39912182092666626 | Val. Loss: 0.4299151301383972\n",
            "Epoch: 35 | Loss: 0.37828904390335083 | Val. Loss: 0.42566871643066406\n",
            "Epoch: 36 | Loss: 0.40233904123306274 | Val. Loss: 0.42134889960289\n",
            "Epoch: 37 | Loss: 0.3750762939453125 | Val. Loss: 0.41717439889907837\n",
            "Epoch: 38 | Loss: 0.34107646346092224 | Val. Loss: 0.41325125098228455\n",
            "Epoch: 39 | Loss: 0.33728349208831787 | Val. Loss: 0.4094611704349518\n",
            "Epoch: 40 | Loss: 0.39027029275894165 | Val. Loss: 0.40601658821105957\n",
            "Epoch: 41 | Loss: 0.33045312762260437 | Val. Loss: 0.40266913175582886\n",
            "Epoch: 42 | Loss: 0.36269840598106384 | Val. Loss: 0.3995492458343506\n",
            "Epoch: 43 | Loss: 0.3536304533481598 | Val. Loss: 0.3965095281600952\n",
            "Epoch: 44 | Loss: 0.3533857762813568 | Val. Loss: 0.39365923404693604\n",
            "Epoch: 45 | Loss: 0.42987650632858276 | Val. Loss: 0.3909173309803009\n",
            "Epoch: 46 | Loss: 0.44106295704841614 | Val. Loss: 0.3882526159286499\n",
            "Epoch: 47 | Loss: 0.4633089601993561 | Val. Loss: 0.3856180012226105\n",
            "Epoch: 48 | Loss: 0.3249998688697815 | Val. Loss: 0.3833294212818146\n",
            "Epoch: 49 | Loss: 0.30144616961479187 | Val. Loss: 0.3811611831188202\n",
            "Epoch: 50 | Loss: 0.31529486179351807 | Val. Loss: 0.3791772425174713\n",
            "Epoch: 51 | Loss: 0.4477998614311218 | Val. Loss: 0.37715429067611694\n",
            "Epoch: 52 | Loss: 0.30821818113327026 | Val. Loss: 0.3754257559776306\n",
            "Epoch: 53 | Loss: 0.3111531138420105 | Val. Loss: 0.37359434366226196\n",
            "Epoch: 54 | Loss: 0.38523411750793457 | Val. Loss: 0.3719829320907593\n",
            "Epoch: 55 | Loss: 0.281418114900589 | Val. Loss: 0.37013086676597595\n",
            "Epoch: 56 | Loss: 0.282884806394577 | Val. Loss: 0.36842742562294006\n",
            "Epoch: 57 | Loss: 0.269072949886322 | Val. Loss: 0.3667387068271637\n",
            "Epoch: 58 | Loss: 0.3861280679702759 | Val. Loss: 0.36536142230033875\n",
            "Epoch: 59 | Loss: 0.32923150062561035 | Val. Loss: 0.3642844557762146\n",
            "Epoch: 60 | Loss: 0.2618737816810608 | Val. Loss: 0.3633420765399933\n",
            "Epoch: 61 | Loss: 0.3973965644836426 | Val. Loss: 0.3625803589820862\n",
            "Epoch: 62 | Loss: 0.26788097620010376 | Val. Loss: 0.3619103729724884\n",
            "Epoch: 63 | Loss: 0.31429746747016907 | Val. Loss: 0.3613813519477844\n",
            "Epoch: 64 | Loss: 0.3088149130344391 | Val. Loss: 0.36084944009780884\n",
            "Epoch: 65 | Loss: 0.35529395937919617 | Val. Loss: 0.3605366349220276\n",
            "Epoch: 66 | Loss: 0.38887861371040344 | Val. Loss: 0.36010223627090454\n",
            "Epoch: 67 | Loss: 0.19094140827655792 | Val. Loss: 0.35986974835395813\n",
            "Epoch: 68 | Loss: 0.25654706358909607 | Val. Loss: 0.3596460223197937\n",
            "Epoch: 69 | Loss: 0.3113579750061035 | Val. Loss: 0.35956162214279175\n",
            "Epoch: 70 | Loss: 0.3371052145957947 | Val. Loss: 0.3594467043876648\n",
            "Epoch: 71 | Loss: 0.2633216977119446 | Val. Loss: 0.3593883812427521\n",
            "Epoch: 72 | Loss: 0.2680744528770447 | Val. Loss: 0.35945647954940796\n",
            "Epoch: 73 | Loss: 0.28704386949539185 | Val. Loss: 0.3594302535057068\n",
            "Epoch: 74 | Loss: 0.44840943813323975 | Val. Loss: 0.3594844937324524\n",
            "Epoch: 75 | Loss: 0.2918819189071655 | Val. Loss: 0.3595162034034729\n",
            "Epoch: 76 | Loss: 0.27387627959251404 | Val. Loss: 0.35956746339797974\n",
            "Epoch: 77 | Loss: 0.24359580874443054 | Val. Loss: 0.35963502526283264\n",
            "Epoch: 78 | Loss: 0.30412226915359497 | Val. Loss: 0.3597416877746582\n",
            "Epoch: 79 | Loss: 0.2705064117908478 | Val. Loss: 0.35981374979019165\n",
            "Epoch: 80 | Loss: 0.34222522377967834 | Val. Loss: 0.3598690330982208\n",
            "Epoch: 81 | Loss: 0.27503836154937744 | Val. Loss: 0.3599798083305359\n",
            "Epoch: 82 | Loss: 0.30858057737350464 | Val. Loss: 0.36013174057006836\n",
            "Epoch: 83 | Loss: 0.33967071771621704 | Val. Loss: 0.360281765460968\n",
            "Epoch: 84 | Loss: 0.3495035469532013 | Val. Loss: 0.360382616519928\n",
            "Epoch: 85 | Loss: 0.3058606684207916 | Val. Loss: 0.360628604888916\n",
            "Epoch: 86 | Loss: 0.28756287693977356 | Val. Loss: 0.36057960987091064\n",
            "Epoch: 87 | Loss: 0.39904698729515076 | Val. Loss: 0.36075249314308167\n",
            "Epoch: 88 | Loss: 0.27119994163513184 | Val. Loss: 0.3607923984527588\n",
            "Epoch: 89 | Loss: 0.31514406204223633 | Val. Loss: 0.3608792722225189\n",
            "Epoch: 90 | Loss: 0.32699012756347656 | Val. Loss: 0.3610076606273651\n",
            "Epoch: 91 | Loss: 0.2789653539657593 | Val. Loss: 0.36115291714668274\n",
            "Epoch: 92 | Loss: 0.30325818061828613 | Val. Loss: 0.36119917035102844\n",
            "Epoch: 93 | Loss: 0.3063086271286011 | Val. Loss: 0.36126086115837097\n",
            "Epoch: 94 | Loss: 0.3418477773666382 | Val. Loss: 0.36143940687179565\n",
            "Epoch: 95 | Loss: 0.3486006259918213 | Val. Loss: 0.36152324080467224\n",
            "Epoch: 96 | Loss: 0.30599552392959595 | Val. Loss: 0.36160343885421753\n",
            "Epoch: 97 | Loss: 0.35793399810791016 | Val. Loss: 0.36168134212493896\n",
            "Epoch: 98 | Loss: 0.4451007544994354 | Val. Loss: 0.36182907223701477\n",
            "Epoch: 99 | Loss: 0.243652805685997 | Val. Loss: 0.36202070116996765\n",
            "Epoch: 100 | Loss: 0.29940155148506165 | Val. Loss: 0.3620709776878357\n",
            "Epoch: 101 | Loss: 0.24672195315361023 | Val. Loss: 0.3621111512184143\n",
            "Epoch: 102 | Loss: 0.2978564202785492 | Val. Loss: 0.36215490102767944\n",
            "Epoch: 103 | Loss: 0.3554721772670746 | Val. Loss: 0.36230966448783875\n",
            "Epoch: 104 | Loss: 0.27476897835731506 | Val. Loss: 0.3623690903186798\n",
            "Epoch: 105 | Loss: 0.34216126799583435 | Val. Loss: 0.36243608593940735\n",
            "Epoch: 106 | Loss: 0.2813114523887634 | Val. Loss: 0.36245614290237427\n",
            "Epoch: 107 | Loss: 0.29259616136550903 | Val. Loss: 0.36250701546669006\n",
            "Epoch: 108 | Loss: 0.2928292453289032 | Val. Loss: 0.36261290311813354\n",
            "Epoch: 109 | Loss: 0.36682775616645813 | Val. Loss: 0.3627195954322815\n",
            "Epoch: 110 | Loss: 0.2809208333492279 | Val. Loss: 0.36272329092025757\n",
            "Epoch: 111 | Loss: 0.3792955279350281 | Val. Loss: 0.36285144090652466\n",
            "Epoch: 112 | Loss: 0.3080475926399231 | Val. Loss: 0.36287111043930054\n",
            "Epoch: 113 | Loss: 0.3634117543697357 | Val. Loss: 0.3629164397716522\n",
            "Epoch: 114 | Loss: 0.30662789940834045 | Val. Loss: 0.36302995681762695\n",
            "Epoch: 115 | Loss: 0.25604259967803955 | Val. Loss: 0.36298584938049316\n",
            "Epoch: 116 | Loss: 0.4053293466567993 | Val. Loss: 0.36302560567855835\n",
            "Epoch: 117 | Loss: 0.2726707458496094 | Val. Loss: 0.36302846670150757\n",
            "Epoch: 118 | Loss: 0.2589972913265228 | Val. Loss: 0.3630894124507904\n",
            "Epoch: 119 | Loss: 0.3062782883644104 | Val. Loss: 0.3632377088069916\n",
            "Epoch: 120 | Loss: 0.30632367730140686 | Val. Loss: 0.36333170533180237\n",
            "Epoch: 121 | Loss: 0.3516647517681122 | Val. Loss: 0.36321109533309937\n",
            "Epoch: 122 | Loss: 0.30651992559432983 | Val. Loss: 0.3632829785346985\n",
            "Epoch: 123 | Loss: 0.21103690564632416 | Val. Loss: 0.3632417321205139\n",
            "Epoch: 124 | Loss: 0.44248026609420776 | Val. Loss: 0.36332961916923523\n",
            "Epoch: 125 | Loss: 0.263772189617157 | Val. Loss: 0.36334937810897827\n",
            "Epoch: 126 | Loss: 0.35262298583984375 | Val. Loss: 0.3634602427482605\n",
            "Epoch: 127 | Loss: 0.5123130679130554 | Val. Loss: 0.3634025752544403\n",
            "Epoch: 128 | Loss: 0.2954689562320709 | Val. Loss: 0.36343955993652344\n",
            "Epoch: 129 | Loss: 0.3696414530277252 | Val. Loss: 0.36352092027664185\n",
            "Epoch: 130 | Loss: 0.3858611583709717 | Val. Loss: 0.36368077993392944\n",
            "Epoch: 131 | Loss: 0.3109404146671295 | Val. Loss: 0.3635598421096802\n",
            "Epoch: 132 | Loss: 0.27211228013038635 | Val. Loss: 0.3635424077510834\n",
            "Epoch: 133 | Loss: 0.26688411831855774 | Val. Loss: 0.36362528800964355\n",
            "Epoch: 134 | Loss: 0.4590051472187042 | Val. Loss: 0.36362701654434204\n",
            "Epoch: 135 | Loss: 0.3368440568447113 | Val. Loss: 0.363659530878067\n",
            "Epoch: 136 | Loss: 0.2518320381641388 | Val. Loss: 0.36376887559890747\n",
            "Epoch: 137 | Loss: 0.34239786863327026 | Val. Loss: 0.36366623640060425\n",
            "Epoch: 138 | Loss: 0.31394389271736145 | Val. Loss: 0.3636719584465027\n",
            "Epoch: 139 | Loss: 0.26264333724975586 | Val. Loss: 0.363777220249176\n",
            "Epoch: 140 | Loss: 0.24193862080574036 | Val. Loss: 0.3638446033000946\n",
            "Epoch: 141 | Loss: 0.298703134059906 | Val. Loss: 0.36380425095558167\n",
            "Epoch: 142 | Loss: 0.33012154698371887 | Val. Loss: 0.36385777592658997\n",
            "Epoch: 143 | Loss: 0.27330586314201355 | Val. Loss: 0.3638600707054138\n",
            "Epoch: 144 | Loss: 0.2146872878074646 | Val. Loss: 0.36385372281074524\n",
            "Epoch: 145 | Loss: 0.36709731817245483 | Val. Loss: 0.36394280195236206\n",
            "Epoch: 146 | Loss: 0.38728001713752747 | Val. Loss: 0.36399880051612854\n",
            "Epoch: 147 | Loss: 0.21658174693584442 | Val. Loss: 0.363840252161026\n",
            "Epoch: 148 | Loss: 0.4506971538066864 | Val. Loss: 0.36381036043167114\n",
            "Epoch: 149 | Loss: 0.30588090419769287 | Val. Loss: 0.36369234323501587\n",
            "Epoch: 150 | Loss: 0.39749401807785034 | Val. Loss: 0.36366263031959534\n",
            "Epoch: 151 | Loss: 0.39954644441604614 | Val. Loss: 0.3637532889842987\n",
            "Epoch: 152 | Loss: 0.17116615176200867 | Val. Loss: 0.3635167181491852\n",
            "Epoch: 153 | Loss: 0.31276845932006836 | Val. Loss: 0.36345088481903076\n",
            "Epoch: 154 | Loss: 0.3144720792770386 | Val. Loss: 0.363361656665802\n",
            "Epoch: 155 | Loss: 0.2463395595550537 | Val. Loss: 0.36332982778549194\n",
            "Epoch: 156 | Loss: 0.32994693517684937 | Val. Loss: 0.3632041811943054\n",
            "Epoch: 157 | Loss: 0.3757692277431488 | Val. Loss: 0.36317628622055054\n",
            "Epoch: 158 | Loss: 0.37303853034973145 | Val. Loss: 0.3630490005016327\n",
            "Epoch: 159 | Loss: 0.32880738377571106 | Val. Loss: 0.3628677427768707\n",
            "Epoch: 160 | Loss: 0.30556681752204895 | Val. Loss: 0.3629315197467804\n",
            "Epoch: 161 | Loss: 0.45653802156448364 | Val. Loss: 0.36299413442611694\n",
            "Epoch: 162 | Loss: 0.24435164034366608 | Val. Loss: 0.36291301250457764\n",
            "Epoch: 163 | Loss: 0.25543782114982605 | Val. Loss: 0.36297374963760376\n",
            "Epoch: 164 | Loss: 0.32795044779777527 | Val. Loss: 0.36281052231788635\n",
            "Epoch: 165 | Loss: 0.3147122859954834 | Val. Loss: 0.36280256509780884\n",
            "Epoch: 166 | Loss: 0.29119545221328735 | Val. Loss: 0.3628397583961487\n",
            "Epoch: 167 | Loss: 0.3403211236000061 | Val. Loss: 0.3629130721092224\n",
            "Epoch: 168 | Loss: 0.3067646920681 | Val. Loss: 0.36284780502319336\n",
            "Epoch: 169 | Loss: 0.27877089381217957 | Val. Loss: 0.36284974217414856\n",
            "Epoch: 170 | Loss: 0.3819724917411804 | Val. Loss: 0.3630310893058777\n",
            "Epoch: 171 | Loss: 0.36377397179603577 | Val. Loss: 0.3629022240638733\n",
            "Epoch: 172 | Loss: 0.3611180782318115 | Val. Loss: 0.362907350063324\n",
            "Epoch: 173 | Loss: 0.19847874343395233 | Val. Loss: 0.36298561096191406\n",
            "Epoch: 174 | Loss: 0.3445715010166168 | Val. Loss: 0.36282554268836975\n",
            "Epoch: 175 | Loss: 0.2862143814563751 | Val. Loss: 0.3628634810447693\n",
            "Epoch: 176 | Loss: 0.23984865844249725 | Val. Loss: 0.36293357610702515\n",
            "Epoch: 177 | Loss: 0.28435900807380676 | Val. Loss: 0.3628966212272644\n",
            "Epoch: 178 | Loss: 0.22993044555187225 | Val. Loss: 0.3630141615867615\n",
            "Epoch: 179 | Loss: 0.39034026861190796 | Val. Loss: 0.36282864212989807\n",
            "Epoch: 180 | Loss: 0.3795817792415619 | Val. Loss: 0.362875759601593\n",
            "Epoch: 181 | Loss: 0.2907508313655853 | Val. Loss: 0.3627221882343292\n",
            "Epoch: 182 | Loss: 0.3238728642463684 | Val. Loss: 0.3627774119377136\n",
            "Epoch: 183 | Loss: 0.36693915724754333 | Val. Loss: 0.3628639876842499\n",
            "Epoch: 184 | Loss: 0.2039678990840912 | Val. Loss: 0.363030344247818\n",
            "Epoch: 185 | Loss: 0.30794575810432434 | Val. Loss: 0.3629494607448578\n",
            "Epoch: 186 | Loss: 0.23078176379203796 | Val. Loss: 0.3630319833755493\n",
            "Epoch: 187 | Loss: 0.4117186963558197 | Val. Loss: 0.36307066679000854\n",
            "Epoch: 188 | Loss: 0.36060261726379395 | Val. Loss: 0.36297544836997986\n",
            "Epoch: 189 | Loss: 0.4181116819381714 | Val. Loss: 0.36304932832717896\n",
            "Epoch: 190 | Loss: 0.2938772141933441 | Val. Loss: 0.3630553185939789\n",
            "Epoch: 191 | Loss: 0.4820568561553955 | Val. Loss: 0.36301761865615845\n",
            "Epoch: 192 | Loss: 0.37865859270095825 | Val. Loss: 0.36295780539512634\n",
            "Epoch: 193 | Loss: 0.197445347905159 | Val. Loss: 0.3630579113960266\n",
            "Epoch: 194 | Loss: 0.47242262959480286 | Val. Loss: 0.36314746737480164\n",
            "Epoch: 195 | Loss: 0.4135226011276245 | Val. Loss: 0.36312758922576904\n",
            "Epoch: 196 | Loss: 0.2826438546180725 | Val. Loss: 0.36321380734443665\n",
            "Epoch: 197 | Loss: 0.24760878086090088 | Val. Loss: 0.36309048533439636\n",
            "Epoch: 198 | Loss: 0.38824662566185 | Val. Loss: 0.3632346987724304\n",
            "Epoch: 199 | Loss: 0.3418014645576477 | Val. Loss: 0.3632316291332245\n",
            "Epoch: 200 | Loss: 0.1577521115541458 | Val. Loss: 0.36313527822494507\n",
            "Epoch: 201 | Loss: 0.36107301712036133 | Val. Loss: 0.3632887005805969\n",
            "Epoch: 202 | Loss: 0.27774426341056824 | Val. Loss: 0.3632960915565491\n",
            "Epoch: 203 | Loss: 0.2496299147605896 | Val. Loss: 0.3632163107395172\n",
            "Epoch: 204 | Loss: 0.2691291868686676 | Val. Loss: 0.36324021220207214\n",
            "Epoch: 205 | Loss: 0.3156183958053589 | Val. Loss: 0.3633069396018982\n",
            "Epoch: 206 | Loss: 0.34132540225982666 | Val. Loss: 0.3632056713104248\n",
            "Epoch: 207 | Loss: 0.43355417251586914 | Val. Loss: 0.36343690752983093\n",
            "Epoch: 208 | Loss: 0.35421663522720337 | Val. Loss: 0.3632845878601074\n",
            "Epoch: 209 | Loss: 0.2957955598831177 | Val. Loss: 0.3631654381752014\n",
            "Epoch: 210 | Loss: 0.27490052580833435 | Val. Loss: 0.36328214406967163\n",
            "Epoch: 211 | Loss: 0.46043550968170166 | Val. Loss: 0.363364577293396\n",
            "Epoch: 212 | Loss: 0.28081196546554565 | Val. Loss: 0.3631596565246582\n",
            "Epoch: 213 | Loss: 0.31737077236175537 | Val. Loss: 0.3632260262966156\n",
            "Epoch: 214 | Loss: 0.28903430700302124 | Val. Loss: 0.36323150992393494\n",
            "Epoch: 215 | Loss: 0.28234705328941345 | Val. Loss: 0.36323896050453186\n",
            "Epoch: 216 | Loss: 0.3736136853694916 | Val. Loss: 0.3633347153663635\n",
            "Epoch: 217 | Loss: 0.2909512221813202 | Val. Loss: 0.36326318979263306\n",
            "Epoch: 218 | Loss: 0.2728644609451294 | Val. Loss: 0.3633403182029724\n",
            "Epoch: 219 | Loss: 0.3243395984172821 | Val. Loss: 0.363250195980072\n",
            "Epoch: 220 | Loss: 0.3391474187374115 | Val. Loss: 0.3631598651409149\n",
            "Epoch: 221 | Loss: 0.2992597818374634 | Val. Loss: 0.36305850744247437\n",
            "Epoch: 222 | Loss: 0.33334213495254517 | Val. Loss: 0.36326295137405396\n",
            "Epoch: 223 | Loss: 0.2559349536895752 | Val. Loss: 0.3632130026817322\n",
            "Epoch: 224 | Loss: 0.39468011260032654 | Val. Loss: 0.36318379640579224\n",
            "Epoch: 225 | Loss: 0.3209310472011566 | Val. Loss: 0.3632710576057434\n",
            "Epoch: 226 | Loss: 0.3206224739551544 | Val. Loss: 0.36313730478286743\n",
            "Epoch: 227 | Loss: 0.3337012231349945 | Val. Loss: 0.36325177550315857\n",
            "Epoch: 228 | Loss: 0.3326120376586914 | Val. Loss: 0.3633262813091278\n",
            "Epoch: 229 | Loss: 0.3569549322128296 | Val. Loss: 0.3631743788719177\n",
            "Epoch: 230 | Loss: 0.4346831738948822 | Val. Loss: 0.3632432520389557\n",
            "Epoch: 231 | Loss: 0.30306026339530945 | Val. Loss: 0.36333101987838745\n",
            "Epoch: 232 | Loss: 0.23414796590805054 | Val. Loss: 0.3629973530769348\n",
            "Epoch: 233 | Loss: 0.21705186367034912 | Val. Loss: 0.3631165325641632\n",
            "Epoch: 234 | Loss: 0.34025996923446655 | Val. Loss: 0.3631882667541504\n",
            "Epoch: 235 | Loss: 0.3038213849067688 | Val. Loss: 0.3630818724632263\n",
            "Epoch: 236 | Loss: 0.27645617723464966 | Val. Loss: 0.36313456296920776\n",
            "Epoch: 237 | Loss: 0.2614814341068268 | Val. Loss: 0.36302122473716736\n",
            "Epoch: 238 | Loss: 0.25649821758270264 | Val. Loss: 0.3631002902984619\n",
            "Epoch: 239 | Loss: 0.47272300720214844 | Val. Loss: 0.36315128207206726\n",
            "Epoch: 240 | Loss: 0.3894706070423126 | Val. Loss: 0.3631810247898102\n",
            "Epoch: 241 | Loss: 0.25436437129974365 | Val. Loss: 0.3630453944206238\n",
            "Epoch: 242 | Loss: 0.2515493929386139 | Val. Loss: 0.36295729875564575\n",
            "Epoch: 243 | Loss: 0.3074408769607544 | Val. Loss: 0.362968385219574\n",
            "Epoch: 244 | Loss: 0.37907272577285767 | Val. Loss: 0.36302101612091064\n",
            "Epoch: 245 | Loss: 0.25669148564338684 | Val. Loss: 0.3630048930644989\n",
            "Epoch: 246 | Loss: 0.2420525997877121 | Val. Loss: 0.3627590537071228\n",
            "Epoch: 247 | Loss: 0.3236585557460785 | Val. Loss: 0.3628390431404114\n",
            "Epoch: 248 | Loss: 0.303684800863266 | Val. Loss: 0.3628060221672058\n",
            "Epoch: 249 | Loss: 0.21191884577274323 | Val. Loss: 0.36273857951164246\n",
            "Epoch: 250 | Loss: 0.3163814842700958 | Val. Loss: 0.3627503514289856\n",
            "Epoch: 251 | Loss: 0.3347817361354828 | Val. Loss: 0.36250635981559753\n",
            "Epoch: 252 | Loss: 0.30373525619506836 | Val. Loss: 0.3623844385147095\n",
            "Epoch: 253 | Loss: 0.4403259754180908 | Val. Loss: 0.36243516206741333\n",
            "Epoch: 254 | Loss: 0.3231663405895233 | Val. Loss: 0.3622448146343231\n",
            "Epoch: 255 | Loss: 0.32345184683799744 | Val. Loss: 0.3622311055660248\n",
            "Epoch: 256 | Loss: 0.25218236446380615 | Val. Loss: 0.36228808760643005\n",
            "Epoch: 257 | Loss: 0.4057227075099945 | Val. Loss: 0.36238402128219604\n",
            "Epoch: 258 | Loss: 0.36750340461730957 | Val. Loss: 0.3622322082519531\n",
            "Epoch: 259 | Loss: 0.34788286685943604 | Val. Loss: 0.3621533215045929\n",
            "Epoch: 260 | Loss: 0.29993826150894165 | Val. Loss: 0.3622806668281555\n",
            "Epoch: 261 | Loss: 0.28195270895957947 | Val. Loss: 0.36198875308036804\n",
            "Epoch: 262 | Loss: 0.3692704737186432 | Val. Loss: 0.3619888424873352\n",
            "Epoch: 263 | Loss: 0.3935929238796234 | Val. Loss: 0.36211615800857544\n",
            "Epoch: 264 | Loss: 0.29948264360427856 | Val. Loss: 0.3620336055755615\n",
            "Epoch: 265 | Loss: 0.32177433371543884 | Val. Loss: 0.3620910942554474\n",
            "Epoch: 266 | Loss: 0.32046273350715637 | Val. Loss: 0.3620855212211609\n",
            "Epoch: 267 | Loss: 0.3888157308101654 | Val. Loss: 0.3618883490562439\n",
            "Epoch: 268 | Loss: 0.2511213719844818 | Val. Loss: 0.36194074153900146\n",
            "Epoch: 269 | Loss: 0.3024030923843384 | Val. Loss: 0.36196663975715637\n",
            "Epoch: 270 | Loss: 0.3002747893333435 | Val. Loss: 0.3620310127735138\n",
            "Epoch: 271 | Loss: 0.4014100730419159 | Val. Loss: 0.3620357811450958\n",
            "Epoch: 272 | Loss: 0.3413183093070984 | Val. Loss: 0.361971914768219\n",
            "Epoch: 273 | Loss: 0.48076802492141724 | Val. Loss: 0.36210718750953674\n",
            "Epoch: 274 | Loss: 0.32802528142929077 | Val. Loss: 0.3620855212211609\n",
            "Epoch: 275 | Loss: 0.38003015518188477 | Val. Loss: 0.3620642125606537\n",
            "Epoch: 276 | Loss: 0.2353917956352234 | Val. Loss: 0.3620125949382782\n",
            "Epoch: 277 | Loss: 0.3139092028141022 | Val. Loss: 0.36201736330986023\n",
            "Epoch: 278 | Loss: 0.3386010229587555 | Val. Loss: 0.36209172010421753\n",
            "Epoch: 279 | Loss: 0.337901771068573 | Val. Loss: 0.36204615235328674\n",
            "Epoch: 280 | Loss: 0.2633094787597656 | Val. Loss: 0.3620430529117584\n",
            "Epoch: 281 | Loss: 0.3378075957298279 | Val. Loss: 0.3620155453681946\n",
            "Epoch: 282 | Loss: 0.2653524875640869 | Val. Loss: 0.36181405186653137\n",
            "Epoch: 283 | Loss: 0.33135971426963806 | Val. Loss: 0.36202532052993774\n",
            "Epoch: 284 | Loss: 0.2365238517522812 | Val. Loss: 0.36193782091140747\n",
            "Epoch: 285 | Loss: 0.401292085647583 | Val. Loss: 0.36200815439224243\n",
            "Epoch: 286 | Loss: 0.3021652102470398 | Val. Loss: 0.3620264232158661\n",
            "Epoch: 287 | Loss: 0.4075194001197815 | Val. Loss: 0.361966609954834\n",
            "Epoch: 288 | Loss: 0.27727818489074707 | Val. Loss: 0.36190125346183777\n",
            "Epoch: 289 | Loss: 0.2757001519203186 | Val. Loss: 0.3618171811103821\n",
            "Epoch: 290 | Loss: 0.3676307499408722 | Val. Loss: 0.36195725202560425\n",
            "Epoch: 291 | Loss: 0.42394736409187317 | Val. Loss: 0.3620383143424988\n",
            "Epoch: 292 | Loss: 0.25876736640930176 | Val. Loss: 0.3620671033859253\n",
            "Epoch: 293 | Loss: 0.34438666701316833 | Val. Loss: 0.36205634474754333\n",
            "Epoch: 294 | Loss: 0.2881214916706085 | Val. Loss: 0.36203208565711975\n",
            "Epoch: 295 | Loss: 0.24221421778202057 | Val. Loss: 0.3618156313896179\n",
            "Epoch: 296 | Loss: 0.3571493923664093 | Val. Loss: 0.36212241649627686\n",
            "Epoch: 297 | Loss: 0.3566194176673889 | Val. Loss: 0.36208197474479675\n",
            "Epoch: 298 | Loss: 0.21586090326309204 | Val. Loss: 0.36204227805137634\n",
            "Epoch: 299 | Loss: 0.20563530921936035 | Val. Loss: 0.36193710565567017\n",
            "Epoch: 300 | Loss: 0.370982825756073 | Val. Loss: 0.3620552122592926\n",
            "Epoch: 301 | Loss: 0.2407979816198349 | Val. Loss: 0.3619043827056885\n",
            "Epoch: 302 | Loss: 0.2761492133140564 | Val. Loss: 0.36209622025489807\n",
            "Epoch: 303 | Loss: 0.35189470648765564 | Val. Loss: 0.3620174527168274\n",
            "Epoch: 304 | Loss: 0.2798764407634735 | Val. Loss: 0.3621046245098114\n",
            "Epoch: 305 | Loss: 0.3810725212097168 | Val. Loss: 0.36217179894447327\n",
            "Epoch: 306 | Loss: 0.2767699360847473 | Val. Loss: 0.3621883690357208\n",
            "Epoch: 307 | Loss: 0.26959195733070374 | Val. Loss: 0.3620372712612152\n",
            "Epoch: 308 | Loss: 0.2624277174472809 | Val. Loss: 0.3621135354042053\n",
            "Epoch: 309 | Loss: 0.27347230911254883 | Val. Loss: 0.36201682686805725\n",
            "Epoch: 310 | Loss: 0.44970613718032837 | Val. Loss: 0.36203035712242126\n",
            "Epoch: 311 | Loss: 0.3144415318965912 | Val. Loss: 0.36204254627227783\n",
            "Epoch: 312 | Loss: 0.3114703297615051 | Val. Loss: 0.36189910769462585\n",
            "Epoch: 313 | Loss: 0.3188714385032654 | Val. Loss: 0.3620900809764862\n",
            "Epoch: 314 | Loss: 0.35220083594322205 | Val. Loss: 0.3620447516441345\n",
            "Epoch: 315 | Loss: 0.37485194206237793 | Val. Loss: 0.36190444231033325\n",
            "Epoch: 316 | Loss: 0.3194179832935333 | Val. Loss: 0.3621307909488678\n",
            "Epoch: 317 | Loss: 0.331528902053833 | Val. Loss: 0.362071692943573\n",
            "Epoch: 318 | Loss: 0.3303137421607971 | Val. Loss: 0.3620254397392273\n",
            "Epoch: 319 | Loss: 0.2509697675704956 | Val. Loss: 0.3620617687702179\n",
            "Epoch: 320 | Loss: 0.340709388256073 | Val. Loss: 0.3619551658630371\n",
            "Epoch: 321 | Loss: 0.26246559619903564 | Val. Loss: 0.36187413334846497\n",
            "Epoch: 322 | Loss: 0.3259391784667969 | Val. Loss: 0.3618548512458801\n",
            "Epoch: 323 | Loss: 0.36362695693969727 | Val. Loss: 0.3621731400489807\n",
            "Epoch: 324 | Loss: 0.20938576757907867 | Val. Loss: 0.3619740307331085\n",
            "Epoch: 325 | Loss: 0.32144680619239807 | Val. Loss: 0.36183708906173706\n",
            "Epoch: 326 | Loss: 0.26572272181510925 | Val. Loss: 0.36174917221069336\n",
            "Epoch: 327 | Loss: 0.44864559173583984 | Val. Loss: 0.3619604706764221\n",
            "Epoch: 328 | Loss: 0.32627928256988525 | Val. Loss: 0.36192312836647034\n",
            "Epoch: 329 | Loss: 0.30438894033432007 | Val. Loss: 0.36176711320877075\n",
            "Epoch: 330 | Loss: 0.35438090562820435 | Val. Loss: 0.3618118464946747\n",
            "Epoch: 331 | Loss: 0.2843654751777649 | Val. Loss: 0.36187630891799927\n",
            "Epoch: 332 | Loss: 0.36507448554039 | Val. Loss: 0.3620043694972992\n",
            "Epoch: 333 | Loss: 0.3009454011917114 | Val. Loss: 0.3618500828742981\n",
            "Epoch: 334 | Loss: 0.2912907004356384 | Val. Loss: 0.36191806197166443\n",
            "Epoch: 335 | Loss: 0.32150694727897644 | Val. Loss: 0.3619927763938904\n",
            "Epoch: 336 | Loss: 0.27273988723754883 | Val. Loss: 0.3619452714920044\n",
            "Epoch: 337 | Loss: 0.30416974425315857 | Val. Loss: 0.361814945936203\n",
            "Epoch: 338 | Loss: 0.30166783928871155 | Val. Loss: 0.3618522584438324\n",
            "Epoch: 339 | Loss: 0.4039543569087982 | Val. Loss: 0.3618932366371155\n",
            "Epoch: 340 | Loss: 0.2907435894012451 | Val. Loss: 0.36190617084503174\n",
            "Epoch: 341 | Loss: 0.33650192618370056 | Val. Loss: 0.3619195818901062\n",
            "Epoch: 342 | Loss: 0.4004114270210266 | Val. Loss: 0.36190009117126465\n",
            "Epoch: 343 | Loss: 0.38587743043899536 | Val. Loss: 0.3617958128452301\n",
            "Epoch: 344 | Loss: 0.3364196717739105 | Val. Loss: 0.36172083020210266\n",
            "Epoch: 345 | Loss: 0.28937244415283203 | Val. Loss: 0.3616669178009033\n",
            "Epoch: 346 | Loss: 0.1741941124200821 | Val. Loss: 0.36179429292678833\n",
            "Epoch: 347 | Loss: 0.32175469398498535 | Val. Loss: 0.36175769567489624\n",
            "Epoch: 348 | Loss: 0.2648945748806 | Val. Loss: 0.3617587983608246\n",
            "Epoch: 349 | Loss: 0.298636257648468 | Val. Loss: 0.36178070306777954\n",
            "Epoch: 350 | Loss: 0.28870832920074463 | Val. Loss: 0.36169928312301636\n",
            "Epoch: 351 | Loss: 0.34612756967544556 | Val. Loss: 0.36183780431747437\n",
            "Epoch: 352 | Loss: 0.3721536695957184 | Val. Loss: 0.361824095249176\n",
            "Epoch: 353 | Loss: 0.29630714654922485 | Val. Loss: 0.3618057072162628\n",
            "Epoch: 354 | Loss: 0.2996951937675476 | Val. Loss: 0.36183661222457886\n",
            "Epoch: 355 | Loss: 0.3340056240558624 | Val. Loss: 0.3616645336151123\n",
            "Epoch: 356 | Loss: 0.3921470642089844 | Val. Loss: 0.36163872480392456\n",
            "Epoch: 357 | Loss: 0.3332422971725464 | Val. Loss: 0.3617473244667053\n",
            "Epoch: 358 | Loss: 0.3383796215057373 | Val. Loss: 0.3617302179336548\n",
            "Epoch: 359 | Loss: 0.23518308997154236 | Val. Loss: 0.3617636561393738\n",
            "Epoch: 360 | Loss: 0.332249253988266 | Val. Loss: 0.36179447174072266\n",
            "Epoch: 361 | Loss: 0.2845901548862457 | Val. Loss: 0.36160725355148315\n",
            "Epoch: 362 | Loss: 0.27906671166419983 | Val. Loss: 0.36152076721191406\n",
            "Epoch: 363 | Loss: 0.30796706676483154 | Val. Loss: 0.3615975081920624\n",
            "Epoch: 364 | Loss: 0.3333955407142639 | Val. Loss: 0.36183837056159973\n",
            "Epoch: 365 | Loss: 0.2638736665248871 | Val. Loss: 0.3617665767669678\n",
            "Epoch: 366 | Loss: 0.31898558139801025 | Val. Loss: 0.36179402470588684\n",
            "Epoch: 367 | Loss: 0.40778180956840515 | Val. Loss: 0.3615153729915619\n",
            "Epoch: 368 | Loss: 0.37846285104751587 | Val. Loss: 0.36152422428131104\n",
            "Epoch: 369 | Loss: 0.2736557722091675 | Val. Loss: 0.36179983615875244\n",
            "Epoch: 370 | Loss: 0.3256428837776184 | Val. Loss: 0.36149847507476807\n",
            "Epoch: 371 | Loss: 0.33245399594306946 | Val. Loss: 0.3615371584892273\n",
            "Epoch: 372 | Loss: 0.39926084876060486 | Val. Loss: 0.3615904450416565\n",
            "Epoch: 373 | Loss: 0.3118212819099426 | Val. Loss: 0.36157339811325073\n",
            "Epoch: 374 | Loss: 0.20900192856788635 | Val. Loss: 0.36145177483558655\n",
            "Epoch: 375 | Loss: 0.31040552258491516 | Val. Loss: 0.36159855127334595\n",
            "Epoch: 376 | Loss: 0.42723575234413147 | Val. Loss: 0.36157143115997314\n",
            "Epoch: 377 | Loss: 0.3514910042285919 | Val. Loss: 0.36152318120002747\n",
            "Epoch: 378 | Loss: 0.45008331537246704 | Val. Loss: 0.3614967465400696\n",
            "Epoch: 379 | Loss: 0.3421807885169983 | Val. Loss: 0.3615584969520569\n",
            "Epoch: 380 | Loss: 0.4180123507976532 | Val. Loss: 0.36157548427581787\n",
            "Epoch: 381 | Loss: 0.2612103521823883 | Val. Loss: 0.3614944517612457\n",
            "Epoch: 382 | Loss: 0.27476322650909424 | Val. Loss: 0.36132878065109253\n",
            "Epoch: 383 | Loss: 0.3329991400241852 | Val. Loss: 0.36143749952316284\n",
            "Epoch: 384 | Loss: 0.25514647364616394 | Val. Loss: 0.36139893531799316\n",
            "Epoch: 385 | Loss: 0.2697197198867798 | Val. Loss: 0.36136409640312195\n",
            "Epoch: 386 | Loss: 0.2266247421503067 | Val. Loss: 0.36153894662857056\n",
            "Epoch: 387 | Loss: 0.3651787042617798 | Val. Loss: 0.3613990247249603\n",
            "Epoch: 388 | Loss: 0.28743550181388855 | Val. Loss: 0.3613891303539276\n",
            "Epoch: 389 | Loss: 0.2861210107803345 | Val. Loss: 0.36148685216903687\n",
            "Epoch: 390 | Loss: 0.20588068664073944 | Val. Loss: 0.3614594340324402\n",
            "Epoch: 391 | Loss: 0.24718600511550903 | Val. Loss: 0.36149317026138306\n",
            "Epoch: 392 | Loss: 0.3488064706325531 | Val. Loss: 0.3616601228713989\n",
            "Epoch: 393 | Loss: 0.39680221676826477 | Val. Loss: 0.36148685216903687\n",
            "Epoch: 394 | Loss: 0.3479759395122528 | Val. Loss: 0.3613797724246979\n",
            "Epoch: 395 | Loss: 0.27575743198394775 | Val. Loss: 0.36130502820014954\n",
            "Epoch: 396 | Loss: 0.24083739519119263 | Val. Loss: 0.36140909790992737\n",
            "Epoch: 397 | Loss: 0.30874699354171753 | Val. Loss: 0.36146003007888794\n",
            "Epoch: 398 | Loss: 0.3937278091907501 | Val. Loss: 0.3613470196723938\n",
            "Epoch: 399 | Loss: 0.2606756389141083 | Val. Loss: 0.3615139126777649\n",
            "Epoch: 400 | Loss: 0.3867242634296417 | Val. Loss: 0.3615160882472992\n",
            "Epoch: 401 | Loss: 0.41451117396354675 | Val. Loss: 0.3614775538444519\n",
            "Epoch: 402 | Loss: 0.2775726020336151 | Val. Loss: 0.36122754216194153\n",
            "Epoch: 403 | Loss: 0.2956576943397522 | Val. Loss: 0.36116641759872437\n",
            "Epoch: 404 | Loss: 0.45569583773612976 | Val. Loss: 0.36138349771499634\n",
            "Epoch: 405 | Loss: 0.38331347703933716 | Val. Loss: 0.36134853959083557\n",
            "Epoch: 406 | Loss: 0.2709331214427948 | Val. Loss: 0.3614596128463745\n",
            "Epoch: 407 | Loss: 0.25586873292922974 | Val. Loss: 0.3613268733024597\n",
            "Epoch: 408 | Loss: 0.24734804034233093 | Val. Loss: 0.36136263608932495\n",
            "Epoch: 409 | Loss: 0.34812992811203003 | Val. Loss: 0.3614169955253601\n",
            "Epoch: 410 | Loss: 0.3720778524875641 | Val. Loss: 0.3612610697746277\n",
            "Epoch: 411 | Loss: 0.36551058292388916 | Val. Loss: 0.3613840639591217\n",
            "Epoch: 412 | Loss: 0.24396854639053345 | Val. Loss: 0.3612559139728546\n",
            "Epoch: 413 | Loss: 0.3601401746273041 | Val. Loss: 0.36136868596076965\n",
            "Epoch: 414 | Loss: 0.24822945892810822 | Val. Loss: 0.3612819314002991\n",
            "Epoch: 415 | Loss: 0.49660181999206543 | Val. Loss: 0.36138638854026794\n",
            "Epoch: 416 | Loss: 0.37596407532691956 | Val. Loss: 0.36118966341018677\n",
            "Epoch: 417 | Loss: 0.29501795768737793 | Val. Loss: 0.3611825704574585\n",
            "Epoch: 418 | Loss: 0.27288925647735596 | Val. Loss: 0.3612101674079895\n",
            "Epoch: 419 | Loss: 0.36505746841430664 | Val. Loss: 0.36130207777023315\n",
            "Epoch: 420 | Loss: 0.2879190444946289 | Val. Loss: 0.36141034960746765\n",
            "Epoch: 421 | Loss: 0.28621459007263184 | Val. Loss: 0.3613504469394684\n",
            "Epoch: 422 | Loss: 0.29423192143440247 | Val. Loss: 0.36121007800102234\n",
            "Epoch: 423 | Loss: 0.3656521141529083 | Val. Loss: 0.36119288206100464\n",
            "Epoch: 424 | Loss: 0.20333890616893768 | Val. Loss: 0.36107468605041504\n",
            "Epoch: 425 | Loss: 0.20780928432941437 | Val. Loss: 0.36109253764152527\n",
            "Epoch: 426 | Loss: 0.42911648750305176 | Val. Loss: 0.36108776926994324\n",
            "Epoch: 427 | Loss: 0.4026501178741455 | Val. Loss: 0.36116814613342285\n",
            "Epoch: 428 | Loss: 0.3571344017982483 | Val. Loss: 0.3610696792602539\n",
            "Epoch: 429 | Loss: 0.30478155612945557 | Val. Loss: 0.36118483543395996\n",
            "Epoch: 430 | Loss: 0.228823721408844 | Val. Loss: 0.36120110750198364\n",
            "Epoch: 431 | Loss: 0.36702054738998413 | Val. Loss: 0.3610597550868988\n",
            "Epoch: 432 | Loss: 0.2892850339412689 | Val. Loss: 0.36107081174850464\n",
            "Epoch: 433 | Loss: 0.3979475200176239 | Val. Loss: 0.36108916997909546\n",
            "Epoch: 434 | Loss: 0.26315152645111084 | Val. Loss: 0.3610016703605652\n",
            "Epoch: 435 | Loss: 0.36056646704673767 | Val. Loss: 0.3609668016433716\n",
            "Epoch: 436 | Loss: 0.23809857666492462 | Val. Loss: 0.36097031831741333\n",
            "Epoch: 437 | Loss: 0.3753069043159485 | Val. Loss: 0.36117324233055115\n",
            "Epoch: 438 | Loss: 0.25899070501327515 | Val. Loss: 0.3611043095588684\n",
            "Epoch: 439 | Loss: 0.23910731077194214 | Val. Loss: 0.36119863390922546\n",
            "Epoch: 440 | Loss: 0.2913137376308441 | Val. Loss: 0.36107751727104187\n",
            "Epoch: 441 | Loss: 0.25000086426734924 | Val. Loss: 0.3608388602733612\n",
            "Epoch: 442 | Loss: 0.4051774740219116 | Val. Loss: 0.360994428396225\n",
            "Epoch: 443 | Loss: 0.24243955314159393 | Val. Loss: 0.361101359128952\n",
            "Epoch: 444 | Loss: 0.38987693190574646 | Val. Loss: 0.3610805869102478\n",
            "Epoch: 445 | Loss: 0.25093361735343933 | Val. Loss: 0.3609599471092224\n",
            "Epoch: 446 | Loss: 0.43407759070396423 | Val. Loss: 0.36107248067855835\n",
            "Epoch: 447 | Loss: 0.35795143246650696 | Val. Loss: 0.3610997796058655\n",
            "Epoch: 448 | Loss: 0.3638261556625366 | Val. Loss: 0.3610121011734009\n",
            "Epoch: 449 | Loss: 0.3498416244983673 | Val. Loss: 0.36094990372657776\n",
            "Epoch: 450 | Loss: 0.2145444005727768 | Val. Loss: 0.36098477244377136\n",
            "Epoch: 451 | Loss: 0.39153167605400085 | Val. Loss: 0.3610633909702301\n",
            "Epoch: 452 | Loss: 0.3397708535194397 | Val. Loss: 0.360962450504303\n",
            "Epoch: 453 | Loss: 0.30950072407722473 | Val. Loss: 0.3608110249042511\n",
            "Epoch: 454 | Loss: 0.3314402401447296 | Val. Loss: 0.36085301637649536\n",
            "Epoch: 455 | Loss: 0.32717061042785645 | Val. Loss: 0.3610280454158783\n",
            "Epoch: 456 | Loss: 0.34011712670326233 | Val. Loss: 0.3609352707862854\n",
            "Epoch: 457 | Loss: 0.4553750455379486 | Val. Loss: 0.3610786199569702\n",
            "Epoch: 458 | Loss: 0.4446793794631958 | Val. Loss: 0.36140352487564087\n",
            "Epoch: 459 | Loss: 0.20099236071109772 | Val. Loss: 0.360861212015152\n",
            "Epoch: 460 | Loss: 0.31101399660110474 | Val. Loss: 0.36095887422561646\n",
            "Epoch: 461 | Loss: 0.30760544538497925 | Val. Loss: 0.361029714345932\n",
            "Epoch: 462 | Loss: 0.5047632455825806 | Val. Loss: 0.36099785566329956\n",
            "Epoch: 463 | Loss: 0.3179680407047272 | Val. Loss: 0.3609127998352051\n",
            "Epoch: 464 | Loss: 0.3369620740413666 | Val. Loss: 0.3607640266418457\n",
            "Epoch: 465 | Loss: 0.25295117497444153 | Val. Loss: 0.36077094078063965\n",
            "Epoch: 466 | Loss: 0.33474263548851013 | Val. Loss: 0.36100515723228455\n",
            "Epoch: 467 | Loss: 0.3396354913711548 | Val. Loss: 0.3608621060848236\n",
            "Epoch: 468 | Loss: 0.17429837584495544 | Val. Loss: 0.36087074875831604\n",
            "Epoch: 469 | Loss: 0.33507105708122253 | Val. Loss: 0.3610107898712158\n",
            "Epoch: 470 | Loss: 0.28398847579956055 | Val. Loss: 0.36090415716171265\n",
            "Epoch: 471 | Loss: 0.33383893966674805 | Val. Loss: 0.36094653606414795\n",
            "Epoch: 472 | Loss: 0.4540477693080902 | Val. Loss: 0.3610595762729645\n",
            "Epoch: 473 | Loss: 0.17914843559265137 | Val. Loss: 0.3608890473842621\n",
            "Epoch: 474 | Loss: 0.27689167857170105 | Val. Loss: 0.36073166131973267\n",
            "Epoch: 475 | Loss: 0.23465250432491302 | Val. Loss: 0.36080485582351685\n",
            "Epoch: 476 | Loss: 0.3677286207675934 | Val. Loss: 0.36095890402793884\n",
            "Epoch: 477 | Loss: 0.4211674630641937 | Val. Loss: 0.36101633310317993\n",
            "Epoch: 478 | Loss: 0.389425665140152 | Val. Loss: 0.36096516251564026\n",
            "Epoch: 479 | Loss: 0.35909777879714966 | Val. Loss: 0.36098289489746094\n",
            "Epoch: 480 | Loss: 0.16123348474502563 | Val. Loss: 0.3606474995613098\n",
            "Epoch: 481 | Loss: 0.3276980519294739 | Val. Loss: 0.3607819676399231\n",
            "Epoch: 482 | Loss: 0.2177380472421646 | Val. Loss: 0.36075055599212646\n",
            "Epoch: 483 | Loss: 0.24207603931427002 | Val. Loss: 0.36078596115112305\n",
            "Epoch: 484 | Loss: 0.2950400710105896 | Val. Loss: 0.3608075678348541\n",
            "Epoch: 485 | Loss: 0.20678992569446564 | Val. Loss: 0.36082378029823303\n",
            "Epoch: 486 | Loss: 0.37780383229255676 | Val. Loss: 0.36078357696533203\n",
            "Epoch: 487 | Loss: 0.34127193689346313 | Val. Loss: 0.36083075404167175\n",
            "Epoch: 488 | Loss: 0.31248119473457336 | Val. Loss: 0.3608126938343048\n",
            "Epoch: 489 | Loss: 0.28903645277023315 | Val. Loss: 0.3606843948364258\n",
            "Epoch: 490 | Loss: 0.3433276414871216 | Val. Loss: 0.36085012555122375\n",
            "Epoch: 491 | Loss: 0.2368135005235672 | Val. Loss: 0.3607671856880188\n",
            "Epoch: 492 | Loss: 0.24318984150886536 | Val. Loss: 0.3609594702720642\n",
            "Epoch: 493 | Loss: 0.3007051944732666 | Val. Loss: 0.36075690388679504\n",
            "Epoch: 494 | Loss: 0.31927090883255005 | Val. Loss: 0.3609233796596527\n",
            "Epoch: 495 | Loss: 0.28715765476226807 | Val. Loss: 0.36069023609161377\n",
            "Epoch: 496 | Loss: 0.2994912564754486 | Val. Loss: 0.36073654890060425\n",
            "Epoch: 497 | Loss: 0.2675478160381317 | Val. Loss: 0.36063432693481445\n",
            "Epoch: 498 | Loss: 0.2711872160434723 | Val. Loss: 0.3605455756187439\n",
            "Epoch: 499 | Loss: 0.39227235317230225 | Val. Loss: 0.3607445955276489\n",
            "Epoch: 500 | Loss: 0.42732447385787964 | Val. Loss: 0.3608117699623108\n",
            "Epoch: 501 | Loss: 0.47268956899642944 | Val. Loss: 0.3607224225997925\n",
            "Epoch: 502 | Loss: 0.32158514857292175 | Val. Loss: 0.36063456535339355\n",
            "Epoch: 503 | Loss: 0.25720882415771484 | Val. Loss: 0.36065730452537537\n",
            "Epoch: 504 | Loss: 0.2296791672706604 | Val. Loss: 0.3606959879398346\n",
            "Epoch: 505 | Loss: 0.33423343300819397 | Val. Loss: 0.36069148778915405\n",
            "Epoch: 506 | Loss: 0.25732317566871643 | Val. Loss: 0.36079925298690796\n",
            "Epoch: 507 | Loss: 0.2644319236278534 | Val. Loss: 0.3607117235660553\n",
            "Epoch: 508 | Loss: 0.25715428590774536 | Val. Loss: 0.3604678809642792\n",
            "Epoch: 509 | Loss: 0.34692996740341187 | Val. Loss: 0.36085671186447144\n",
            "Epoch: 510 | Loss: 0.17756451666355133 | Val. Loss: 0.3607761263847351\n",
            "Epoch: 511 | Loss: 0.21425440907478333 | Val. Loss: 0.36053335666656494\n",
            "Epoch: 512 | Loss: 0.3361535370349884 | Val. Loss: 0.36068832874298096\n",
            "Epoch: 513 | Loss: 0.24374523758888245 | Val. Loss: 0.36072832345962524\n",
            "Epoch: 514 | Loss: 0.24208779633045197 | Val. Loss: 0.36068931221961975\n",
            "Epoch: 515 | Loss: 0.24346116185188293 | Val. Loss: 0.3605290949344635\n",
            "Epoch: 516 | Loss: 0.3090537488460541 | Val. Loss: 0.3606642484664917\n",
            "Epoch: 517 | Loss: 0.27561357617378235 | Val. Loss: 0.3604969382286072\n",
            "Epoch: 518 | Loss: 0.31718307733535767 | Val. Loss: 0.3606155812740326\n",
            "Epoch: 519 | Loss: 0.42417123913764954 | Val. Loss: 0.360624223947525\n",
            "Epoch: 520 | Loss: 0.2820059359073639 | Val. Loss: 0.3607190251350403\n",
            "Epoch: 521 | Loss: 0.29378843307495117 | Val. Loss: 0.3606083393096924\n",
            "Epoch: 522 | Loss: 0.2887866795063019 | Val. Loss: 0.3606685400009155\n",
            "Epoch: 523 | Loss: 0.20149677991867065 | Val. Loss: 0.3606563210487366\n",
            "Epoch: 524 | Loss: 0.38994789123535156 | Val. Loss: 0.3606652617454529\n",
            "Epoch: 525 | Loss: 0.3907017409801483 | Val. Loss: 0.3605153262615204\n",
            "Epoch: 526 | Loss: 0.27526259422302246 | Val. Loss: 0.3604477047920227\n",
            "Epoch: 527 | Loss: 0.3088186979293823 | Val. Loss: 0.36055561900138855\n",
            "Epoch: 528 | Loss: 0.2936561107635498 | Val. Loss: 0.36055412888526917\n",
            "Epoch: 529 | Loss: 0.22513334453105927 | Val. Loss: 0.36055299639701843\n",
            "Epoch: 530 | Loss: 0.2975367307662964 | Val. Loss: 0.36056676506996155\n",
            "Epoch: 531 | Loss: 0.32493892312049866 | Val. Loss: 0.36047905683517456\n",
            "Epoch: 532 | Loss: 0.2944219410419464 | Val. Loss: 0.3605503439903259\n",
            "Epoch: 533 | Loss: 0.44389963150024414 | Val. Loss: 0.3604976534843445\n",
            "Epoch: 534 | Loss: 0.31073275208473206 | Val. Loss: 0.360504150390625\n",
            "Epoch: 535 | Loss: 0.3366211950778961 | Val. Loss: 0.3605322241783142\n",
            "Epoch: 536 | Loss: 0.26115888357162476 | Val. Loss: 0.360407292842865\n",
            "Epoch: 537 | Loss: 0.4702998399734497 | Val. Loss: 0.3605632185935974\n",
            "Epoch: 538 | Loss: 0.2529006004333496 | Val. Loss: 0.3607492446899414\n",
            "Epoch: 539 | Loss: 0.24017514288425446 | Val. Loss: 0.3605387806892395\n",
            "Epoch: 540 | Loss: 0.4023408889770508 | Val. Loss: 0.360420286655426\n",
            "Epoch: 541 | Loss: 0.42010805010795593 | Val. Loss: 0.36048880219459534\n",
            "Epoch: 542 | Loss: 0.4229895770549774 | Val. Loss: 0.36071673035621643\n",
            "Epoch: 543 | Loss: 0.3799026608467102 | Val. Loss: 0.3607400059700012\n",
            "Epoch: 544 | Loss: 0.3084332346916199 | Val. Loss: 0.36037856340408325\n",
            "Epoch: 545 | Loss: 0.33015212416648865 | Val. Loss: 0.36034291982650757\n",
            "Epoch: 546 | Loss: 0.1710607260465622 | Val. Loss: 0.36054614186286926\n",
            "Epoch: 547 | Loss: 0.3571453094482422 | Val. Loss: 0.3605971932411194\n",
            "Epoch: 548 | Loss: 0.32532230019569397 | Val. Loss: 0.3603664040565491\n",
            "Epoch: 549 | Loss: 0.2844879925251007 | Val. Loss: 0.3603840470314026\n",
            "Epoch: 550 | Loss: 0.243910551071167 | Val. Loss: 0.36042481660842896\n",
            "Epoch: 551 | Loss: 0.308672696352005 | Val. Loss: 0.36034339666366577\n",
            "Epoch: 552 | Loss: 0.3678553104400635 | Val. Loss: 0.3602917492389679\n",
            "Epoch: 553 | Loss: 0.27532893419265747 | Val. Loss: 0.3603997230529785\n",
            "Epoch: 554 | Loss: 0.30214783549308777 | Val. Loss: 0.36023956537246704\n",
            "Epoch: 555 | Loss: 0.36177295446395874 | Val. Loss: 0.3604549169540405\n",
            "Epoch: 556 | Loss: 0.34206774830818176 | Val. Loss: 0.36023345589637756\n",
            "Epoch: 557 | Loss: 0.2043990045785904 | Val. Loss: 0.3603708744049072\n",
            "Epoch: 558 | Loss: 0.30382320284843445 | Val. Loss: 0.36029356718063354\n",
            "Epoch: 559 | Loss: 0.24711893498897552 | Val. Loss: 0.3603781461715698\n",
            "Epoch: 560 | Loss: 0.373004287481308 | Val. Loss: 0.3602644205093384\n",
            "Epoch: 561 | Loss: 0.3083762526512146 | Val. Loss: 0.36032944917678833\n",
            "Epoch: 562 | Loss: 0.3121475875377655 | Val. Loss: 0.36029234528541565\n",
            "Epoch: 563 | Loss: 0.40717288851737976 | Val. Loss: 0.36058059334754944\n",
            "Epoch: 564 | Loss: 0.41581085324287415 | Val. Loss: 0.36028987169265747\n",
            "Epoch: 565 | Loss: 0.2936331331729889 | Val. Loss: 0.36037227511405945\n",
            "Epoch: 566 | Loss: 0.2498069405555725 | Val. Loss: 0.36051133275032043\n",
            "Epoch: 567 | Loss: 0.242434561252594 | Val. Loss: 0.36042144894599915\n",
            "Epoch: 568 | Loss: 0.29729440808296204 | Val. Loss: 0.36039116978645325\n",
            "Epoch: 569 | Loss: 0.36495524644851685 | Val. Loss: 0.3602793216705322\n",
            "Epoch: 570 | Loss: 0.38125988841056824 | Val. Loss: 0.360223650932312\n",
            "Epoch: 571 | Loss: 0.2623260021209717 | Val. Loss: 0.3603288531303406\n",
            "Epoch: 572 | Loss: 0.3758041560649872 | Val. Loss: 0.36025139689445496\n",
            "Epoch: 573 | Loss: 0.3342091143131256 | Val. Loss: 0.3601900637149811\n",
            "Epoch: 574 | Loss: 0.325271338224411 | Val. Loss: 0.3601323962211609\n",
            "Epoch: 575 | Loss: 0.34286484122276306 | Val. Loss: 0.3601435422897339\n",
            "Epoch: 576 | Loss: 0.3912647068500519 | Val. Loss: 0.3603460192680359\n",
            "Epoch: 577 | Loss: 0.2295653223991394 | Val. Loss: 0.36029893159866333\n",
            "Epoch: 578 | Loss: 0.389296293258667 | Val. Loss: 0.360320121049881\n",
            "Epoch: 579 | Loss: 0.415696918964386 | Val. Loss: 0.3603614866733551\n",
            "Epoch: 580 | Loss: 0.4393765330314636 | Val. Loss: 0.3602519631385803\n",
            "Epoch: 581 | Loss: 0.2753600776195526 | Val. Loss: 0.3602273762226105\n",
            "Epoch: 582 | Loss: 0.271048903465271 | Val. Loss: 0.3602128028869629\n",
            "Epoch: 583 | Loss: 0.3588324785232544 | Val. Loss: 0.36013540625572205\n",
            "Epoch: 584 | Loss: 0.3513588607311249 | Val. Loss: 0.36017459630966187\n",
            "Epoch: 585 | Loss: 0.2677106559276581 | Val. Loss: 0.36012575030326843\n",
            "Epoch: 586 | Loss: 0.3701210916042328 | Val. Loss: 0.36025145649909973\n",
            "Epoch: 587 | Loss: 0.3746732473373413 | Val. Loss: 0.36016690731048584\n",
            "Epoch: 588 | Loss: 0.2581245005130768 | Val. Loss: 0.36031022667884827\n",
            "Epoch: 589 | Loss: 0.3387667238712311 | Val. Loss: 0.36029452085494995\n",
            "Epoch: 590 | Loss: 0.3549301028251648 | Val. Loss: 0.36010000109672546\n",
            "Epoch: 591 | Loss: 0.4587549865245819 | Val. Loss: 0.36007553339004517\n",
            "Epoch: 592 | Loss: 0.2598452568054199 | Val. Loss: 0.36006537079811096\n",
            "Epoch: 593 | Loss: 0.265342116355896 | Val. Loss: 0.3602789044380188\n",
            "Epoch: 594 | Loss: 0.34914785623550415 | Val. Loss: 0.36004742980003357\n",
            "Epoch: 595 | Loss: 0.36891913414001465 | Val. Loss: 0.3599812090396881\n",
            "Epoch: 596 | Loss: 0.3722389340400696 | Val. Loss: 0.3600795269012451\n",
            "Epoch: 597 | Loss: 0.279548317193985 | Val. Loss: 0.35998114943504333\n",
            "Epoch: 598 | Loss: 0.36955246329307556 | Val. Loss: 0.35989993810653687\n",
            "Epoch: 599 | Loss: 0.3010675311088562 | Val. Loss: 0.3598967492580414\n",
            "Epoch: 600 | Loss: 0.3908993899822235 | Val. Loss: 0.35994210839271545\n",
            "Epoch: 601 | Loss: 0.2742815315723419 | Val. Loss: 0.35994112491607666\n",
            "Epoch: 602 | Loss: 0.39901185035705566 | Val. Loss: 0.36000943183898926\n",
            "Epoch: 603 | Loss: 0.33080512285232544 | Val. Loss: 0.36020269989967346\n",
            "Epoch: 604 | Loss: 0.28514334559440613 | Val. Loss: 0.3600866496562958\n",
            "Epoch: 605 | Loss: 0.26863667368888855 | Val. Loss: 0.36003491282463074\n",
            "Epoch: 606 | Loss: 0.2974640130996704 | Val. Loss: 0.3601738512516022\n",
            "Epoch: 607 | Loss: 0.3030128479003906 | Val. Loss: 0.3599366843700409\n",
            "Epoch: 608 | Loss: 0.2725047469139099 | Val. Loss: 0.3599333167076111\n",
            "Epoch: 609 | Loss: 0.18258920311927795 | Val. Loss: 0.3599843680858612\n",
            "Epoch: 610 | Loss: 0.22408241033554077 | Val. Loss: 0.359893798828125\n",
            "Epoch: 611 | Loss: 0.29349589347839355 | Val. Loss: 0.36017534136772156\n",
            "Epoch: 612 | Loss: 0.35248422622680664 | Val. Loss: 0.35985255241394043\n",
            "Epoch: 613 | Loss: 0.3558632731437683 | Val. Loss: 0.3599063754081726\n",
            "Epoch: 614 | Loss: 0.27122530341148376 | Val. Loss: 0.35996493697166443\n",
            "Epoch: 615 | Loss: 0.26480865478515625 | Val. Loss: 0.3599535822868347\n",
            "Epoch: 616 | Loss: 0.3078407347202301 | Val. Loss: 0.35995155572891235\n",
            "Epoch: 617 | Loss: 0.25156670808792114 | Val. Loss: 0.3597882390022278\n",
            "Epoch: 618 | Loss: 0.2528548240661621 | Val. Loss: 0.35968413949012756\n",
            "Epoch: 619 | Loss: 0.3365050256252289 | Val. Loss: 0.35969680547714233\n",
            "Epoch: 620 | Loss: 0.2953738272190094 | Val. Loss: 0.35975372791290283\n",
            "Epoch: 621 | Loss: 0.25612449645996094 | Val. Loss: 0.35985487699508667\n",
            "Epoch: 622 | Loss: 0.3088018298149109 | Val. Loss: 0.3598550260066986\n",
            "Epoch: 623 | Loss: 0.262015700340271 | Val. Loss: 0.3595324158668518\n",
            "Epoch: 624 | Loss: 0.3368372619152069 | Val. Loss: 0.35963791608810425\n",
            "Epoch: 625 | Loss: 0.31499484181404114 | Val. Loss: 0.3597411513328552\n",
            "Epoch: 626 | Loss: 0.2489250749349594 | Val. Loss: 0.3597603738307953\n",
            "Epoch: 627 | Loss: 0.2664976716041565 | Val. Loss: 0.3598068356513977\n",
            "Epoch: 628 | Loss: 0.27603834867477417 | Val. Loss: 0.359671950340271\n",
            "Epoch: 629 | Loss: 0.3421552777290344 | Val. Loss: 0.3597258925437927\n",
            "Epoch: 630 | Loss: 0.32018646597862244 | Val. Loss: 0.3595729172229767\n",
            "Epoch: 631 | Loss: 0.31125345826148987 | Val. Loss: 0.35960835218429565\n",
            "Epoch: 632 | Loss: 0.3439342975616455 | Val. Loss: 0.3595832288265228\n",
            "Epoch: 633 | Loss: 0.3895319700241089 | Val. Loss: 0.35972732305526733\n",
            "Epoch: 634 | Loss: 0.24893337488174438 | Val. Loss: 0.35944968461990356\n",
            "Epoch: 635 | Loss: 0.2774743139743805 | Val. Loss: 0.35941052436828613\n",
            "Epoch: 636 | Loss: 0.5424689054489136 | Val. Loss: 0.3596334159374237\n",
            "Epoch: 637 | Loss: 0.42353907227516174 | Val. Loss: 0.3595573306083679\n",
            "Epoch: 638 | Loss: 0.1795051246881485 | Val. Loss: 0.35950082540512085\n",
            "Epoch: 639 | Loss: 0.3792003393173218 | Val. Loss: 0.3594341576099396\n",
            "Epoch: 640 | Loss: 0.29441145062446594 | Val. Loss: 0.3594784438610077\n",
            "Epoch: 641 | Loss: 0.3371579647064209 | Val. Loss: 0.35935813188552856\n",
            "Epoch: 642 | Loss: 0.24184417724609375 | Val. Loss: 0.3594563603401184\n",
            "Epoch: 643 | Loss: 0.27131718397140503 | Val. Loss: 0.3594626784324646\n",
            "Epoch: 644 | Loss: 0.22086592018604279 | Val. Loss: 0.35938119888305664\n",
            "Epoch: 645 | Loss: 0.31634801626205444 | Val. Loss: 0.3594440221786499\n",
            "Epoch: 646 | Loss: 0.2964950501918793 | Val. Loss: 0.3591933250427246\n",
            "Epoch: 647 | Loss: 0.3342447280883789 | Val. Loss: 0.3592399060726166\n",
            "Epoch: 648 | Loss: 0.40773600339889526 | Val. Loss: 0.35930129885673523\n",
            "Epoch: 649 | Loss: 0.30385875701904297 | Val. Loss: 0.35927966237068176\n",
            "Epoch: 650 | Loss: 0.31045976281166077 | Val. Loss: 0.35917776823043823\n",
            "Epoch: 651 | Loss: 0.2247614860534668 | Val. Loss: 0.3592716157436371\n",
            "Epoch: 652 | Loss: 0.36896443367004395 | Val. Loss: 0.35938137769699097\n",
            "Epoch: 653 | Loss: 0.25476884841918945 | Val. Loss: 0.35935744643211365\n",
            "Epoch: 654 | Loss: 0.2815236449241638 | Val. Loss: 0.35913655161857605\n",
            "Epoch: 655 | Loss: 0.26541867852211 | Val. Loss: 0.3591316342353821\n",
            "Epoch: 656 | Loss: 0.18528269231319427 | Val. Loss: 0.35927289724349976\n",
            "Epoch: 657 | Loss: 0.2638237178325653 | Val. Loss: 0.3592739701271057\n",
            "Epoch: 658 | Loss: 0.31477752327919006 | Val. Loss: 0.359073668718338\n",
            "Epoch: 659 | Loss: 0.3931882083415985 | Val. Loss: 0.3591799736022949\n",
            "Epoch: 660 | Loss: 0.13769112527370453 | Val. Loss: 0.3592143654823303\n",
            "Epoch: 661 | Loss: 0.25315824151039124 | Val. Loss: 0.3589915335178375\n",
            "Epoch: 662 | Loss: 0.3419424295425415 | Val. Loss: 0.35930106043815613\n",
            "Epoch: 663 | Loss: 0.3217298090457916 | Val. Loss: 0.3591846823692322\n",
            "Epoch: 664 | Loss: 0.3559044897556305 | Val. Loss: 0.35892969369888306\n",
            "Epoch: 665 | Loss: 0.37195852398872375 | Val. Loss: 0.3589276969432831\n",
            "Epoch: 666 | Loss: 0.35303688049316406 | Val. Loss: 0.35900282859802246\n",
            "Epoch: 667 | Loss: 0.3115413188934326 | Val. Loss: 0.35903283953666687\n",
            "Epoch: 668 | Loss: 0.2192942202091217 | Val. Loss: 0.35893887281417847\n",
            "Epoch: 669 | Loss: 0.17338217794895172 | Val. Loss: 0.358936071395874\n",
            "Epoch: 670 | Loss: 0.26467663049697876 | Val. Loss: 0.35908955335617065\n",
            "Epoch: 671 | Loss: 0.19334672391414642 | Val. Loss: 0.359052449464798\n",
            "Epoch: 672 | Loss: 0.43451839685440063 | Val. Loss: 0.35933464765548706\n",
            "Epoch: 673 | Loss: 0.24345940351486206 | Val. Loss: 0.3590037226676941\n",
            "Epoch: 674 | Loss: 0.3402993381023407 | Val. Loss: 0.3590952754020691\n",
            "Epoch: 675 | Loss: 0.3673958480358124 | Val. Loss: 0.35907450318336487\n",
            "Epoch: 676 | Loss: 0.1841243952512741 | Val. Loss: 0.3589684069156647\n",
            "Epoch: 677 | Loss: 0.27117979526519775 | Val. Loss: 0.35891789197921753\n",
            "Epoch: 678 | Loss: 0.24829518795013428 | Val. Loss: 0.3587859272956848\n",
            "Epoch: 679 | Loss: 0.27128657698631287 | Val. Loss: 0.35878652334213257\n",
            "Epoch: 680 | Loss: 0.29260706901550293 | Val. Loss: 0.35879191756248474\n",
            "Epoch: 681 | Loss: 0.2886221408843994 | Val. Loss: 0.3588954508304596\n",
            "Epoch: 682 | Loss: 0.24337029457092285 | Val. Loss: 0.35887327790260315\n",
            "Epoch: 683 | Loss: 0.30914565920829773 | Val. Loss: 0.35901403427124023\n",
            "Epoch: 684 | Loss: 0.32899418473243713 | Val. Loss: 0.3588450253009796\n",
            "Epoch: 685 | Loss: 0.30756646394729614 | Val. Loss: 0.35885363817214966\n",
            "Epoch: 686 | Loss: 0.4387661814689636 | Val. Loss: 0.3589151203632355\n",
            "Epoch: 687 | Loss: 0.24290278553962708 | Val. Loss: 0.3587595820426941\n",
            "Epoch: 688 | Loss: 0.22689080238342285 | Val. Loss: 0.358825147151947\n",
            "Epoch: 689 | Loss: 0.2546428442001343 | Val. Loss: 0.3588031530380249\n",
            "Epoch: 690 | Loss: 0.4220605194568634 | Val. Loss: 0.358829140663147\n",
            "Epoch: 691 | Loss: 0.3432389199733734 | Val. Loss: 0.35877493023872375\n",
            "Epoch: 692 | Loss: 0.428802490234375 | Val. Loss: 0.35894641280174255\n",
            "Epoch: 693 | Loss: 0.45330101251602173 | Val. Loss: 0.3589884042739868\n",
            "Epoch: 694 | Loss: 0.37582653760910034 | Val. Loss: 0.358962744474411\n",
            "Epoch: 695 | Loss: 0.3308344781398773 | Val. Loss: 0.35856619477272034\n",
            "Epoch: 696 | Loss: 0.31360575556755066 | Val. Loss: 0.35791218280792236\n",
            "Epoch: 697 | Loss: 0.3177630603313446 | Val. Loss: 0.35728004574775696\n",
            "Epoch: 698 | Loss: 0.3169124722480774 | Val. Loss: 0.35690149664878845\n",
            "Epoch: 699 | Loss: 0.34300005435943604 | Val. Loss: 0.35668227076530457\n",
            "Epoch: 700 | Loss: 0.31999802589416504 | Val. Loss: 0.35679319500923157\n",
            "Epoch: 701 | Loss: 0.2960183322429657 | Val. Loss: 0.35664182901382446\n",
            "Epoch: 702 | Loss: 0.24255090951919556 | Val. Loss: 0.35666126012802124\n",
            "Epoch: 703 | Loss: 0.3494134247303009 | Val. Loss: 0.3564251959323883\n",
            "Epoch: 704 | Loss: 0.2998262941837311 | Val. Loss: 0.35612964630126953\n",
            "Epoch: 705 | Loss: 0.39626628160476685 | Val. Loss: 0.3561117947101593\n",
            "Epoch: 706 | Loss: 0.3961098790168762 | Val. Loss: 0.35625725984573364\n",
            "Epoch: 707 | Loss: 0.3633723258972168 | Val. Loss: 0.3559947907924652\n",
            "Epoch: 708 | Loss: 0.2849799394607544 | Val. Loss: 0.35605698823928833\n",
            "Epoch: 709 | Loss: 0.4053541421890259 | Val. Loss: 0.3558659851551056\n",
            "Epoch: 710 | Loss: 0.2844684422016144 | Val. Loss: 0.355816513299942\n",
            "Epoch: 711 | Loss: 0.2954285442829132 | Val. Loss: 0.35574331879615784\n",
            "Epoch: 712 | Loss: 0.2966732680797577 | Val. Loss: 0.355630099773407\n",
            "Epoch: 713 | Loss: 0.36118051409721375 | Val. Loss: 0.3557114005088806\n",
            "Epoch: 714 | Loss: 0.19538748264312744 | Val. Loss: 0.35549116134643555\n",
            "Epoch: 715 | Loss: 0.18531115353107452 | Val. Loss: 0.35556018352508545\n",
            "Epoch: 716 | Loss: 0.2625998854637146 | Val. Loss: 0.3554385304450989\n",
            "Epoch: 717 | Loss: 0.2453175187110901 | Val. Loss: 0.3556785583496094\n",
            "Epoch: 718 | Loss: 0.25243353843688965 | Val. Loss: 0.35554587841033936\n",
            "Epoch: 719 | Loss: 0.18778465688228607 | Val. Loss: 0.35559114813804626\n",
            "Epoch: 720 | Loss: 0.3411371111869812 | Val. Loss: 0.3554198145866394\n",
            "Epoch: 721 | Loss: 0.4035981595516205 | Val. Loss: 0.35545971989631653\n",
            "Epoch: 722 | Loss: 0.23436900973320007 | Val. Loss: 0.35526785254478455\n",
            "Epoch: 723 | Loss: 0.41254425048828125 | Val. Loss: 0.35535430908203125\n",
            "Epoch: 724 | Loss: 0.2704930007457733 | Val. Loss: 0.35529476404190063\n",
            "Epoch: 725 | Loss: 0.34092044830322266 | Val. Loss: 0.3550906777381897\n",
            "Epoch: 726 | Loss: 0.3700229823589325 | Val. Loss: 0.35508301854133606\n",
            "Epoch: 727 | Loss: 0.2760321795940399 | Val. Loss: 0.35498762130737305\n",
            "Epoch: 728 | Loss: 0.3518650531768799 | Val. Loss: 0.3554527759552002\n",
            "Epoch: 729 | Loss: 0.29539138078689575 | Val. Loss: 0.35492709279060364\n",
            "Epoch: 730 | Loss: 0.378667950630188 | Val. Loss: 0.35484644770622253\n",
            "Epoch: 731 | Loss: 0.2997065782546997 | Val. Loss: 0.35480573773384094\n",
            "Epoch: 732 | Loss: 0.35689499974250793 | Val. Loss: 0.35478901863098145\n",
            "Epoch: 733 | Loss: 0.31816431879997253 | Val. Loss: 0.3548588752746582\n",
            "Epoch: 734 | Loss: 0.3848791718482971 | Val. Loss: 0.3547285199165344\n",
            "Epoch: 735 | Loss: 0.2523038983345032 | Val. Loss: 0.35490116477012634\n",
            "Epoch: 736 | Loss: 0.24800173938274384 | Val. Loss: 0.35493576526641846\n",
            "Epoch: 737 | Loss: 0.19821856915950775 | Val. Loss: 0.3546698987483978\n",
            "Epoch: 738 | Loss: 0.24629859626293182 | Val. Loss: 0.3544841706752777\n",
            "Epoch: 739 | Loss: 0.2966598868370056 | Val. Loss: 0.3547786772251129\n",
            "Epoch: 740 | Loss: 0.2702424228191376 | Val. Loss: 0.35454580187797546\n",
            "Epoch: 741 | Loss: 0.29805558919906616 | Val. Loss: 0.35449090600013733\n",
            "Epoch: 742 | Loss: 0.19331642985343933 | Val. Loss: 0.3542421758174896\n",
            "Epoch: 743 | Loss: 0.28511133790016174 | Val. Loss: 0.3546017110347748\n",
            "Epoch: 744 | Loss: 0.3327069580554962 | Val. Loss: 0.3543199598789215\n",
            "Epoch: 745 | Loss: 0.33761805295944214 | Val. Loss: 0.35405540466308594\n",
            "Epoch: 746 | Loss: 0.31240108609199524 | Val. Loss: 0.35410669445991516\n",
            "Epoch: 747 | Loss: 0.2142757922410965 | Val. Loss: 0.35408300161361694\n",
            "Epoch: 748 | Loss: 0.36667677760124207 | Val. Loss: 0.3541340231895447\n",
            "Epoch: 749 | Loss: 0.3519105911254883 | Val. Loss: 0.3540026545524597\n",
            "Epoch: 750 | Loss: 0.3487960398197174 | Val. Loss: 0.35400134325027466\n",
            "Epoch: 751 | Loss: 0.33791640400886536 | Val. Loss: 0.35414278507232666\n",
            "Epoch: 752 | Loss: 0.37669625878334045 | Val. Loss: 0.3542173206806183\n",
            "Epoch: 753 | Loss: 0.3825221657752991 | Val. Loss: 0.35417014360427856\n",
            "Epoch: 754 | Loss: 0.22871477901935577 | Val. Loss: 0.35422173142433167\n",
            "Epoch: 755 | Loss: 0.3551015853881836 | Val. Loss: 0.3542577922344208\n",
            "Epoch: 756 | Loss: 0.24888955056667328 | Val. Loss: 0.3542250692844391\n",
            "Epoch: 757 | Loss: 0.29846876859664917 | Val. Loss: 0.35400059819221497\n",
            "Epoch: 758 | Loss: 0.26241597533226013 | Val. Loss: 0.3542376160621643\n",
            "Epoch: 759 | Loss: 0.5388289093971252 | Val. Loss: 0.35417822003364563\n",
            "Epoch: 760 | Loss: 0.24792230129241943 | Val. Loss: 0.3539785146713257\n",
            "Epoch: 761 | Loss: 0.369857519865036 | Val. Loss: 0.35387855768203735\n",
            "Epoch: 762 | Loss: 0.21582360565662384 | Val. Loss: 0.35377630591392517\n",
            "Epoch: 763 | Loss: 0.3482198417186737 | Val. Loss: 0.3540760278701782\n",
            "Epoch: 764 | Loss: 0.48733770847320557 | Val. Loss: 0.3541085422039032\n",
            "Epoch: 765 | Loss: 0.22014418244361877 | Val. Loss: 0.35406002402305603\n",
            "Epoch: 766 | Loss: 0.22376610338687897 | Val. Loss: 0.3538866341114044\n",
            "Epoch: 767 | Loss: 0.2893669903278351 | Val. Loss: 0.35374245047569275\n",
            "Epoch: 768 | Loss: 0.2522343099117279 | Val. Loss: 0.3536679446697235\n",
            "Epoch: 769 | Loss: 0.32390856742858887 | Val. Loss: 0.35387060046195984\n",
            "Epoch: 770 | Loss: 0.33416128158569336 | Val. Loss: 0.35372212529182434\n",
            "Epoch: 771 | Loss: 0.38287562131881714 | Val. Loss: 0.3537615239620209\n",
            "Epoch: 772 | Loss: 0.25561580061912537 | Val. Loss: 0.3535524606704712\n",
            "Epoch: 773 | Loss: 0.29374855756759644 | Val. Loss: 0.3535308241844177\n",
            "Epoch: 774 | Loss: 0.31433817744255066 | Val. Loss: 0.3534012734889984\n",
            "Epoch: 775 | Loss: 0.2468137890100479 | Val. Loss: 0.353720098733902\n",
            "Epoch: 776 | Loss: 0.2865842580795288 | Val. Loss: 0.35355323553085327\n",
            "Epoch: 777 | Loss: 0.3321036100387573 | Val. Loss: 0.35331326723098755\n",
            "Epoch: 778 | Loss: 0.219447523355484 | Val. Loss: 0.3533938527107239\n",
            "Epoch: 779 | Loss: 0.2711103558540344 | Val. Loss: 0.353355348110199\n",
            "Epoch: 780 | Loss: 0.3336277902126312 | Val. Loss: 0.3533194661140442\n",
            "Epoch: 781 | Loss: 0.2499956339597702 | Val. Loss: 0.3534165322780609\n",
            "Epoch: 782 | Loss: 0.34287309646606445 | Val. Loss: 0.3534262776374817\n",
            "Epoch: 783 | Loss: 0.2754634618759155 | Val. Loss: 0.3535710275173187\n",
            "Epoch: 784 | Loss: 0.3510677218437195 | Val. Loss: 0.35349535942077637\n",
            "Epoch: 785 | Loss: 0.35712042450904846 | Val. Loss: 0.3534250557422638\n",
            "Epoch: 786 | Loss: 0.3329528868198395 | Val. Loss: 0.35315343737602234\n",
            "Epoch: 787 | Loss: 0.3493560254573822 | Val. Loss: 0.3533703684806824\n",
            "Epoch: 788 | Loss: 0.22374364733695984 | Val. Loss: 0.3533172607421875\n",
            "Epoch: 789 | Loss: 0.3653310239315033 | Val. Loss: 0.3529438078403473\n",
            "Epoch: 790 | Loss: 0.3010123372077942 | Val. Loss: 0.35318443179130554\n",
            "Epoch: 791 | Loss: 0.3895578980445862 | Val. Loss: 0.35320642590522766\n",
            "Epoch: 792 | Loss: 0.22125066816806793 | Val. Loss: 0.353060245513916\n",
            "Epoch: 793 | Loss: 0.32270845770835876 | Val. Loss: 0.35303178429603577\n",
            "Epoch: 794 | Loss: 0.1803787350654602 | Val. Loss: 0.35288435220718384\n",
            "Epoch: 795 | Loss: 0.4066614508628845 | Val. Loss: 0.3529720902442932\n",
            "Epoch: 796 | Loss: 0.39701902866363525 | Val. Loss: 0.35307595133781433\n",
            "Epoch: 797 | Loss: 0.42984917759895325 | Val. Loss: 0.35302844643592834\n",
            "Epoch: 798 | Loss: 0.1854969710111618 | Val. Loss: 0.3529536724090576\n",
            "Epoch: 799 | Loss: 0.3487626314163208 | Val. Loss: 0.35285645723342896\n",
            "Epoch: 800 | Loss: 0.29210737347602844 | Val. Loss: 0.3526797294616699\n",
            "Epoch: 801 | Loss: 0.25720661878585815 | Val. Loss: 0.35276105999946594\n",
            "Epoch: 802 | Loss: 0.4721980392932892 | Val. Loss: 0.3529747724533081\n",
            "Epoch: 803 | Loss: 0.35716378688812256 | Val. Loss: 0.35266438126564026\n",
            "Epoch: 804 | Loss: 0.30274948477745056 | Val. Loss: 0.3524980843067169\n",
            "Epoch: 805 | Loss: 0.3773364722728729 | Val. Loss: 0.35303977131843567\n",
            "Epoch: 806 | Loss: 0.42191290855407715 | Val. Loss: 0.35296303033828735\n",
            "Epoch: 807 | Loss: 0.29115942120552063 | Val. Loss: 0.35269537568092346\n",
            "Epoch: 808 | Loss: 0.34668830037117004 | Val. Loss: 0.35249629616737366\n",
            "Epoch: 809 | Loss: 0.29866793751716614 | Val. Loss: 0.3525920510292053\n",
            "Epoch: 810 | Loss: 0.3064579367637634 | Val. Loss: 0.3522994816303253\n",
            "Epoch: 811 | Loss: 0.248480424284935 | Val. Loss: 0.35246676206588745\n",
            "Epoch: 812 | Loss: 0.29701220989227295 | Val. Loss: 0.3526553213596344\n",
            "Epoch: 813 | Loss: 0.27529677748680115 | Val. Loss: 0.35250595211982727\n",
            "Epoch: 814 | Loss: 0.26895952224731445 | Val. Loss: 0.35240525007247925\n",
            "Epoch: 815 | Loss: 0.232029527425766 | Val. Loss: 0.3524462878704071\n",
            "Epoch: 816 | Loss: 0.24228790402412415 | Val. Loss: 0.35237962007522583\n",
            "Epoch: 817 | Loss: 0.2739975154399872 | Val. Loss: 0.3524399399757385\n",
            "Epoch: 818 | Loss: 0.3477301597595215 | Val. Loss: 0.3522941768169403\n",
            "Epoch: 819 | Loss: 0.28010988235473633 | Val. Loss: 0.35224440693855286\n",
            "Epoch: 820 | Loss: 0.2563045918941498 | Val. Loss: 0.3521076440811157\n",
            "Epoch: 821 | Loss: 0.37611639499664307 | Val. Loss: 0.35191574692726135\n",
            "Epoch: 822 | Loss: 0.3679671287536621 | Val. Loss: 0.3516610264778137\n",
            "Epoch: 823 | Loss: 0.40172553062438965 | Val. Loss: 0.35217100381851196\n",
            "Epoch: 824 | Loss: 0.2684341371059418 | Val. Loss: 0.3523436188697815\n",
            "Epoch: 825 | Loss: 0.2639547288417816 | Val. Loss: 0.35226157307624817\n",
            "Epoch: 826 | Loss: 0.3442869484424591 | Val. Loss: 0.35195392370224\n",
            "Epoch: 827 | Loss: 0.3214527666568756 | Val. Loss: 0.3518091142177582\n",
            "Epoch: 828 | Loss: 0.21248948574066162 | Val. Loss: 0.3516344428062439\n",
            "Epoch: 829 | Loss: 0.4305153489112854 | Val. Loss: 0.35185134410858154\n",
            "Epoch: 830 | Loss: 0.3299155831336975 | Val. Loss: 0.3520006239414215\n",
            "Epoch: 831 | Loss: 0.25945499539375305 | Val. Loss: 0.3518734872341156\n",
            "Epoch: 832 | Loss: 0.2700393795967102 | Val. Loss: 0.3516902029514313\n",
            "Epoch: 833 | Loss: 0.23357555270195007 | Val. Loss: 0.35142943263053894\n",
            "Epoch: 834 | Loss: 0.30786165595054626 | Val. Loss: 0.3514402508735657\n",
            "Epoch: 835 | Loss: 0.3641957640647888 | Val. Loss: 0.3516251742839813\n",
            "Epoch: 836 | Loss: 0.3655874729156494 | Val. Loss: 0.35139206051826477\n",
            "Epoch: 837 | Loss: 0.39364615082740784 | Val. Loss: 0.3516959846019745\n",
            "Epoch: 838 | Loss: 0.21410436928272247 | Val. Loss: 0.35152682662010193\n",
            "Epoch: 839 | Loss: 0.21962270140647888 | Val. Loss: 0.3516639471054077\n",
            "Epoch: 840 | Loss: 0.5038988590240479 | Val. Loss: 0.3515304923057556\n",
            "Epoch: 841 | Loss: 0.3449096083641052 | Val. Loss: 0.3515035808086395\n",
            "Epoch: 842 | Loss: 0.35163143277168274 | Val. Loss: 0.3513020873069763\n",
            "Epoch: 843 | Loss: 0.4902811646461487 | Val. Loss: 0.3513428568840027\n",
            "Epoch: 844 | Loss: 0.289299875497818 | Val. Loss: 0.3512973189353943\n",
            "Epoch: 845 | Loss: 0.34366855025291443 | Val. Loss: 0.351192444562912\n",
            "Epoch: 846 | Loss: 0.4030706584453583 | Val. Loss: 0.3510010838508606\n",
            "Epoch: 847 | Loss: 0.3846341371536255 | Val. Loss: 0.35134798288345337\n",
            "Epoch: 848 | Loss: 0.3032701909542084 | Val. Loss: 0.3511226773262024\n",
            "Epoch: 849 | Loss: 0.2826586663722992 | Val. Loss: 0.35118231177330017\n",
            "Epoch: 850 | Loss: 0.21147538721561432 | Val. Loss: 0.35138049721717834\n",
            "Epoch: 851 | Loss: 0.21859458088874817 | Val. Loss: 0.3508012890815735\n",
            "Epoch: 852 | Loss: 0.3509388566017151 | Val. Loss: 0.3508623242378235\n",
            "Epoch: 853 | Loss: 0.33409345149993896 | Val. Loss: 0.35092058777809143\n",
            "Epoch: 854 | Loss: 0.2740837037563324 | Val. Loss: 0.3510221242904663\n",
            "Epoch: 855 | Loss: 0.29392004013061523 | Val. Loss: 0.35113391280174255\n",
            "Epoch: 856 | Loss: 0.34350940585136414 | Val. Loss: 0.3511287271976471\n",
            "Epoch: 857 | Loss: 0.22495311498641968 | Val. Loss: 0.3510618805885315\n",
            "Epoch: 858 | Loss: 0.27618297934532166 | Val. Loss: 0.3508710563182831\n",
            "Epoch: 859 | Loss: 0.3897362947463989 | Val. Loss: 0.3509036898612976\n",
            "Epoch: 860 | Loss: 0.23259301483631134 | Val. Loss: 0.3510361611843109\n",
            "Epoch: 861 | Loss: 0.3119923770427704 | Val. Loss: 0.35098716616630554\n",
            "Epoch: 862 | Loss: 0.434670627117157 | Val. Loss: 0.3506985604763031\n",
            "Epoch: 863 | Loss: 0.20713146030902863 | Val. Loss: 0.350712388753891\n",
            "Epoch: 864 | Loss: 0.20746131241321564 | Val. Loss: 0.35084518790245056\n",
            "Epoch: 865 | Loss: 0.2761962115764618 | Val. Loss: 0.35093098878860474\n",
            "Epoch: 866 | Loss: 0.2913050949573517 | Val. Loss: 0.3507747948169708\n",
            "Epoch: 867 | Loss: 0.2876276671886444 | Val. Loss: 0.35091444849967957\n",
            "Epoch: 868 | Loss: 0.2631184458732605 | Val. Loss: 0.35073453187942505\n",
            "Epoch: 869 | Loss: 0.27587395906448364 | Val. Loss: 0.3504522442817688\n",
            "Epoch: 870 | Loss: 0.3192967176437378 | Val. Loss: 0.35076600313186646\n",
            "Epoch: 871 | Loss: 0.46411946415901184 | Val. Loss: 0.3504149317741394\n",
            "Epoch: 872 | Loss: 0.2759806513786316 | Val. Loss: 0.3506872057914734\n",
            "Epoch: 873 | Loss: 0.27162864804267883 | Val. Loss: 0.35066765546798706\n",
            "Epoch: 874 | Loss: 0.23482024669647217 | Val. Loss: 0.35052064061164856\n",
            "Epoch: 875 | Loss: 0.40787720680236816 | Val. Loss: 0.3506666123867035\n",
            "Epoch: 876 | Loss: 0.2142038494348526 | Val. Loss: 0.35037022829055786\n",
            "Epoch: 877 | Loss: 0.3807450234889984 | Val. Loss: 0.35049599409103394\n",
            "Epoch: 878 | Loss: 0.32044464349746704 | Val. Loss: 0.3505202829837799\n",
            "Epoch: 879 | Loss: 0.2679685056209564 | Val. Loss: 0.35037755966186523\n",
            "Epoch: 880 | Loss: 0.41724693775177 | Val. Loss: 0.35047003626823425\n",
            "Epoch: 881 | Loss: 0.2982819974422455 | Val. Loss: 0.35062021017074585\n",
            "Epoch: 882 | Loss: 0.3463456630706787 | Val. Loss: 0.3504352867603302\n",
            "Epoch: 883 | Loss: 0.318404883146286 | Val. Loss: 0.3505222201347351\n",
            "Epoch: 884 | Loss: 0.37286806106567383 | Val. Loss: 0.35026854276657104\n",
            "Epoch: 885 | Loss: 0.2990548312664032 | Val. Loss: 0.3504255414009094\n",
            "Epoch: 886 | Loss: 0.3267653286457062 | Val. Loss: 0.3501782715320587\n",
            "Epoch: 887 | Loss: 0.30638593435287476 | Val. Loss: 0.3503607511520386\n",
            "Epoch: 888 | Loss: 0.3569349944591522 | Val. Loss: 0.3501834273338318\n",
            "Epoch: 889 | Loss: 0.2680988311767578 | Val. Loss: 0.3503236174583435\n",
            "Epoch: 890 | Loss: 0.25960445404052734 | Val. Loss: 0.3503841757774353\n",
            "Epoch: 891 | Loss: 0.32775023579597473 | Val. Loss: 0.3501625061035156\n",
            "Epoch: 892 | Loss: 0.4314464032649994 | Val. Loss: 0.3504123091697693\n",
            "Epoch: 893 | Loss: 0.44035160541534424 | Val. Loss: 0.3502576947212219\n",
            "Epoch: 894 | Loss: 0.37609371542930603 | Val. Loss: 0.35026708245277405\n",
            "Epoch: 895 | Loss: 0.3445875644683838 | Val. Loss: 0.35024574398994446\n",
            "Epoch: 896 | Loss: 0.3296542465686798 | Val. Loss: 0.34989139437675476\n",
            "Epoch: 897 | Loss: 0.3450368046760559 | Val. Loss: 0.34976568818092346\n",
            "Epoch: 898 | Loss: 0.2115585058927536 | Val. Loss: 0.3499148488044739\n",
            "Epoch: 899 | Loss: 0.317719429731369 | Val. Loss: 0.3500169813632965\n",
            "Epoch: 900 | Loss: 0.30623355507850647 | Val. Loss: 0.35003188252449036\n",
            "Epoch: 901 | Loss: 0.26612231135368347 | Val. Loss: 0.3500654101371765\n",
            "Epoch: 902 | Loss: 0.2957499027252197 | Val. Loss: 0.3499535918235779\n",
            "Epoch: 903 | Loss: 0.24286900460720062 | Val. Loss: 0.3498505651950836\n",
            "Epoch: 904 | Loss: 0.2686992287635803 | Val. Loss: 0.34978705644607544\n",
            "Epoch: 905 | Loss: 0.3153042495250702 | Val. Loss: 0.34964725375175476\n",
            "Epoch: 906 | Loss: 0.35522085428237915 | Val. Loss: 0.34999534487724304\n",
            "Epoch: 907 | Loss: 0.2901689410209656 | Val. Loss: 0.3499596416950226\n",
            "Epoch: 908 | Loss: 0.37370702624320984 | Val. Loss: 0.35011380910873413\n",
            "Epoch: 909 | Loss: 0.3952731490135193 | Val. Loss: 0.35006245970726013\n",
            "Epoch: 910 | Loss: 0.16618108749389648 | Val. Loss: 0.3498261570930481\n",
            "Epoch: 911 | Loss: 0.2226226031780243 | Val. Loss: 0.34979796409606934\n",
            "Epoch: 912 | Loss: 0.30983275175094604 | Val. Loss: 0.3498288691043854\n",
            "Epoch: 913 | Loss: 0.36762773990631104 | Val. Loss: 0.34964174032211304\n",
            "Epoch: 914 | Loss: 0.35191985964775085 | Val. Loss: 0.3500797152519226\n",
            "Epoch: 915 | Loss: 0.2961026430130005 | Val. Loss: 0.35037508606910706\n",
            "Epoch: 916 | Loss: 0.30302107334136963 | Val. Loss: 0.34979110956192017\n",
            "Epoch: 917 | Loss: 0.2917691469192505 | Val. Loss: 0.3498823642730713\n",
            "Epoch: 918 | Loss: 0.3476106822490692 | Val. Loss: 0.3498419225215912\n",
            "Epoch: 919 | Loss: 0.24924862384796143 | Val. Loss: 0.34978359937667847\n",
            "Epoch: 920 | Loss: 0.30818861722946167 | Val. Loss: 0.3497936725616455\n",
            "Epoch: 921 | Loss: 0.3765623867511749 | Val. Loss: 0.349604994058609\n",
            "Epoch: 922 | Loss: 0.2670329511165619 | Val. Loss: 0.3495473265647888\n",
            "Epoch: 923 | Loss: 0.28226450085639954 | Val. Loss: 0.3499085307121277\n",
            "Epoch: 924 | Loss: 0.3580020070075989 | Val. Loss: 0.34991663694381714\n",
            "Epoch: 925 | Loss: 0.30979883670806885 | Val. Loss: 0.34970203042030334\n",
            "Epoch: 926 | Loss: 0.35080355405807495 | Val. Loss: 0.3498346507549286\n",
            "Epoch: 927 | Loss: 0.28239235281944275 | Val. Loss: 0.3497212827205658\n",
            "Epoch: 928 | Loss: 0.22545821964740753 | Val. Loss: 0.34984856843948364\n",
            "Epoch: 929 | Loss: 0.3509281575679779 | Val. Loss: 0.3496624231338501\n",
            "Epoch: 930 | Loss: 0.38891565799713135 | Val. Loss: 0.34953972697257996\n",
            "Epoch: 931 | Loss: 0.33643630146980286 | Val. Loss: 0.34920984506607056\n",
            "Epoch: 932 | Loss: 0.20439955592155457 | Val. Loss: 0.34953996539115906\n",
            "Epoch: 933 | Loss: 0.44054287672042847 | Val. Loss: 0.34972041845321655\n",
            "Epoch: 934 | Loss: 0.3423853814601898 | Val. Loss: 0.3497680723667145\n",
            "Epoch: 935 | Loss: 0.23035582900047302 | Val. Loss: 0.34943801164627075\n",
            "Epoch: 936 | Loss: 0.32922807335853577 | Val. Loss: 0.3494578003883362\n",
            "Epoch: 937 | Loss: 0.41644343733787537 | Val. Loss: 0.3495064675807953\n",
            "Epoch: 938 | Loss: 0.2156079113483429 | Val. Loss: 0.3496677279472351\n",
            "Epoch: 939 | Loss: 0.2610272765159607 | Val. Loss: 0.3496140241622925\n",
            "Epoch: 940 | Loss: 0.24329233169555664 | Val. Loss: 0.34941357374191284\n",
            "Epoch: 941 | Loss: 0.333954781293869 | Val. Loss: 0.34931740164756775\n",
            "Epoch: 942 | Loss: 0.43204420804977417 | Val. Loss: 0.34935280680656433\n",
            "Epoch: 943 | Loss: 0.41817784309387207 | Val. Loss: 0.34984076023101807\n",
            "Epoch: 944 | Loss: 0.3395989239215851 | Val. Loss: 0.3498086631298065\n",
            "Epoch: 945 | Loss: 0.29666098952293396 | Val. Loss: 0.34972715377807617\n",
            "Epoch: 946 | Loss: 0.3982033431529999 | Val. Loss: 0.34982091188430786\n",
            "Epoch: 947 | Loss: 0.2100203037261963 | Val. Loss: 0.34939780831336975\n",
            "Epoch: 948 | Loss: 0.4290333688259125 | Val. Loss: 0.3496320843696594\n",
            "Epoch: 949 | Loss: 0.35179826617240906 | Val. Loss: 0.34966933727264404\n",
            "Epoch: 950 | Loss: 0.33179399371147156 | Val. Loss: 0.34954845905303955\n",
            "Epoch: 951 | Loss: 0.347353458404541 | Val. Loss: 0.34958505630493164\n",
            "Epoch: 952 | Loss: 0.2004522979259491 | Val. Loss: 0.34932878613471985\n",
            "Epoch: 953 | Loss: 0.40021657943725586 | Val. Loss: 0.3494824767112732\n",
            "Epoch: 954 | Loss: 0.40227678418159485 | Val. Loss: 0.34936800599098206\n",
            "Epoch: 955 | Loss: 0.2609279155731201 | Val. Loss: 0.34917300939559937\n",
            "Epoch: 956 | Loss: 0.30104973912239075 | Val. Loss: 0.3493596911430359\n",
            "Epoch: 957 | Loss: 0.30974477529525757 | Val. Loss: 0.3493197560310364\n",
            "Epoch: 958 | Loss: 0.1936233788728714 | Val. Loss: 0.3494780659675598\n",
            "Epoch: 959 | Loss: 0.28726068139076233 | Val. Loss: 0.34946736693382263\n",
            "Epoch: 960 | Loss: 0.2527034878730774 | Val. Loss: 0.34941452741622925\n",
            "Epoch: 961 | Loss: 0.3344666659832001 | Val. Loss: 0.34938761591911316\n",
            "Epoch: 962 | Loss: 0.3148235082626343 | Val. Loss: 0.34916144609451294\n",
            "Epoch: 963 | Loss: 0.3308609426021576 | Val. Loss: 0.34920069575309753\n",
            "Epoch: 964 | Loss: 0.25067076086997986 | Val. Loss: 0.34947431087493896\n",
            "Epoch: 965 | Loss: 0.3265904486179352 | Val. Loss: 0.3495403528213501\n",
            "Epoch: 966 | Loss: 0.2713705897331238 | Val. Loss: 0.3496339023113251\n",
            "Epoch: 967 | Loss: 0.29624679684638977 | Val. Loss: 0.34910938143730164\n",
            "Epoch: 968 | Loss: 0.30051833391189575 | Val. Loss: 0.34919193387031555\n",
            "Epoch: 969 | Loss: 0.4039684534072876 | Val. Loss: 0.34932655096054077\n",
            "Epoch: 970 | Loss: 0.38567182421684265 | Val. Loss: 0.3489864766597748\n",
            "Epoch: 971 | Loss: 0.2296966165304184 | Val. Loss: 0.3493975102901459\n",
            "Epoch: 972 | Loss: 0.22998274862766266 | Val. Loss: 0.3493494391441345\n",
            "Epoch: 973 | Loss: 0.23012249171733856 | Val. Loss: 0.3492250442504883\n",
            "Epoch: 974 | Loss: 0.2474260926246643 | Val. Loss: 0.3491036593914032\n",
            "Epoch: 975 | Loss: 0.3963768482208252 | Val. Loss: 0.34926891326904297\n",
            "Epoch: 976 | Loss: 0.344356507062912 | Val. Loss: 0.34912121295928955\n",
            "Epoch: 977 | Loss: 0.343715101480484 | Val. Loss: 0.3490843176841736\n",
            "Epoch: 978 | Loss: 0.38978877663612366 | Val. Loss: 0.34962204098701477\n",
            "Epoch: 979 | Loss: 0.45445680618286133 | Val. Loss: 0.3495164215564728\n",
            "Epoch: 980 | Loss: 0.2448587417602539 | Val. Loss: 0.3490607738494873\n",
            "Epoch: 981 | Loss: 0.3199242353439331 | Val. Loss: 0.349334180355072\n",
            "Epoch: 982 | Loss: 0.21803082525730133 | Val. Loss: 0.34910744428634644\n",
            "Epoch: 983 | Loss: 0.23015770316123962 | Val. Loss: 0.34878847002983093\n",
            "Epoch: 984 | Loss: 0.2632632255554199 | Val. Loss: 0.34861230850219727\n",
            "Epoch: 985 | Loss: 0.2851245403289795 | Val. Loss: 0.34889912605285645\n",
            "Epoch: 986 | Loss: 0.3652629852294922 | Val. Loss: 0.3490789234638214\n",
            "Epoch: 987 | Loss: 0.3230084776878357 | Val. Loss: 0.3489944040775299\n",
            "Epoch: 988 | Loss: 0.3518106937408447 | Val. Loss: 0.34913522005081177\n",
            "Epoch: 989 | Loss: 0.2174452394247055 | Val. Loss: 0.3492599129676819\n",
            "Epoch: 990 | Loss: 0.23762546479701996 | Val. Loss: 0.34905827045440674\n",
            "Epoch: 991 | Loss: 0.22486910223960876 | Val. Loss: 0.34917157888412476\n",
            "Epoch: 992 | Loss: 0.33518561720848083 | Val. Loss: 0.34890949726104736\n",
            "Epoch: 993 | Loss: 0.30521681904792786 | Val. Loss: 0.34915170073509216\n",
            "Epoch: 994 | Loss: 0.2414638251066208 | Val. Loss: 0.348879873752594\n",
            "Epoch: 995 | Loss: 0.27216970920562744 | Val. Loss: 0.3487592339515686\n",
            "Epoch: 996 | Loss: 0.40181097388267517 | Val. Loss: 0.34899717569351196\n",
            "Epoch: 997 | Loss: 0.4093568027019501 | Val. Loss: 0.34916701912879944\n",
            "Epoch: 998 | Loss: 0.2134031057357788 | Val. Loss: 0.34895914793014526\n",
            "Epoch: 999 | Loss: 0.33833175897598267 | Val. Loss: 0.34890127182006836\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Loss')"
            ]
          },
          "metadata": {},
          "execution_count": 24
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testando o modelo\n",
        "\n",
        "# Converte os dados de teste em arrays numpy\n",
        "x_teste_np = data_test.iloc[:, :-1].to_numpy()\n",
        "d_teste_np = data_test.iloc[:, -1].to_numpy()\n",
        "\n",
        "# Aplica a normalização no conjunto de teste\n",
        "# x_teste_np_norm = (x_teste_np - media_treino) / desvio_padrao_treino\n",
        "\n",
        "# Testa o modelo com os dados de teste\n",
        "y_teste_tensor_lda = model_lda(x_teste_lda)\n",
        "y_teste_np = y_teste_tensor_lda.cpu().detach().numpy()\n",
        "\n",
        "predicoes_lda = np.argmax(y_teste_np, axis=1)\n",
        "acuracia_lda = np.mean(predicoes_lda == d_teste_np)\n",
        "\n",
        "# Taxa de erros\n",
        "\n",
        "Taxa_de_erro_lda = (1 - acuracia_lda) * 100\n",
        "\n",
        "print(f\"Acurácia: {acuracia_lda*100:.2f}%\")\n",
        "print(f\"Taxa de erro: {Taxa_de_erro_lda:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TrxVG52jSLM",
        "outputId": "61c326ed-8503-434f-e18a-0e3d01424adc"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia: 80.00%\n",
            "Taxa de erro: 20.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GRv9Swq3j467"
      },
      "execution_count": 25,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}