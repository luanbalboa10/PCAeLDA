{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26fxjoVADscQ"
      },
      "source": [
        "Para abrir o notebook no Google Colab, altere o domínio `github.com` para `githubtocolab.com`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sf-Xuxsf3K_T"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\">\n",
        "Para praticar programação, é importante que você erre, leia as mensagens de erro e tente corrigí-los.\n",
        "    \n",
        "Dessa forma, no Google Colab, é importante que você DESATIVE OS RECURSOS DE AUTOCOMPLETAR:\n",
        "\n",
        "- Menu Ferramentas -> Configurações\n",
        "- Na janela que é aberta:\n",
        "  - Seção Editor -> Desativar \"Mostrar sugestões de preenchimento de código com base no contexto\"\n",
        "  - Seção Assistência de IA -> Desabilitar itens\n",
        "\n",
        "Na versão em inglês:\n",
        "\n",
        "- Menu Tools -> Settings\n",
        "- Na janela que é aberta:\n",
        "  - Seção Editor -> Desativar \"Show context-powered code completions\"\n",
        "  - Seção AI Assistance -> Desabilitar itens\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c04thLl8Dzh4"
      },
      "source": [
        "# PSI5892 - Aula de Exercícios\n",
        "\n",
        "# Análise de componentes principais (PCA) e análise de discriminantes lineares (LDA)\n",
        "\n",
        "Neste exercício, você vai trabalhar com uma aplicação de redes neurais para  para a área de saúde. O objetivo é obter um modelo de predição de uma doença cardíaca baseado em dados com características extraídas de exames clínicos laboratoriais.\n",
        "\n",
        "## Dados disponibilizados\n",
        "\n",
        "Os dados para treinamento e teste do modelo estão disponíveis no formato CSV, em um arquivo zip disponível [neste link](./data.zip).\n",
        "\n",
        "Após extrair os arquivos, utiliza a biblioteca Pandas para carregar os `DataFrames` `data_train` e `data_test`, como mostrado a seguir:\n",
        "\n",
        "``` python\n",
        "import pandas as pd\n",
        "\n",
        "data_train = pd.read_csv(\"data_train.csv\").drop(columns=[\"Unnamed: 0\"])\n",
        "\n",
        "data_test = pd.read_csv(\"data_test.csv\").drop(columns=[\"Unnamed: 0\"])\n",
        "```\n",
        "\n",
        "Os dados consistem de 800 exemplos de treinamento e 225 para teste, cada um contendo 13 características de entrada, representadas pelas colunas de 0 a 12 e a saída desejada binária, indicando se o paciente é portador ou não da doença, representada pela coluna 13.\n",
        "\n",
        "O objetivo é treinar uma rede neural com estes dados, avaliar o desempenho e depois comparar com o desempenho obtido usando o PCA para realizar redução de dimensionalidade. Use como referência o exemplo mostrado [neste Jupyter Notebook](./PCA_IRIS.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thFmnLOj3K_Y"
      },
      "source": [
        "# Exercício 1\n",
        "\n",
        "Implemente uma rede neural para classificar se o indivíduo é portador ou não da doença cardíaca (coluna 13) usando como entrada os dados dos exames laboratoriais (colunas 1 a 12). Calcule a acurácia obtida nos dados de teste.\n",
        "\n",
        "## Resolução"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "x8pe0-5-3K_b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregamento dos dados\n",
        "\n",
        "data_train = pd.read_csv(\"data_train.csv\").drop(columns=[\"Unnamed: 0\"])\n",
        "data_test = pd.read_csv(\"data_test.csv\").drop(columns=[\"Unnamed: 0\"])"
      ],
      "metadata": {
        "id": "qehWt4DP3oS9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "659kqytE51Za",
        "outputId": "9184d829-1095-463e-92c7-acbfd2e837fc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        0    1    2      3      4    5    6      7    8    9   10   11   12  \\\n",
              "0    52.0  1.0  0.0  125.0  212.0  0.0  1.0  168.0  0.0  1.0  2.0  2.0  3.0   \n",
              "1    53.0  1.0  0.0  140.0  203.0  1.0  0.0  155.0  1.0  3.1  0.0  0.0  3.0   \n",
              "2    70.0  1.0  0.0  145.0  174.0  0.0  1.0  125.0  1.0  2.6  0.0  0.0  3.0   \n",
              "3    61.0  1.0  0.0  148.0  203.0  0.0  1.0  161.0  0.0  0.0  2.0  1.0  3.0   \n",
              "4    62.0  0.0  0.0  138.0  294.0  1.0  1.0  106.0  0.0  1.9  1.0  3.0  2.0   \n",
              "..    ...  ...  ...    ...    ...  ...  ...    ...  ...  ...  ...  ...  ...   \n",
              "795  62.0  1.0  1.0  128.0  208.0  1.0  0.0  140.0  0.0  0.0  2.0  0.0  2.0   \n",
              "796  41.0  1.0  1.0  135.0  203.0  0.0  1.0  132.0  0.0  0.0  1.0  0.0  1.0   \n",
              "797  65.0  0.0  0.0  150.0  225.0  0.0  0.0  114.0  0.0  1.0  1.0  3.0  3.0   \n",
              "798  59.0  1.0  3.0  170.0  288.0  0.0  0.0  159.0  0.0  0.2  1.0  0.0  3.0   \n",
              "799  43.0  1.0  0.0  115.0  303.0  0.0  1.0  181.0  0.0  1.2  1.0  0.0  2.0   \n",
              "\n",
              "      13  \n",
              "0    0.0  \n",
              "1    0.0  \n",
              "2    0.0  \n",
              "3    0.0  \n",
              "4    0.0  \n",
              "..   ...  \n",
              "795  1.0  \n",
              "796  1.0  \n",
              "797  0.0  \n",
              "798  0.0  \n",
              "799  1.0  \n",
              "\n",
              "[800 rows x 14 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-30fa5f09-8b9d-437f-987d-63def057d79c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>52.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>125.0</td>\n",
              "      <td>212.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>168.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>53.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>140.0</td>\n",
              "      <td>203.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>155.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>70.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>145.0</td>\n",
              "      <td>174.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>125.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>61.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>148.0</td>\n",
              "      <td>203.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>161.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>62.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>138.0</td>\n",
              "      <td>294.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>106.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.9</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>795</th>\n",
              "      <td>62.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>128.0</td>\n",
              "      <td>208.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>140.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>796</th>\n",
              "      <td>41.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>135.0</td>\n",
              "      <td>203.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>797</th>\n",
              "      <td>65.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>225.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>798</th>\n",
              "      <td>59.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>288.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>159.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>799</th>\n",
              "      <td>43.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>115.0</td>\n",
              "      <td>303.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>181.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>800 rows × 14 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-30fa5f09-8b9d-437f-987d-63def057d79c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-30fa5f09-8b9d-437f-987d-63def057d79c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-30fa5f09-8b9d-437f-987d-63def057d79c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-d0fa963c-a7fe-43aa-aaf5-d20a0c8b4132\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d0fa963c-a7fe-43aa-aaf5-d20a0c8b4132')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-d0fa963c-a7fe-43aa-aaf5-d20a0c8b4132 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_8b79a961-309e-464d-b643-b3e637ef152b\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('data_train')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_8b79a961-309e-464d-b643-b3e637ef152b button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('data_train');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data_train",
              "summary": "{\n  \"name\": \"data_train\",\n  \"rows\": 800,\n  \"fields\": [\n    {\n      \"column\": \"0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9.232059014220622,\n        \"min\": 29.0,\n        \"max\": 77.0,\n        \"num_unique_values\": 41,\n        \"samples\": [\n          65.0,\n          50.0,\n          54.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4657949722734747,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0156588458305733,\n        \"min\": 0.0,\n        \"max\": 3.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1.0,\n          3.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 17.857721656988417,\n        \"min\": 94.0,\n        \"max\": 200.0,\n        \"num_unique_values\": 49,\n        \"samples\": [\n          128.0,\n          172.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"4\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 53.06672764469413,\n        \"min\": 126.0,\n        \"max\": 564.0,\n        \"num_unique_values\": 152,\n        \"samples\": [\n          267.0,\n          262.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"5\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.36331933429607305,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"6\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5245833915292517,\n        \"min\": 0.0,\n        \"max\": 2.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"7\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 23.076981688574847,\n        \"min\": 71.0,\n        \"max\": 202.0,\n        \"num_unique_values\": 90,\n        \"samples\": [\n          180.0,\n          152.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"8\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4718466239634246,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"9\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.1848183376013328,\n        \"min\": 0.0,\n        \"max\": 6.2,\n        \"num_unique_values\": 40,\n        \"samples\": [\n          2.8,\n          0.4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"10\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6151202859667018,\n        \"min\": 0.0,\n        \"max\": 2.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"11\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0495953985349717,\n        \"min\": 0.0,\n        \"max\": 4.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.0,\n          4.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"12\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6212295531420899,\n        \"min\": 0.0,\n        \"max\": 3.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"13\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.49991238281133826,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_total = len(data_train)\n",
        "n_treino = int(n_total * 0.8) # Divisão 80/20 (treino/validação)\n",
        "\n",
        "indices_aleatorios = np.random.permutation(n_total)\n",
        "\n",
        "indices_treino = indices_aleatorios[:n_treino]\n",
        "indices_val = indices_aleatorios[n_treino:]\n",
        "\n",
        "# DataFrames de treino e validação\n",
        "df_treino = data_train.iloc[indices_treino]\n",
        "df_val = data_train.iloc[indices_val]\n",
        "\n",
        "print(len(df_treino))\n",
        "print(len(df_val))\n",
        "\n",
        "# Dados de treino\n",
        "x_treino_np = df_treino.iloc[:, :-1].to_numpy()\n",
        "d_treino_np = df_treino.iloc[:, -1].to_numpy()\n",
        "\n",
        "# Dados de validação\n",
        "x_val_np = df_val.iloc[:, :-1].to_numpy()\n",
        "d_val_np = df_val.iloc[:, -1].to_numpy()\n",
        "\n",
        "# ============================================================== #\n",
        "# Normalizar os dados de treino e val\n",
        "\n",
        "# Calcula a média das características\n",
        "media_treino = np.mean(x_treino_np, axis=0)\n",
        "\n",
        "# Calcula o desvio padrao das características\n",
        "desvio_padrao_treino = np.std(x_treino_np, axis=0, ddof=1)\n",
        "\n",
        "# Aplica a normalização no conjunto\n",
        "x_treino_np_norm = (x_treino_np - media_treino) / desvio_padrao_treino\n",
        "x_val_np_norm = (x_val_np - media_treino) / desvio_padrao_treino\n",
        "\n",
        "# ============================================================== #\n",
        "# Converter para tensores PyTorch\n",
        "x_treino_tensor = torch.tensor(x_treino_np_norm, dtype=torch.float32)\n",
        "d_treino_tensor = torch.tensor(d_treino_np, dtype=torch.long)\n",
        "\n",
        "x_val_tensor = torch.tensor(x_val_np_norm, dtype=torch.float32)\n",
        "d_val_tensor = torch.tensor(d_val_np, dtype=torch.long)\n",
        "\n",
        "# print(f\"x_treino: {x_treino_np.shape}\")\n",
        "# print(f\"d_treino: {d_treino_np.shape}\")\n",
        "# print(f\"x_val: {x_val_np.shape}\")\n",
        "# print(f\"d_val: {d_val_np.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkTDgfH7FVDQ",
        "outputId": "d9261cb4-fd45-4c20-e894-2985018f0a5d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "640\n",
            "160\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.model = nn.Sequential(\n",
        "        nn.Linear(13, 8),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.Linear(8, 4),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.Linear(4, 4),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.Linear(4, 2),\n",
        "    )\n",
        "\n",
        "    self._init_weights()\n",
        "\n",
        "  def _init_weights(self):\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Linear):\n",
        "        nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
        "\n",
        "        # Inicializa os bias com zero\n",
        "        if m.bias is not None:\n",
        "          nn.init.constant_(m.bias, 0)\n",
        "\n",
        "  def forward(self, x):\n",
        "    output = self.model(x)\n",
        "    return output"
      ],
      "metadata": {
        "id": "IAjXWGec9nSf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Model().to(device=device)\n",
        "\n",
        "# Taxa de aprendizado\n",
        "eta = 0.001\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=eta)\n",
        "\n",
        "Nb = 64 # Tamanho do mini-batch\n",
        "Ne = 1000 # Número de épocas\n"
      ],
      "metadata": {
        "id": "0itvtGvd-HCM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_set = TensorDataset(x_treino_tensor, d_treino_tensor)\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=Nb, shuffle=True)"
      ],
      "metadata": {
        "id": "Uhve4y04HqVW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Treinamento\n",
        "losses = []\n",
        "val_losses = []\n",
        "\n",
        "x_val_tensor = x_val_tensor.to(device=device)\n",
        "d_val_tensor = d_val_tensor.to(device=device)\n",
        "\n",
        "for epoch in range(Ne):\n",
        "  for n, (X, d) in enumerate(train_loader):\n",
        "\n",
        "    X = X.to(device=device)\n",
        "    d = d.to(device=device)\n",
        "\n",
        "    # Treinamento\n",
        "    model.train()\n",
        "    model.zero_grad()\n",
        "    y = model(X)\n",
        "    loss = loss_function(y, d)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Validação\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      y_val = model(x_val_tensor)\n",
        "      val_loss = loss_function(y_val, d_val_tensor)\n",
        "\n",
        "    losses.append(loss.item())\n",
        "    val_losses.append(val_loss.item())\n",
        "\n",
        "    if epoch % 1 == 0 and n == x_treino_tensor.shape[0]//Nb - 1:\n",
        "      print(f\"Epoch: {epoch} | Loss: {loss} | Val. Loss: {val_loss}\")\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(losses)\n",
        "plt.plot(val_losses, alpha=0.8)\n",
        "plt.legend([\"Loss\", \"Val. Loss\"])\n",
        "plt.xlabel(\"Batch\")\n",
        "plt.ylabel(\"Loss\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CmkDGipS-qmR",
        "outputId": "5fc9756b-0b91-4967-f194-f50fd4cedbab"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | Loss: 1.1117432117462158 | Val. Loss: 1.006931185722351\n",
            "Epoch: 1 | Loss: 0.874767005443573 | Val. Loss: 0.9091665148735046\n",
            "Epoch: 2 | Loss: 0.8718933463096619 | Val. Loss: 0.8369107246398926\n",
            "Epoch: 3 | Loss: 0.7873045802116394 | Val. Loss: 0.7852379083633423\n",
            "Epoch: 4 | Loss: 0.7184702157974243 | Val. Loss: 0.7457254528999329\n",
            "Epoch: 5 | Loss: 0.8258110880851746 | Val. Loss: 0.7148048281669617\n",
            "Epoch: 6 | Loss: 0.7319589257240295 | Val. Loss: 0.689213216304779\n",
            "Epoch: 7 | Loss: 0.6962003111839294 | Val. Loss: 0.6667453646659851\n",
            "Epoch: 8 | Loss: 0.6762041449546814 | Val. Loss: 0.6464430093765259\n",
            "Epoch: 9 | Loss: 0.5536878108978271 | Val. Loss: 0.6263147592544556\n",
            "Epoch: 10 | Loss: 0.5572153925895691 | Val. Loss: 0.6067589521408081\n",
            "Epoch: 11 | Loss: 0.6220384836196899 | Val. Loss: 0.5894861221313477\n",
            "Epoch: 12 | Loss: 0.5398086905479431 | Val. Loss: 0.5755870938301086\n",
            "Epoch: 13 | Loss: 0.5239238739013672 | Val. Loss: 0.5618473291397095\n",
            "Epoch: 14 | Loss: 0.5003175139427185 | Val. Loss: 0.5482184886932373\n",
            "Epoch: 15 | Loss: 0.6532996296882629 | Val. Loss: 0.5348178148269653\n",
            "Epoch: 16 | Loss: 0.48786598443984985 | Val. Loss: 0.5216172337532043\n",
            "Epoch: 17 | Loss: 0.5246638655662537 | Val. Loss: 0.5095864534378052\n",
            "Epoch: 18 | Loss: 0.43410712480545044 | Val. Loss: 0.49849945306777954\n",
            "Epoch: 19 | Loss: 0.43549370765686035 | Val. Loss: 0.48728054761886597\n",
            "Epoch: 20 | Loss: 0.42090854048728943 | Val. Loss: 0.4761478304862976\n",
            "Epoch: 21 | Loss: 0.4711814522743225 | Val. Loss: 0.4665408730506897\n",
            "Epoch: 22 | Loss: 0.477057546377182 | Val. Loss: 0.45807820558547974\n",
            "Epoch: 23 | Loss: 0.3762563467025757 | Val. Loss: 0.4487488269805908\n",
            "Epoch: 24 | Loss: 0.44420409202575684 | Val. Loss: 0.4408058226108551\n",
            "Epoch: 25 | Loss: 0.3729173541069031 | Val. Loss: 0.4337625503540039\n",
            "Epoch: 26 | Loss: 0.4020889401435852 | Val. Loss: 0.42700451612472534\n",
            "Epoch: 27 | Loss: 0.4652976095676422 | Val. Loss: 0.4208222031593323\n",
            "Epoch: 28 | Loss: 0.4566360414028168 | Val. Loss: 0.4154062271118164\n",
            "Epoch: 29 | Loss: 0.3555017113685608 | Val. Loss: 0.4086240231990814\n",
            "Epoch: 30 | Loss: 0.30443593859672546 | Val. Loss: 0.4020553231239319\n",
            "Epoch: 31 | Loss: 0.3160286545753479 | Val. Loss: 0.39555948972702026\n",
            "Epoch: 32 | Loss: 0.3483904302120209 | Val. Loss: 0.3890667259693146\n",
            "Epoch: 33 | Loss: 0.4244063198566437 | Val. Loss: 0.38368913531303406\n",
            "Epoch: 34 | Loss: 0.4380793571472168 | Val. Loss: 0.37867942452430725\n",
            "Epoch: 35 | Loss: 0.33970439434051514 | Val. Loss: 0.3714970350265503\n",
            "Epoch: 36 | Loss: 0.30385419726371765 | Val. Loss: 0.3668097257614136\n",
            "Epoch: 37 | Loss: 0.25384923815727234 | Val. Loss: 0.3623870611190796\n",
            "Epoch: 38 | Loss: 0.38283464312553406 | Val. Loss: 0.3562939763069153\n",
            "Epoch: 39 | Loss: 0.2297242432832718 | Val. Loss: 0.3526475727558136\n",
            "Epoch: 40 | Loss: 0.295983225107193 | Val. Loss: 0.3472660779953003\n",
            "Epoch: 41 | Loss: 0.29526227712631226 | Val. Loss: 0.34233319759368896\n",
            "Epoch: 42 | Loss: 0.2758185863494873 | Val. Loss: 0.33886975049972534\n",
            "Epoch: 43 | Loss: 0.30769529938697815 | Val. Loss: 0.3354150056838989\n",
            "Epoch: 44 | Loss: 0.2947733700275421 | Val. Loss: 0.3328438401222229\n",
            "Epoch: 45 | Loss: 0.36066150665283203 | Val. Loss: 0.3284578323364258\n",
            "Epoch: 46 | Loss: 0.29415270686149597 | Val. Loss: 0.32609784603118896\n",
            "Epoch: 47 | Loss: 0.2351323962211609 | Val. Loss: 0.3229118287563324\n",
            "Epoch: 48 | Loss: 0.3316958546638489 | Val. Loss: 0.3197537958621979\n",
            "Epoch: 49 | Loss: 0.2736448049545288 | Val. Loss: 0.3167940676212311\n",
            "Epoch: 50 | Loss: 0.2822563052177429 | Val. Loss: 0.31510066986083984\n",
            "Epoch: 51 | Loss: 0.29705125093460083 | Val. Loss: 0.31276699900627136\n",
            "Epoch: 52 | Loss: 0.2629331946372986 | Val. Loss: 0.30983561277389526\n",
            "Epoch: 53 | Loss: 0.23250600695610046 | Val. Loss: 0.3081774413585663\n",
            "Epoch: 54 | Loss: 0.22887346148490906 | Val. Loss: 0.30598747730255127\n",
            "Epoch: 55 | Loss: 0.2791177034378052 | Val. Loss: 0.3043268322944641\n",
            "Epoch: 56 | Loss: 0.2312217503786087 | Val. Loss: 0.3024030327796936\n",
            "Epoch: 57 | Loss: 0.3467273414134979 | Val. Loss: 0.29991403222084045\n",
            "Epoch: 58 | Loss: 0.24949461221694946 | Val. Loss: 0.3002016544342041\n",
            "Epoch: 59 | Loss: 0.23574864864349365 | Val. Loss: 0.298799991607666\n",
            "Epoch: 60 | Loss: 0.23623204231262207 | Val. Loss: 0.2967548370361328\n",
            "Epoch: 61 | Loss: 0.2818051874637604 | Val. Loss: 0.2968507707118988\n",
            "Epoch: 62 | Loss: 0.28107911348342896 | Val. Loss: 0.29491376876831055\n",
            "Epoch: 63 | Loss: 0.3110678791999817 | Val. Loss: 0.29448625445365906\n",
            "Epoch: 64 | Loss: 0.23147240281105042 | Val. Loss: 0.29269903898239136\n",
            "Epoch: 65 | Loss: 0.175625741481781 | Val. Loss: 0.29212719202041626\n",
            "Epoch: 66 | Loss: 0.19756387174129486 | Val. Loss: 0.29271191358566284\n",
            "Epoch: 67 | Loss: 0.3382527828216553 | Val. Loss: 0.291193425655365\n",
            "Epoch: 68 | Loss: 0.29916366934776306 | Val. Loss: 0.28925949335098267\n",
            "Epoch: 69 | Loss: 0.3932386636734009 | Val. Loss: 0.2894107699394226\n",
            "Epoch: 70 | Loss: 0.2922893762588501 | Val. Loss: 0.2886151671409607\n",
            "Epoch: 71 | Loss: 0.2773270308971405 | Val. Loss: 0.28836140036582947\n",
            "Epoch: 72 | Loss: 0.27493399381637573 | Val. Loss: 0.28695765137672424\n",
            "Epoch: 73 | Loss: 0.20376069843769073 | Val. Loss: 0.2874893844127655\n",
            "Epoch: 74 | Loss: 0.22276265919208527 | Val. Loss: 0.28665122389793396\n",
            "Epoch: 75 | Loss: 0.2945541441440582 | Val. Loss: 0.2866553068161011\n",
            "Epoch: 76 | Loss: 0.2145233303308487 | Val. Loss: 0.2856872081756592\n",
            "Epoch: 77 | Loss: 0.249323770403862 | Val. Loss: 0.28524190187454224\n",
            "Epoch: 78 | Loss: 0.25276675820350647 | Val. Loss: 0.28407156467437744\n",
            "Epoch: 79 | Loss: 0.21193628013134003 | Val. Loss: 0.2846405506134033\n",
            "Epoch: 80 | Loss: 0.2289244830608368 | Val. Loss: 0.2841205596923828\n",
            "Epoch: 81 | Loss: 0.1640336662530899 | Val. Loss: 0.28373533487319946\n",
            "Epoch: 82 | Loss: 0.24539881944656372 | Val. Loss: 0.28338295221328735\n",
            "Epoch: 83 | Loss: 0.20494748651981354 | Val. Loss: 0.2826908230781555\n",
            "Epoch: 84 | Loss: 0.3483031988143921 | Val. Loss: 0.28252026438713074\n",
            "Epoch: 85 | Loss: 0.2672436833381653 | Val. Loss: 0.28126639127731323\n",
            "Epoch: 86 | Loss: 0.28636789321899414 | Val. Loss: 0.28165537118911743\n",
            "Epoch: 87 | Loss: 0.1887407749891281 | Val. Loss: 0.28143438696861267\n",
            "Epoch: 88 | Loss: 0.1350710690021515 | Val. Loss: 0.2801308035850525\n",
            "Epoch: 89 | Loss: 0.31890618801116943 | Val. Loss: 0.27890610694885254\n",
            "Epoch: 90 | Loss: 0.20194263756275177 | Val. Loss: 0.27891117334365845\n",
            "Epoch: 91 | Loss: 0.26411527395248413 | Val. Loss: 0.2790409028530121\n",
            "Epoch: 92 | Loss: 0.31876999139785767 | Val. Loss: 0.27848976850509644\n",
            "Epoch: 93 | Loss: 0.3775079846382141 | Val. Loss: 0.2783221900463104\n",
            "Epoch: 94 | Loss: 0.2703782916069031 | Val. Loss: 0.27759578824043274\n",
            "Epoch: 95 | Loss: 0.3152644634246826 | Val. Loss: 0.27635180950164795\n",
            "Epoch: 96 | Loss: 0.3083583414554596 | Val. Loss: 0.2756488025188446\n",
            "Epoch: 97 | Loss: 0.30485373735427856 | Val. Loss: 0.27597668766975403\n",
            "Epoch: 98 | Loss: 0.2995884418487549 | Val. Loss: 0.2758278250694275\n",
            "Epoch: 99 | Loss: 0.2997570335865021 | Val. Loss: 0.2757991850376129\n",
            "Epoch: 100 | Loss: 0.2394414097070694 | Val. Loss: 0.27460789680480957\n",
            "Epoch: 101 | Loss: 0.2643219828605652 | Val. Loss: 0.27298420667648315\n",
            "Epoch: 102 | Loss: 0.16957224905490875 | Val. Loss: 0.27347519993782043\n",
            "Epoch: 103 | Loss: 0.27911457419395447 | Val. Loss: 0.2735012471675873\n",
            "Epoch: 104 | Loss: 0.31864118576049805 | Val. Loss: 0.2719893455505371\n",
            "Epoch: 105 | Loss: 0.2039744257926941 | Val. Loss: 0.2713659405708313\n",
            "Epoch: 106 | Loss: 0.22310420870780945 | Val. Loss: 0.27229005098342896\n",
            "Epoch: 107 | Loss: 0.2646230459213257 | Val. Loss: 0.2711125910282135\n",
            "Epoch: 108 | Loss: 0.22788795828819275 | Val. Loss: 0.2717667520046234\n",
            "Epoch: 109 | Loss: 0.23088183999061584 | Val. Loss: 0.26961737871170044\n",
            "Epoch: 110 | Loss: 0.2269498109817505 | Val. Loss: 0.26971226930618286\n",
            "Epoch: 111 | Loss: 0.18965035676956177 | Val. Loss: 0.26842156052589417\n",
            "Epoch: 112 | Loss: 0.23968930542469025 | Val. Loss: 0.26945623755455017\n",
            "Epoch: 113 | Loss: 0.2111138552427292 | Val. Loss: 0.27054399251937866\n",
            "Epoch: 114 | Loss: 0.22704915702342987 | Val. Loss: 0.26858991384506226\n",
            "Epoch: 115 | Loss: 0.2924644351005554 | Val. Loss: 0.2674398720264435\n",
            "Epoch: 116 | Loss: 0.2853851616382599 | Val. Loss: 0.2674641013145447\n",
            "Epoch: 117 | Loss: 0.34099671244621277 | Val. Loss: 0.2665455937385559\n",
            "Epoch: 118 | Loss: 0.2352418303489685 | Val. Loss: 0.26762866973876953\n",
            "Epoch: 119 | Loss: 0.1797875612974167 | Val. Loss: 0.26668572425842285\n",
            "Epoch: 120 | Loss: 0.3155863285064697 | Val. Loss: 0.2659684419631958\n",
            "Epoch: 121 | Loss: 0.26633840799331665 | Val. Loss: 0.26567327976226807\n",
            "Epoch: 122 | Loss: 0.18372377753257751 | Val. Loss: 0.26431044936180115\n",
            "Epoch: 123 | Loss: 0.23146551847457886 | Val. Loss: 0.26379987597465515\n",
            "Epoch: 124 | Loss: 0.322070837020874 | Val. Loss: 0.2644936144351959\n",
            "Epoch: 125 | Loss: 0.22676484286785126 | Val. Loss: 0.2641732096672058\n",
            "Epoch: 126 | Loss: 0.26465368270874023 | Val. Loss: 0.264059454202652\n",
            "Epoch: 127 | Loss: 0.24161463975906372 | Val. Loss: 0.26386240124702454\n",
            "Epoch: 128 | Loss: 0.33749526739120483 | Val. Loss: 0.2616325318813324\n",
            "Epoch: 129 | Loss: 0.235329270362854 | Val. Loss: 0.2622419595718384\n",
            "Epoch: 130 | Loss: 0.21288341283798218 | Val. Loss: 0.2630388140678406\n",
            "Epoch: 131 | Loss: 0.1741916835308075 | Val. Loss: 0.2615852952003479\n",
            "Epoch: 132 | Loss: 0.2210204303264618 | Val. Loss: 0.2607811391353607\n",
            "Epoch: 133 | Loss: 0.25625163316726685 | Val. Loss: 0.26147571206092834\n",
            "Epoch: 134 | Loss: 0.23196199536323547 | Val. Loss: 0.2593945860862732\n",
            "Epoch: 135 | Loss: 0.21869924664497375 | Val. Loss: 0.25885963439941406\n",
            "Epoch: 136 | Loss: 0.2913641929626465 | Val. Loss: 0.2588178217411041\n",
            "Epoch: 137 | Loss: 0.17485462129116058 | Val. Loss: 0.2590305209159851\n",
            "Epoch: 138 | Loss: 0.2508150637149811 | Val. Loss: 0.25767529010772705\n",
            "Epoch: 139 | Loss: 0.0841839388012886 | Val. Loss: 0.2574284076690674\n",
            "Epoch: 140 | Loss: 0.16761183738708496 | Val. Loss: 0.25698819756507874\n",
            "Epoch: 141 | Loss: 0.18259340524673462 | Val. Loss: 0.25640103220939636\n",
            "Epoch: 142 | Loss: 0.14869266748428345 | Val. Loss: 0.25503313541412354\n",
            "Epoch: 143 | Loss: 0.30070558190345764 | Val. Loss: 0.2538187503814697\n",
            "Epoch: 144 | Loss: 0.24121646583080292 | Val. Loss: 0.2529396712779999\n",
            "Epoch: 145 | Loss: 0.2096758335828781 | Val. Loss: 0.25206121802330017\n",
            "Epoch: 146 | Loss: 0.17874188721179962 | Val. Loss: 0.2503637373447418\n",
            "Epoch: 147 | Loss: 0.3379681706428528 | Val. Loss: 0.24995383620262146\n",
            "Epoch: 148 | Loss: 0.24649113416671753 | Val. Loss: 0.24846389889717102\n",
            "Epoch: 149 | Loss: 0.2015940248966217 | Val. Loss: 0.24817581474781036\n",
            "Epoch: 150 | Loss: 0.1736774742603302 | Val. Loss: 0.247715026140213\n",
            "Epoch: 151 | Loss: 0.26984894275665283 | Val. Loss: 0.2463875561952591\n",
            "Epoch: 152 | Loss: 0.2578237056732178 | Val. Loss: 0.24553874135017395\n",
            "Epoch: 153 | Loss: 0.16593541204929352 | Val. Loss: 0.24352934956550598\n",
            "Epoch: 154 | Loss: 0.20203472673892975 | Val. Loss: 0.24216100573539734\n",
            "Epoch: 155 | Loss: 0.17913411557674408 | Val. Loss: 0.24253647029399872\n",
            "Epoch: 156 | Loss: 0.19820669293403625 | Val. Loss: 0.24130794405937195\n",
            "Epoch: 157 | Loss: 0.12046707421541214 | Val. Loss: 0.24119707942008972\n",
            "Epoch: 158 | Loss: 0.27705997228622437 | Val. Loss: 0.24088747799396515\n",
            "Epoch: 159 | Loss: 0.11452332139015198 | Val. Loss: 0.23911865055561066\n",
            "Epoch: 160 | Loss: 0.17205196619033813 | Val. Loss: 0.23866057395935059\n",
            "Epoch: 161 | Loss: 0.19174979627132416 | Val. Loss: 0.23915226757526398\n",
            "Epoch: 162 | Loss: 0.223672017455101 | Val. Loss: 0.2377769500017166\n",
            "Epoch: 163 | Loss: 0.12132715433835983 | Val. Loss: 0.23665666580200195\n",
            "Epoch: 164 | Loss: 0.16506484150886536 | Val. Loss: 0.23596075177192688\n",
            "Epoch: 165 | Loss: 0.24141791462898254 | Val. Loss: 0.23638370633125305\n",
            "Epoch: 166 | Loss: 0.2681375741958618 | Val. Loss: 0.23646649718284607\n",
            "Epoch: 167 | Loss: 0.22169482707977295 | Val. Loss: 0.23553964495658875\n",
            "Epoch: 168 | Loss: 0.16878089308738708 | Val. Loss: 0.2347751408815384\n",
            "Epoch: 169 | Loss: 0.2302258163690567 | Val. Loss: 0.23614458739757538\n",
            "Epoch: 170 | Loss: 0.2257227748632431 | Val. Loss: 0.233864426612854\n",
            "Epoch: 171 | Loss: 0.20698170363903046 | Val. Loss: 0.23300035297870636\n",
            "Epoch: 172 | Loss: 0.11135563999414444 | Val. Loss: 0.23145589232444763\n",
            "Epoch: 173 | Loss: 0.13187043368816376 | Val. Loss: 0.2318006008863449\n",
            "Epoch: 174 | Loss: 0.23434053361415863 | Val. Loss: 0.23262211680412292\n",
            "Epoch: 175 | Loss: 0.17363698780536652 | Val. Loss: 0.2314811497926712\n",
            "Epoch: 176 | Loss: 0.20959779620170593 | Val. Loss: 0.23064756393432617\n",
            "Epoch: 177 | Loss: 0.1275809407234192 | Val. Loss: 0.22992220520973206\n",
            "Epoch: 178 | Loss: 0.18080967664718628 | Val. Loss: 0.2272137701511383\n",
            "Epoch: 179 | Loss: 0.13187859952449799 | Val. Loss: 0.2270313799381256\n",
            "Epoch: 180 | Loss: 0.21877239644527435 | Val. Loss: 0.2270713597536087\n",
            "Epoch: 181 | Loss: 0.16414874792099 | Val. Loss: 0.22632448375225067\n",
            "Epoch: 182 | Loss: 0.28585898876190186 | Val. Loss: 0.22584319114685059\n",
            "Epoch: 183 | Loss: 0.20465189218521118 | Val. Loss: 0.22343599796295166\n",
            "Epoch: 184 | Loss: 0.18175289034843445 | Val. Loss: 0.22320277988910675\n",
            "Epoch: 185 | Loss: 0.16635830700397491 | Val. Loss: 0.2236299216747284\n",
            "Epoch: 186 | Loss: 0.23839624226093292 | Val. Loss: 0.22177846729755402\n",
            "Epoch: 187 | Loss: 0.11971316486597061 | Val. Loss: 0.21999363601207733\n",
            "Epoch: 188 | Loss: 0.21957433223724365 | Val. Loss: 0.22040729224681854\n",
            "Epoch: 189 | Loss: 0.14609858393669128 | Val. Loss: 0.22008410096168518\n",
            "Epoch: 190 | Loss: 0.1450575590133667 | Val. Loss: 0.21886387467384338\n",
            "Epoch: 191 | Loss: 0.1408396065235138 | Val. Loss: 0.21956650912761688\n",
            "Epoch: 192 | Loss: 0.21715518832206726 | Val. Loss: 0.21764394640922546\n",
            "Epoch: 193 | Loss: 0.08524749428033829 | Val. Loss: 0.2177232950925827\n",
            "Epoch: 194 | Loss: 0.19250114262104034 | Val. Loss: 0.21791236102581024\n",
            "Epoch: 195 | Loss: 0.20212261378765106 | Val. Loss: 0.21547344326972961\n",
            "Epoch: 196 | Loss: 0.2138928920030594 | Val. Loss: 0.21835513412952423\n",
            "Epoch: 197 | Loss: 0.1893901228904724 | Val. Loss: 0.21530184149742126\n",
            "Epoch: 198 | Loss: 0.1922009289264679 | Val. Loss: 0.2144927680492401\n",
            "Epoch: 199 | Loss: 0.14287319779396057 | Val. Loss: 0.2128656804561615\n",
            "Epoch: 200 | Loss: 0.19065549969673157 | Val. Loss: 0.21275794506072998\n",
            "Epoch: 201 | Loss: 0.21224012970924377 | Val. Loss: 0.212908074259758\n",
            "Epoch: 202 | Loss: 0.11906608194112778 | Val. Loss: 0.21296551823616028\n",
            "Epoch: 203 | Loss: 0.14589273929595947 | Val. Loss: 0.2131587564945221\n",
            "Epoch: 204 | Loss: 0.15990930795669556 | Val. Loss: 0.2099677324295044\n",
            "Epoch: 205 | Loss: 0.18841199576854706 | Val. Loss: 0.21029773354530334\n",
            "Epoch: 206 | Loss: 0.21007534861564636 | Val. Loss: 0.209781676530838\n",
            "Epoch: 207 | Loss: 0.16178655624389648 | Val. Loss: 0.2107570916414261\n",
            "Epoch: 208 | Loss: 0.15905533730983734 | Val. Loss: 0.20850029587745667\n",
            "Epoch: 209 | Loss: 0.12232451885938644 | Val. Loss: 0.20753172039985657\n",
            "Epoch: 210 | Loss: 0.24043560028076172 | Val. Loss: 0.20904052257537842\n",
            "Epoch: 211 | Loss: 0.17178001999855042 | Val. Loss: 0.20817551016807556\n",
            "Epoch: 212 | Loss: 0.10727744549512863 | Val. Loss: 0.20673151314258575\n",
            "Epoch: 213 | Loss: 0.15544894337654114 | Val. Loss: 0.20675165951251984\n",
            "Epoch: 214 | Loss: 0.1319504827260971 | Val. Loss: 0.20675750076770782\n",
            "Epoch: 215 | Loss: 0.15550819039344788 | Val. Loss: 0.20773212611675262\n",
            "Epoch: 216 | Loss: 0.11339166015386581 | Val. Loss: 0.20555444061756134\n",
            "Epoch: 217 | Loss: 0.16181063652038574 | Val. Loss: 0.20536324381828308\n",
            "Epoch: 218 | Loss: 0.10662597417831421 | Val. Loss: 0.20681485533714294\n",
            "Epoch: 219 | Loss: 0.12807047367095947 | Val. Loss: 0.20534542202949524\n",
            "Epoch: 220 | Loss: 0.07811927050352097 | Val. Loss: 0.2055932730436325\n",
            "Epoch: 221 | Loss: 0.1677604764699936 | Val. Loss: 0.2042141705751419\n",
            "Epoch: 222 | Loss: 0.13996325433254242 | Val. Loss: 0.20481984317302704\n",
            "Epoch: 223 | Loss: 0.22111257910728455 | Val. Loss: 0.20491620898246765\n",
            "Epoch: 224 | Loss: 0.08227112889289856 | Val. Loss: 0.20311398804187775\n",
            "Epoch: 225 | Loss: 0.09444089233875275 | Val. Loss: 0.20195722579956055\n",
            "Epoch: 226 | Loss: 0.18726782500743866 | Val. Loss: 0.20454120635986328\n",
            "Epoch: 227 | Loss: 0.09694058448076248 | Val. Loss: 0.20391269028186798\n",
            "Epoch: 228 | Loss: 0.1720624715089798 | Val. Loss: 0.2030302733182907\n",
            "Epoch: 229 | Loss: 0.1324152946472168 | Val. Loss: 0.20181214809417725\n",
            "Epoch: 230 | Loss: 0.15261250734329224 | Val. Loss: 0.2030775249004364\n",
            "Epoch: 231 | Loss: 0.14405696094036102 | Val. Loss: 0.2023114413022995\n",
            "Epoch: 232 | Loss: 0.19934771955013275 | Val. Loss: 0.20140008628368378\n",
            "Epoch: 233 | Loss: 0.18120989203453064 | Val. Loss: 0.2017274796962738\n",
            "Epoch: 234 | Loss: 0.14689919352531433 | Val. Loss: 0.1995817869901657\n",
            "Epoch: 235 | Loss: 0.18619300425052643 | Val. Loss: 0.2022945135831833\n",
            "Epoch: 236 | Loss: 0.14923706650733948 | Val. Loss: 0.2001270055770874\n",
            "Epoch: 237 | Loss: 0.15247921645641327 | Val. Loss: 0.2006121426820755\n",
            "Epoch: 238 | Loss: 0.24972572922706604 | Val. Loss: 0.20017056167125702\n",
            "Epoch: 239 | Loss: 0.13271601498126984 | Val. Loss: 0.20038354396820068\n",
            "Epoch: 240 | Loss: 0.10551540553569794 | Val. Loss: 0.20108863711357117\n",
            "Epoch: 241 | Loss: 0.2162872552871704 | Val. Loss: 0.20006708800792694\n",
            "Epoch: 242 | Loss: 0.11302608996629715 | Val. Loss: 0.20041462779045105\n",
            "Epoch: 243 | Loss: 0.12261715531349182 | Val. Loss: 0.19923701882362366\n",
            "Epoch: 244 | Loss: 0.13359493017196655 | Val. Loss: 0.19941076636314392\n",
            "Epoch: 245 | Loss: 0.10880374163389206 | Val. Loss: 0.19899903237819672\n",
            "Epoch: 246 | Loss: 0.2406555414199829 | Val. Loss: 0.20025376975536346\n",
            "Epoch: 247 | Loss: 0.14793595671653748 | Val. Loss: 0.19885055720806122\n",
            "Epoch: 248 | Loss: 0.18964563310146332 | Val. Loss: 0.19868478178977966\n",
            "Epoch: 249 | Loss: 0.17957818508148193 | Val. Loss: 0.19895170629024506\n",
            "Epoch: 250 | Loss: 0.19069194793701172 | Val. Loss: 0.1987949162721634\n",
            "Epoch: 251 | Loss: 0.1271335482597351 | Val. Loss: 0.19944725930690765\n",
            "Epoch: 252 | Loss: 0.1425103396177292 | Val. Loss: 0.19867028295993805\n",
            "Epoch: 253 | Loss: 0.1857864260673523 | Val. Loss: 0.19905515015125275\n",
            "Epoch: 254 | Loss: 0.20671868324279785 | Val. Loss: 0.1984764039516449\n",
            "Epoch: 255 | Loss: 0.1052248477935791 | Val. Loss: 0.19811676442623138\n",
            "Epoch: 256 | Loss: 0.1537925750017166 | Val. Loss: 0.19733276963233948\n",
            "Epoch: 257 | Loss: 0.10088270157575607 | Val. Loss: 0.197869673371315\n",
            "Epoch: 258 | Loss: 0.15043650567531586 | Val. Loss: 0.1961374431848526\n",
            "Epoch: 259 | Loss: 0.18102487921714783 | Val. Loss: 0.19912955164909363\n",
            "Epoch: 260 | Loss: 0.19106756150722504 | Val. Loss: 0.19860334694385529\n",
            "Epoch: 261 | Loss: 0.11369328200817108 | Val. Loss: 0.19714947044849396\n",
            "Epoch: 262 | Loss: 0.18284744024276733 | Val. Loss: 0.19615884125232697\n",
            "Epoch: 263 | Loss: 0.14390258491039276 | Val. Loss: 0.19610755145549774\n",
            "Epoch: 264 | Loss: 0.09937340021133423 | Val. Loss: 0.196923166513443\n",
            "Epoch: 265 | Loss: 0.10473974049091339 | Val. Loss: 0.19581526517868042\n",
            "Epoch: 266 | Loss: 0.10335507988929749 | Val. Loss: 0.1948406845331192\n",
            "Epoch: 267 | Loss: 0.1229935958981514 | Val. Loss: 0.19346974790096283\n",
            "Epoch: 268 | Loss: 0.10948240011930466 | Val. Loss: 0.19388556480407715\n",
            "Epoch: 269 | Loss: 0.06764402985572815 | Val. Loss: 0.19324499368667603\n",
            "Epoch: 270 | Loss: 0.10977321863174438 | Val. Loss: 0.19222229719161987\n",
            "Epoch: 271 | Loss: 0.19649234414100647 | Val. Loss: 0.19266340136528015\n",
            "Epoch: 272 | Loss: 0.09081552922725677 | Val. Loss: 0.1919010579586029\n",
            "Epoch: 273 | Loss: 0.10674560815095901 | Val. Loss: 0.19033695757389069\n",
            "Epoch: 274 | Loss: 0.07888657599687576 | Val. Loss: 0.19167521595954895\n",
            "Epoch: 275 | Loss: 0.06489939987659454 | Val. Loss: 0.19009310007095337\n",
            "Epoch: 276 | Loss: 0.10813282430171967 | Val. Loss: 0.19068221747875214\n",
            "Epoch: 277 | Loss: 0.09306041151285172 | Val. Loss: 0.1902419924736023\n",
            "Epoch: 278 | Loss: 0.12938837707042694 | Val. Loss: 0.18858009576797485\n",
            "Epoch: 279 | Loss: 0.11650236696004868 | Val. Loss: 0.18836241960525513\n",
            "Epoch: 280 | Loss: 0.09172286093235016 | Val. Loss: 0.18883012235164642\n",
            "Epoch: 281 | Loss: 0.04816141352057457 | Val. Loss: 0.18686352670192719\n",
            "Epoch: 282 | Loss: 0.1815236210823059 | Val. Loss: 0.18843317031860352\n",
            "Epoch: 283 | Loss: 0.10538680106401443 | Val. Loss: 0.18778541684150696\n",
            "Epoch: 284 | Loss: 0.11218780279159546 | Val. Loss: 0.18672747910022736\n",
            "Epoch: 285 | Loss: 0.1706356257200241 | Val. Loss: 0.1871155947446823\n",
            "Epoch: 286 | Loss: 0.11568969488143921 | Val. Loss: 0.1871282309293747\n",
            "Epoch: 287 | Loss: 0.09054852277040482 | Val. Loss: 0.1852836012840271\n",
            "Epoch: 288 | Loss: 0.17840993404388428 | Val. Loss: 0.18621167540550232\n",
            "Epoch: 289 | Loss: 0.20965538918972015 | Val. Loss: 0.1867983043193817\n",
            "Epoch: 290 | Loss: 0.1706177443265915 | Val. Loss: 0.18478520214557648\n",
            "Epoch: 291 | Loss: 0.10341346263885498 | Val. Loss: 0.1850157231092453\n",
            "Epoch: 292 | Loss: 0.15028874576091766 | Val. Loss: 0.18647514283657074\n",
            "Epoch: 293 | Loss: 0.1334041804075241 | Val. Loss: 0.18482734262943268\n",
            "Epoch: 294 | Loss: 0.1332341432571411 | Val. Loss: 0.18381048738956451\n",
            "Epoch: 295 | Loss: 0.13597528636455536 | Val. Loss: 0.18569926917552948\n",
            "Epoch: 296 | Loss: 0.20359794795513153 | Val. Loss: 0.1842164695262909\n",
            "Epoch: 297 | Loss: 0.23042277991771698 | Val. Loss: 0.18366551399230957\n",
            "Epoch: 298 | Loss: 0.0737716555595398 | Val. Loss: 0.1841227114200592\n",
            "Epoch: 299 | Loss: 0.14302802085876465 | Val. Loss: 0.18449929356575012\n",
            "Epoch: 300 | Loss: 0.05726514384150505 | Val. Loss: 0.18231940269470215\n",
            "Epoch: 301 | Loss: 0.2054678052663803 | Val. Loss: 0.18373677134513855\n",
            "Epoch: 302 | Loss: 0.13484981656074524 | Val. Loss: 0.18399009108543396\n",
            "Epoch: 303 | Loss: 0.13625527918338776 | Val. Loss: 0.1821669638156891\n",
            "Epoch: 304 | Loss: 0.0893479660153389 | Val. Loss: 0.18132904171943665\n",
            "Epoch: 305 | Loss: 0.09047487378120422 | Val. Loss: 0.18297387659549713\n",
            "Epoch: 306 | Loss: 0.14133292436599731 | Val. Loss: 0.18158948421478271\n",
            "Epoch: 307 | Loss: 0.10884256660938263 | Val. Loss: 0.1823141723871231\n",
            "Epoch: 308 | Loss: 0.06063975393772125 | Val. Loss: 0.18011118471622467\n",
            "Epoch: 309 | Loss: 0.14070874452590942 | Val. Loss: 0.1805444061756134\n",
            "Epoch: 310 | Loss: 0.12416154891252518 | Val. Loss: 0.18153947591781616\n",
            "Epoch: 311 | Loss: 0.14327329397201538 | Val. Loss: 0.17991337180137634\n",
            "Epoch: 312 | Loss: 0.15233618021011353 | Val. Loss: 0.18069922924041748\n",
            "Epoch: 313 | Loss: 0.0892096608877182 | Val. Loss: 0.18145640194416046\n",
            "Epoch: 314 | Loss: 0.1143471896648407 | Val. Loss: 0.1797705739736557\n",
            "Epoch: 315 | Loss: 0.08719857037067413 | Val. Loss: 0.18128259479999542\n",
            "Epoch: 316 | Loss: 0.148554727435112 | Val. Loss: 0.17978180944919586\n",
            "Epoch: 317 | Loss: 0.0676814466714859 | Val. Loss: 0.1789645105600357\n",
            "Epoch: 318 | Loss: 0.1291479468345642 | Val. Loss: 0.1798296719789505\n",
            "Epoch: 319 | Loss: 0.11779624223709106 | Val. Loss: 0.17875966429710388\n",
            "Epoch: 320 | Loss: 0.16969263553619385 | Val. Loss: 0.17949780821800232\n",
            "Epoch: 321 | Loss: 0.12396824359893799 | Val. Loss: 0.17857854068279266\n",
            "Epoch: 322 | Loss: 0.14064645767211914 | Val. Loss: 0.17740127444267273\n",
            "Epoch: 323 | Loss: 0.11761326342821121 | Val. Loss: 0.17873327434062958\n",
            "Epoch: 324 | Loss: 0.154514342546463 | Val. Loss: 0.17664742469787598\n",
            "Epoch: 325 | Loss: 0.09097941219806671 | Val. Loss: 0.1793949156999588\n",
            "Epoch: 326 | Loss: 0.06905173510313034 | Val. Loss: 0.17695540189743042\n",
            "Epoch: 327 | Loss: 0.13946029543876648 | Val. Loss: 0.17812564969062805\n",
            "Epoch: 328 | Loss: 0.2027476280927658 | Val. Loss: 0.17806674540042877\n",
            "Epoch: 329 | Loss: 0.08749756217002869 | Val. Loss: 0.17765264213085175\n",
            "Epoch: 330 | Loss: 0.11919349431991577 | Val. Loss: 0.17718225717544556\n",
            "Epoch: 331 | Loss: 0.07963398098945618 | Val. Loss: 0.1761416792869568\n",
            "Epoch: 332 | Loss: 0.09501703083515167 | Val. Loss: 0.17529895901679993\n",
            "Epoch: 333 | Loss: 0.15511761605739594 | Val. Loss: 0.17704102396965027\n",
            "Epoch: 334 | Loss: 0.11295390129089355 | Val. Loss: 0.1769055277109146\n",
            "Epoch: 335 | Loss: 0.06305468082427979 | Val. Loss: 0.17606563866138458\n",
            "Epoch: 336 | Loss: 0.07732803374528885 | Val. Loss: 0.17583037912845612\n",
            "Epoch: 337 | Loss: 0.09024293720722198 | Val. Loss: 0.17557239532470703\n",
            "Epoch: 338 | Loss: 0.07601013034582138 | Val. Loss: 0.17521151900291443\n",
            "Epoch: 339 | Loss: 0.0660189837217331 | Val. Loss: 0.17558668553829193\n",
            "Epoch: 340 | Loss: 0.15011614561080933 | Val. Loss: 0.17532488703727722\n",
            "Epoch: 341 | Loss: 0.08481371402740479 | Val. Loss: 0.17479759454727173\n",
            "Epoch: 342 | Loss: 0.18989653885364532 | Val. Loss: 0.17503276467323303\n",
            "Epoch: 343 | Loss: 0.1383136659860611 | Val. Loss: 0.1739925593137741\n",
            "Epoch: 344 | Loss: 0.08298177272081375 | Val. Loss: 0.1745944321155548\n",
            "Epoch: 345 | Loss: 0.1125081405043602 | Val. Loss: 0.17369364202022552\n",
            "Epoch: 346 | Loss: 0.14187128841876984 | Val. Loss: 0.17464810609817505\n",
            "Epoch: 347 | Loss: 0.0963413417339325 | Val. Loss: 0.1730274260044098\n",
            "Epoch: 348 | Loss: 0.11921417713165283 | Val. Loss: 0.17271727323532104\n",
            "Epoch: 349 | Loss: 0.1222899854183197 | Val. Loss: 0.17276620864868164\n",
            "Epoch: 350 | Loss: 0.12667633593082428 | Val. Loss: 0.17261864244937897\n",
            "Epoch: 351 | Loss: 0.10248655080795288 | Val. Loss: 0.17421655356884003\n",
            "Epoch: 352 | Loss: 0.08242886513471603 | Val. Loss: 0.17322152853012085\n",
            "Epoch: 353 | Loss: 0.11207379400730133 | Val. Loss: 0.17095047235488892\n",
            "Epoch: 354 | Loss: 0.1109759733080864 | Val. Loss: 0.17182263731956482\n",
            "Epoch: 355 | Loss: 0.12355415523052216 | Val. Loss: 0.17235076427459717\n",
            "Epoch: 356 | Loss: 0.08285122364759445 | Val. Loss: 0.1705596148967743\n",
            "Epoch: 357 | Loss: 0.12367311120033264 | Val. Loss: 0.17263665795326233\n",
            "Epoch: 358 | Loss: 0.16605816781520844 | Val. Loss: 0.1695912927389145\n",
            "Epoch: 359 | Loss: 0.08314631879329681 | Val. Loss: 0.1684497892856598\n",
            "Epoch: 360 | Loss: 0.16306166350841522 | Val. Loss: 0.17023447155952454\n",
            "Epoch: 361 | Loss: 0.09322476387023926 | Val. Loss: 0.16836535930633545\n",
            "Epoch: 362 | Loss: 0.088824063539505 | Val. Loss: 0.16796977818012238\n",
            "Epoch: 363 | Loss: 0.10104572772979736 | Val. Loss: 0.16850139200687408\n",
            "Epoch: 364 | Loss: 0.0702182874083519 | Val. Loss: 0.16964024305343628\n",
            "Epoch: 365 | Loss: 0.11318090558052063 | Val. Loss: 0.16768893599510193\n",
            "Epoch: 366 | Loss: 0.07985251396894455 | Val. Loss: 0.16727885603904724\n",
            "Epoch: 367 | Loss: 0.14931721985340118 | Val. Loss: 0.16578646004199982\n",
            "Epoch: 368 | Loss: 0.11392196267843246 | Val. Loss: 0.16658847033977509\n",
            "Epoch: 369 | Loss: 0.10872262716293335 | Val. Loss: 0.167580246925354\n",
            "Epoch: 370 | Loss: 0.05310538038611412 | Val. Loss: 0.1659201830625534\n",
            "Epoch: 371 | Loss: 0.06888601928949356 | Val. Loss: 0.1649583876132965\n",
            "Epoch: 372 | Loss: 0.07378413528203964 | Val. Loss: 0.1646539866924286\n",
            "Epoch: 373 | Loss: 0.10767261683940887 | Val. Loss: 0.16689050197601318\n",
            "Epoch: 374 | Loss: 0.15815132856369019 | Val. Loss: 0.16345837712287903\n",
            "Epoch: 375 | Loss: 0.14456620812416077 | Val. Loss: 0.16477249562740326\n",
            "Epoch: 376 | Loss: 0.07800232619047165 | Val. Loss: 0.1629781723022461\n",
            "Epoch: 377 | Loss: 0.15646380186080933 | Val. Loss: 0.1658456027507782\n",
            "Epoch: 378 | Loss: 0.1008475050330162 | Val. Loss: 0.1628817617893219\n",
            "Epoch: 379 | Loss: 0.16755133867263794 | Val. Loss: 0.16496017575263977\n",
            "Epoch: 380 | Loss: 0.10424740612506866 | Val. Loss: 0.162904292345047\n",
            "Epoch: 381 | Loss: 0.05581465736031532 | Val. Loss: 0.16385337710380554\n",
            "Epoch: 382 | Loss: 0.029430676251649857 | Val. Loss: 0.16209545731544495\n",
            "Epoch: 383 | Loss: 0.07723435014486313 | Val. Loss: 0.1616699993610382\n",
            "Epoch: 384 | Loss: 0.11596158891916275 | Val. Loss: 0.16316047310829163\n",
            "Epoch: 385 | Loss: 0.05272934213280678 | Val. Loss: 0.16128341853618622\n",
            "Epoch: 386 | Loss: 0.10389788448810577 | Val. Loss: 0.16141337156295776\n",
            "Epoch: 387 | Loss: 0.10145937651395798 | Val. Loss: 0.16033045947551727\n",
            "Epoch: 388 | Loss: 0.12256420403718948 | Val. Loss: 0.16430822014808655\n",
            "Epoch: 389 | Loss: 0.11084741353988647 | Val. Loss: 0.16067594289779663\n",
            "Epoch: 390 | Loss: 0.10133855044841766 | Val. Loss: 0.16107772290706635\n",
            "Epoch: 391 | Loss: 0.12773026525974274 | Val. Loss: 0.16053375601768494\n",
            "Epoch: 392 | Loss: 0.05470356345176697 | Val. Loss: 0.16101570427417755\n",
            "Epoch: 393 | Loss: 0.060813672840595245 | Val. Loss: 0.1629619151353836\n",
            "Epoch: 394 | Loss: 0.07407203316688538 | Val. Loss: 0.16099640727043152\n",
            "Epoch: 395 | Loss: 0.10946828126907349 | Val. Loss: 0.15977215766906738\n",
            "Epoch: 396 | Loss: 0.05519021674990654 | Val. Loss: 0.16170687973499298\n",
            "Epoch: 397 | Loss: 0.17964039742946625 | Val. Loss: 0.16295015811920166\n",
            "Epoch: 398 | Loss: 0.07807370275259018 | Val. Loss: 0.16091331839561462\n",
            "Epoch: 399 | Loss: 0.04135835915803909 | Val. Loss: 0.15953701734542847\n",
            "Epoch: 400 | Loss: 0.10236841440200806 | Val. Loss: 0.16184255480766296\n",
            "Epoch: 401 | Loss: 0.03581777587532997 | Val. Loss: 0.161010280251503\n",
            "Epoch: 402 | Loss: 0.05995265021920204 | Val. Loss: 0.16116690635681152\n",
            "Epoch: 403 | Loss: 0.08104553818702698 | Val. Loss: 0.16037629544734955\n",
            "Epoch: 404 | Loss: 0.10766340792179108 | Val. Loss: 0.15998293459415436\n",
            "Epoch: 405 | Loss: 0.0631384402513504 | Val. Loss: 0.1608414351940155\n",
            "Epoch: 406 | Loss: 0.05490735545754433 | Val. Loss: 0.16017849743366241\n",
            "Epoch: 407 | Loss: 0.09297313541173935 | Val. Loss: 0.16092556715011597\n",
            "Epoch: 408 | Loss: 0.11524155735969543 | Val. Loss: 0.16154108941555023\n",
            "Epoch: 409 | Loss: 0.10572757571935654 | Val. Loss: 0.1604808270931244\n",
            "Epoch: 410 | Loss: 0.06826052814722061 | Val. Loss: 0.15957719087600708\n",
            "Epoch: 411 | Loss: 0.1639977991580963 | Val. Loss: 0.16096405684947968\n",
            "Epoch: 412 | Loss: 0.09243848919868469 | Val. Loss: 0.159464493393898\n",
            "Epoch: 413 | Loss: 0.14400790631771088 | Val. Loss: 0.15987014770507812\n",
            "Epoch: 414 | Loss: 0.1011284664273262 | Val. Loss: 0.15853866934776306\n",
            "Epoch: 415 | Loss: 0.08522821962833405 | Val. Loss: 0.15888434648513794\n",
            "Epoch: 416 | Loss: 0.03341270238161087 | Val. Loss: 0.1597088873386383\n",
            "Epoch: 417 | Loss: 0.04800223559141159 | Val. Loss: 0.15864500403404236\n",
            "Epoch: 418 | Loss: 0.11612661927938461 | Val. Loss: 0.15963569283485413\n",
            "Epoch: 419 | Loss: 0.2127041071653366 | Val. Loss: 0.15901552140712738\n",
            "Epoch: 420 | Loss: 0.10211561620235443 | Val. Loss: 0.15725116431713104\n",
            "Epoch: 421 | Loss: 0.04644676297903061 | Val. Loss: 0.15770769119262695\n",
            "Epoch: 422 | Loss: 0.04459354653954506 | Val. Loss: 0.15744605660438538\n",
            "Epoch: 423 | Loss: 0.057903800159692764 | Val. Loss: 0.158771350979805\n",
            "Epoch: 424 | Loss: 0.07507225126028061 | Val. Loss: 0.15911345183849335\n",
            "Epoch: 425 | Loss: 0.06833911687135696 | Val. Loss: 0.15633977949619293\n",
            "Epoch: 426 | Loss: 0.06590713560581207 | Val. Loss: 0.1587924361228943\n",
            "Epoch: 427 | Loss: 0.2012508660554886 | Val. Loss: 0.15836966037750244\n",
            "Epoch: 428 | Loss: 0.0711241215467453 | Val. Loss: 0.1569116711616516\n",
            "Epoch: 429 | Loss: 0.09633426368236542 | Val. Loss: 0.15596379339694977\n",
            "Epoch: 430 | Loss: 0.08684393018484116 | Val. Loss: 0.1560867428779602\n",
            "Epoch: 431 | Loss: 0.09967902302742004 | Val. Loss: 0.15523210167884827\n",
            "Epoch: 432 | Loss: 0.07196052372455597 | Val. Loss: 0.158149853348732\n",
            "Epoch: 433 | Loss: 0.13837113976478577 | Val. Loss: 0.15569710731506348\n",
            "Epoch: 434 | Loss: 0.04184587299823761 | Val. Loss: 0.1548357605934143\n",
            "Epoch: 435 | Loss: 0.14437034726142883 | Val. Loss: 0.15613892674446106\n",
            "Epoch: 436 | Loss: 0.06275267153978348 | Val. Loss: 0.15599951148033142\n",
            "Epoch: 437 | Loss: 0.04722646251320839 | Val. Loss: 0.15531736612319946\n",
            "Epoch: 438 | Loss: 0.057296719402074814 | Val. Loss: 0.1543031632900238\n",
            "Epoch: 439 | Loss: 0.08884947746992111 | Val. Loss: 0.15377236902713776\n",
            "Epoch: 440 | Loss: 0.08906814455986023 | Val. Loss: 0.1556985080242157\n",
            "Epoch: 441 | Loss: 0.06101081892848015 | Val. Loss: 0.15416577458381653\n",
            "Epoch: 442 | Loss: 0.04734889045357704 | Val. Loss: 0.15397359430789948\n",
            "Epoch: 443 | Loss: 0.08065561205148697 | Val. Loss: 0.15410804748535156\n",
            "Epoch: 444 | Loss: 0.10226183384656906 | Val. Loss: 0.15390583872795105\n",
            "Epoch: 445 | Loss: 0.05693934112787247 | Val. Loss: 0.15482783317565918\n",
            "Epoch: 446 | Loss: 0.06301221251487732 | Val. Loss: 0.15264567732810974\n",
            "Epoch: 447 | Loss: 0.12963545322418213 | Val. Loss: 0.15394578874111176\n",
            "Epoch: 448 | Loss: 0.09564577043056488 | Val. Loss: 0.15345966815948486\n",
            "Epoch: 449 | Loss: 0.08716625720262527 | Val. Loss: 0.1520421952009201\n",
            "Epoch: 450 | Loss: 0.08960116654634476 | Val. Loss: 0.15243513882160187\n",
            "Epoch: 451 | Loss: 0.05938415974378586 | Val. Loss: 0.15318481624126434\n",
            "Epoch: 452 | Loss: 0.06820882856845856 | Val. Loss: 0.15148605406284332\n",
            "Epoch: 453 | Loss: 0.09322409331798553 | Val. Loss: 0.1493479460477829\n",
            "Epoch: 454 | Loss: 0.061185941100120544 | Val. Loss: 0.14920155704021454\n",
            "Epoch: 455 | Loss: 0.08017239719629288 | Val. Loss: 0.14600244164466858\n",
            "Epoch: 456 | Loss: 0.04453083500266075 | Val. Loss: 0.14965707063674927\n",
            "Epoch: 457 | Loss: 0.08830948173999786 | Val. Loss: 0.14888885617256165\n",
            "Epoch: 458 | Loss: 0.09635894745588303 | Val. Loss: 0.14435943961143494\n",
            "Epoch: 459 | Loss: 0.04089749976992607 | Val. Loss: 0.1477857530117035\n",
            "Epoch: 460 | Loss: 0.0908164456486702 | Val. Loss: 0.14760969579219818\n",
            "Epoch: 461 | Loss: 0.054661065340042114 | Val. Loss: 0.14436043798923492\n",
            "Epoch: 462 | Loss: 0.04837535694241524 | Val. Loss: 0.14545366168022156\n",
            "Epoch: 463 | Loss: 0.057856831699609756 | Val. Loss: 0.14336982369422913\n",
            "Epoch: 464 | Loss: 0.05568316578865051 | Val. Loss: 0.14230792224407196\n",
            "Epoch: 465 | Loss: 0.06678043305873871 | Val. Loss: 0.14593057334423065\n",
            "Epoch: 466 | Loss: 0.09218310564756393 | Val. Loss: 0.14131590723991394\n",
            "Epoch: 467 | Loss: 0.03621618449687958 | Val. Loss: 0.14233727753162384\n",
            "Epoch: 468 | Loss: 0.05923641845583916 | Val. Loss: 0.14110253751277924\n",
            "Epoch: 469 | Loss: 0.062472280114889145 | Val. Loss: 0.14202602207660675\n",
            "Epoch: 470 | Loss: 0.05455576255917549 | Val. Loss: 0.1405797153711319\n",
            "Epoch: 471 | Loss: 0.06765591353178024 | Val. Loss: 0.1391020566225052\n",
            "Epoch: 472 | Loss: 0.11016473174095154 | Val. Loss: 0.14045582711696625\n",
            "Epoch: 473 | Loss: 0.1037730798125267 | Val. Loss: 0.13969877362251282\n",
            "Epoch: 474 | Loss: 0.07001686841249466 | Val. Loss: 0.13853368163108826\n",
            "Epoch: 475 | Loss: 0.08094236999750137 | Val. Loss: 0.13941368460655212\n",
            "Epoch: 476 | Loss: 0.09394806623458862 | Val. Loss: 0.1369834691286087\n",
            "Epoch: 477 | Loss: 0.052531514316797256 | Val. Loss: 0.14068633317947388\n",
            "Epoch: 478 | Loss: 0.05256565660238266 | Val. Loss: 0.13881918787956238\n",
            "Epoch: 479 | Loss: 0.08009807020425797 | Val. Loss: 0.1374490112066269\n",
            "Epoch: 480 | Loss: 0.05952854081988335 | Val. Loss: 0.13716371357440948\n",
            "Epoch: 481 | Loss: 0.06555812805891037 | Val. Loss: 0.13769838213920593\n",
            "Epoch: 482 | Loss: 0.04430485516786575 | Val. Loss: 0.13447058200836182\n",
            "Epoch: 483 | Loss: 0.0824551060795784 | Val. Loss: 0.1354798972606659\n",
            "Epoch: 484 | Loss: 0.030384736135601997 | Val. Loss: 0.13300959765911102\n",
            "Epoch: 485 | Loss: 0.043010786175727844 | Val. Loss: 0.13360252976417542\n",
            "Epoch: 486 | Loss: 0.03317924588918686 | Val. Loss: 0.1325109750032425\n",
            "Epoch: 487 | Loss: 0.03540519252419472 | Val. Loss: 0.13194604218006134\n",
            "Epoch: 488 | Loss: 0.0359208844602108 | Val. Loss: 0.1320391148328781\n",
            "Epoch: 489 | Loss: 0.03249882161617279 | Val. Loss: 0.13321180641651154\n",
            "Epoch: 490 | Loss: 0.05218074843287468 | Val. Loss: 0.13121336698532104\n",
            "Epoch: 491 | Loss: 0.03786569461226463 | Val. Loss: 0.13171042501926422\n",
            "Epoch: 492 | Loss: 0.02199130691587925 | Val. Loss: 0.13216577470302582\n",
            "Epoch: 493 | Loss: 0.07745463401079178 | Val. Loss: 0.13147535920143127\n",
            "Epoch: 494 | Loss: 0.03245888277888298 | Val. Loss: 0.13210344314575195\n",
            "Epoch: 495 | Loss: 0.03624115139245987 | Val. Loss: 0.13002566993236542\n",
            "Epoch: 496 | Loss: 0.06273819506168365 | Val. Loss: 0.13071706891059875\n",
            "Epoch: 497 | Loss: 0.07165592163801193 | Val. Loss: 0.12992799282073975\n",
            "Epoch: 498 | Loss: 0.09107585996389389 | Val. Loss: 0.1300077587366104\n",
            "Epoch: 499 | Loss: 0.03942430764436722 | Val. Loss: 0.1306871771812439\n",
            "Epoch: 500 | Loss: 0.04154985025525093 | Val. Loss: 0.1294322907924652\n",
            "Epoch: 501 | Loss: 0.05186554417014122 | Val. Loss: 0.12943117320537567\n",
            "Epoch: 502 | Loss: 0.05003241077065468 | Val. Loss: 0.12869718670845032\n",
            "Epoch: 503 | Loss: 0.018260076642036438 | Val. Loss: 0.12887904047966003\n",
            "Epoch: 504 | Loss: 0.055192239582538605 | Val. Loss: 0.12848809361457825\n",
            "Epoch: 505 | Loss: 0.05351093038916588 | Val. Loss: 0.12924666702747345\n",
            "Epoch: 506 | Loss: 0.0898706465959549 | Val. Loss: 0.12859463691711426\n",
            "Epoch: 507 | Loss: 0.06233692541718483 | Val. Loss: 0.12837061285972595\n",
            "Epoch: 508 | Loss: 0.0808773934841156 | Val. Loss: 0.12742280960083008\n",
            "Epoch: 509 | Loss: 0.049886129796504974 | Val. Loss: 0.1276870220899582\n",
            "Epoch: 510 | Loss: 0.07897163182497025 | Val. Loss: 0.1281788945198059\n",
            "Epoch: 511 | Loss: 0.03688275068998337 | Val. Loss: 0.1268780529499054\n",
            "Epoch: 512 | Loss: 0.08424542844295502 | Val. Loss: 0.1275889277458191\n",
            "Epoch: 513 | Loss: 0.026600176468491554 | Val. Loss: 0.12635383009910583\n",
            "Epoch: 514 | Loss: 0.06277715414762497 | Val. Loss: 0.1262514293193817\n",
            "Epoch: 515 | Loss: 0.036216720938682556 | Val. Loss: 0.12596750259399414\n",
            "Epoch: 516 | Loss: 0.04819193482398987 | Val. Loss: 0.1257244050502777\n",
            "Epoch: 517 | Loss: 0.03315246105194092 | Val. Loss: 0.12640349566936493\n",
            "Epoch: 518 | Loss: 0.04340233653783798 | Val. Loss: 0.12548194825649261\n",
            "Epoch: 519 | Loss: 0.07424512505531311 | Val. Loss: 0.12589116394519806\n",
            "Epoch: 520 | Loss: 0.07001404464244843 | Val. Loss: 0.1250930279493332\n",
            "Epoch: 521 | Loss: 0.046800725162029266 | Val. Loss: 0.12541718780994415\n",
            "Epoch: 522 | Loss: 0.06343675404787064 | Val. Loss: 0.12462518364191055\n",
            "Epoch: 523 | Loss: 0.0372474268078804 | Val. Loss: 0.12442786991596222\n",
            "Epoch: 524 | Loss: 0.08164571225643158 | Val. Loss: 0.12372918426990509\n",
            "Epoch: 525 | Loss: 0.09492084383964539 | Val. Loss: 0.12510156631469727\n",
            "Epoch: 526 | Loss: 0.0674922987818718 | Val. Loss: 0.12450368702411652\n",
            "Epoch: 527 | Loss: 0.0173580814152956 | Val. Loss: 0.12362215667963028\n",
            "Epoch: 528 | Loss: 0.061482809484004974 | Val. Loss: 0.12368540465831757\n",
            "Epoch: 529 | Loss: 0.04649852216243744 | Val. Loss: 0.12335141003131866\n",
            "Epoch: 530 | Loss: 0.060543712228536606 | Val. Loss: 0.122194804251194\n",
            "Epoch: 531 | Loss: 0.05511021614074707 | Val. Loss: 0.12318017333745956\n",
            "Epoch: 532 | Loss: 0.08865167945623398 | Val. Loss: 0.12244872748851776\n",
            "Epoch: 533 | Loss: 0.05515698716044426 | Val. Loss: 0.12203850597143173\n",
            "Epoch: 534 | Loss: 0.008043212816119194 | Val. Loss: 0.12096013873815536\n",
            "Epoch: 535 | Loss: 0.040784697979688644 | Val. Loss: 0.12206603586673737\n",
            "Epoch: 536 | Loss: 0.036890190094709396 | Val. Loss: 0.12024630606174469\n",
            "Epoch: 537 | Loss: 0.023644370958209038 | Val. Loss: 0.12020057439804077\n",
            "Epoch: 538 | Loss: 0.03846212849020958 | Val. Loss: 0.12041016668081284\n",
            "Epoch: 539 | Loss: 0.038241028785705566 | Val. Loss: 0.12063474953174591\n",
            "Epoch: 540 | Loss: 0.036289650946855545 | Val. Loss: 0.12035304307937622\n",
            "Epoch: 541 | Loss: 0.05482408404350281 | Val. Loss: 0.11974585056304932\n",
            "Epoch: 542 | Loss: 0.06570489704608917 | Val. Loss: 0.11841660737991333\n",
            "Epoch: 543 | Loss: 0.03533204272389412 | Val. Loss: 0.11836175620555878\n",
            "Epoch: 544 | Loss: 0.03827902674674988 | Val. Loss: 0.11883256584405899\n",
            "Epoch: 545 | Loss: 0.03438446670770645 | Val. Loss: 0.11804156005382538\n",
            "Epoch: 546 | Loss: 0.05426028370857239 | Val. Loss: 0.11802732944488525\n",
            "Epoch: 547 | Loss: 0.054562199860811234 | Val. Loss: 0.11924207210540771\n",
            "Epoch: 548 | Loss: 0.038887109607458115 | Val. Loss: 0.11694276332855225\n",
            "Epoch: 549 | Loss: 0.04474122077226639 | Val. Loss: 0.11733697354793549\n",
            "Epoch: 550 | Loss: 0.021152453497052193 | Val. Loss: 0.11651837825775146\n",
            "Epoch: 551 | Loss: 0.0898166298866272 | Val. Loss: 0.11685105413198471\n",
            "Epoch: 552 | Loss: 0.12169711291790009 | Val. Loss: 0.11739145219326019\n",
            "Epoch: 553 | Loss: 0.0839400440454483 | Val. Loss: 0.11560510098934174\n",
            "Epoch: 554 | Loss: 0.0772198811173439 | Val. Loss: 0.11494235694408417\n",
            "Epoch: 555 | Loss: 0.027090394869446754 | Val. Loss: 0.11529538780450821\n",
            "Epoch: 556 | Loss: 0.0587521530687809 | Val. Loss: 0.11452623456716537\n",
            "Epoch: 557 | Loss: 0.02293645590543747 | Val. Loss: 0.11380585283041\n",
            "Epoch: 558 | Loss: 0.04990534484386444 | Val. Loss: 0.11341267824172974\n",
            "Epoch: 559 | Loss: 0.032821279019117355 | Val. Loss: 0.11310026794672012\n",
            "Epoch: 560 | Loss: 0.06348753720521927 | Val. Loss: 0.11403597891330719\n",
            "Epoch: 561 | Loss: 0.03599856421351433 | Val. Loss: 0.11447039991617203\n",
            "Epoch: 562 | Loss: 0.07228758931159973 | Val. Loss: 0.11362932622432709\n",
            "Epoch: 563 | Loss: 0.04393639788031578 | Val. Loss: 0.11320100724697113\n",
            "Epoch: 564 | Loss: 0.046525560319423676 | Val. Loss: 0.11305990070104599\n",
            "Epoch: 565 | Loss: 0.0726030021905899 | Val. Loss: 0.11251957714557648\n",
            "Epoch: 566 | Loss: 0.06482609361410141 | Val. Loss: 0.11239203065633774\n",
            "Epoch: 567 | Loss: 0.039256028831005096 | Val. Loss: 0.11231715977191925\n",
            "Epoch: 568 | Loss: 0.0358249731361866 | Val. Loss: 0.11219652742147446\n",
            "Epoch: 569 | Loss: 0.08419136703014374 | Val. Loss: 0.11207333952188492\n",
            "Epoch: 570 | Loss: 0.05452454090118408 | Val. Loss: 0.11097979545593262\n",
            "Epoch: 571 | Loss: 0.0637526586651802 | Val. Loss: 0.10992000252008438\n",
            "Epoch: 572 | Loss: 0.049815576523542404 | Val. Loss: 0.11039429903030396\n",
            "Epoch: 573 | Loss: 0.03695952892303467 | Val. Loss: 0.10922131687402725\n",
            "Epoch: 574 | Loss: 0.043852418661117554 | Val. Loss: 0.10836680978536606\n",
            "Epoch: 575 | Loss: 0.04633382707834244 | Val. Loss: 0.10858289897441864\n",
            "Epoch: 576 | Loss: 0.021113796159625053 | Val. Loss: 0.1082574874162674\n",
            "Epoch: 577 | Loss: 0.03192738816142082 | Val. Loss: 0.1081179603934288\n",
            "Epoch: 578 | Loss: 0.05614662915468216 | Val. Loss: 0.1085410863161087\n",
            "Epoch: 579 | Loss: 0.025086799636483192 | Val. Loss: 0.10876965522766113\n",
            "Epoch: 580 | Loss: 0.06207749992609024 | Val. Loss: 0.1083429828286171\n",
            "Epoch: 581 | Loss: 0.0643400028347969 | Val. Loss: 0.107072614133358\n",
            "Epoch: 582 | Loss: 0.04257364571094513 | Val. Loss: 0.10753989219665527\n",
            "Epoch: 583 | Loss: 0.040241312235593796 | Val. Loss: 0.10695961862802505\n",
            "Epoch: 584 | Loss: 0.04022636264562607 | Val. Loss: 0.10632327944040298\n",
            "Epoch: 585 | Loss: 0.058392204344272614 | Val. Loss: 0.10562660545110703\n",
            "Epoch: 586 | Loss: 0.04130733385682106 | Val. Loss: 0.10508749634027481\n",
            "Epoch: 587 | Loss: 0.034353360533714294 | Val. Loss: 0.10536615550518036\n",
            "Epoch: 588 | Loss: 0.09623245149850845 | Val. Loss: 0.1045706495642662\n",
            "Epoch: 589 | Loss: 0.08496297150850296 | Val. Loss: 0.10414357483386993\n",
            "Epoch: 590 | Loss: 0.05996231362223625 | Val. Loss: 0.10463429987430573\n",
            "Epoch: 591 | Loss: 0.11008556932210922 | Val. Loss: 0.10561804473400116\n",
            "Epoch: 592 | Loss: 0.03352223336696625 | Val. Loss: 0.10479883849620819\n",
            "Epoch: 593 | Loss: 0.05483730882406235 | Val. Loss: 0.10514169931411743\n",
            "Epoch: 594 | Loss: 0.051230184733867645 | Val. Loss: 0.10411664098501205\n",
            "Epoch: 595 | Loss: 0.03315165638923645 | Val. Loss: 0.10383790731430054\n",
            "Epoch: 596 | Loss: 0.0612671822309494 | Val. Loss: 0.10469230264425278\n",
            "Epoch: 597 | Loss: 0.041317202150821686 | Val. Loss: 0.10135801881551743\n",
            "Epoch: 598 | Loss: 0.057261787354946136 | Val. Loss: 0.10099682956933975\n",
            "Epoch: 599 | Loss: 0.03215019777417183 | Val. Loss: 0.10270096361637115\n",
            "Epoch: 600 | Loss: 0.021285850554704666 | Val. Loss: 0.10341934114694595\n",
            "Epoch: 601 | Loss: 0.048544060438871384 | Val. Loss: 0.10319380462169647\n",
            "Epoch: 602 | Loss: 0.042881932109594345 | Val. Loss: 0.10298216342926025\n",
            "Epoch: 603 | Loss: 0.03522995859384537 | Val. Loss: 0.10231540352106094\n",
            "Epoch: 604 | Loss: 0.03210379183292389 | Val. Loss: 0.10060198605060577\n",
            "Epoch: 605 | Loss: 0.026145899668335915 | Val. Loss: 0.10129735618829727\n",
            "Epoch: 606 | Loss: 0.02930787205696106 | Val. Loss: 0.1023344174027443\n",
            "Epoch: 607 | Loss: 0.056367337703704834 | Val. Loss: 0.10066311061382294\n",
            "Epoch: 608 | Loss: 0.0649176687002182 | Val. Loss: 0.10154708474874496\n",
            "Epoch: 609 | Loss: 0.048320237547159195 | Val. Loss: 0.09947541356086731\n",
            "Epoch: 610 | Loss: 0.06417299807071686 | Val. Loss: 0.10037320852279663\n",
            "Epoch: 611 | Loss: 0.015323235653340816 | Val. Loss: 0.100345179438591\n",
            "Epoch: 612 | Loss: 0.030146975070238113 | Val. Loss: 0.10137514024972916\n",
            "Epoch: 613 | Loss: 0.054217733442783356 | Val. Loss: 0.09965087473392487\n",
            "Epoch: 614 | Loss: 0.039124201983213425 | Val. Loss: 0.09963919967412949\n",
            "Epoch: 615 | Loss: 0.06681254506111145 | Val. Loss: 0.10024770349264145\n",
            "Epoch: 616 | Loss: 0.0649484246969223 | Val. Loss: 0.10076725482940674\n",
            "Epoch: 617 | Loss: 0.06168266385793686 | Val. Loss: 0.09900342673063278\n",
            "Epoch: 618 | Loss: 0.023292772471904755 | Val. Loss: 0.09564540535211563\n",
            "Epoch: 619 | Loss: 0.0604778453707695 | Val. Loss: 0.09608718007802963\n",
            "Epoch: 620 | Loss: 0.054753415286540985 | Val. Loss: 0.09738368541002274\n",
            "Epoch: 621 | Loss: 0.03372969850897789 | Val. Loss: 0.09748878329992294\n",
            "Epoch: 622 | Loss: 0.06564821302890778 | Val. Loss: 0.09922951459884644\n",
            "Epoch: 623 | Loss: 0.024199293926358223 | Val. Loss: 0.09796315431594849\n",
            "Epoch: 624 | Loss: 0.05765178054571152 | Val. Loss: 0.09624218940734863\n",
            "Epoch: 625 | Loss: 0.020415781065821648 | Val. Loss: 0.09743120521306992\n",
            "Epoch: 626 | Loss: 0.01996750570833683 | Val. Loss: 0.09877030551433563\n",
            "Epoch: 627 | Loss: 0.031375329941511154 | Val. Loss: 0.0979892835021019\n",
            "Epoch: 628 | Loss: 0.021504271775484085 | Val. Loss: 0.09677378833293915\n",
            "Epoch: 629 | Loss: 0.053196899592876434 | Val. Loss: 0.09691111743450165\n",
            "Epoch: 630 | Loss: 0.04444870352745056 | Val. Loss: 0.0968962088227272\n",
            "Epoch: 631 | Loss: 0.031488072127103806 | Val. Loss: 0.0974227786064148\n",
            "Epoch: 632 | Loss: 0.08358234167098999 | Val. Loss: 0.09583833068609238\n",
            "Epoch: 633 | Loss: 0.047823715955019 | Val. Loss: 0.09421621263027191\n",
            "Epoch: 634 | Loss: 0.030492285266518593 | Val. Loss: 0.09573004394769669\n",
            "Epoch: 635 | Loss: 0.05608905851840973 | Val. Loss: 0.0960380956530571\n",
            "Epoch: 636 | Loss: 0.007613396737724543 | Val. Loss: 0.09529303014278412\n",
            "Epoch: 637 | Loss: 0.020310714840888977 | Val. Loss: 0.09566624462604523\n",
            "Epoch: 638 | Loss: 0.07843758910894394 | Val. Loss: 0.09419087320566177\n",
            "Epoch: 639 | Loss: 0.06439586728811264 | Val. Loss: 0.09380687773227692\n",
            "Epoch: 640 | Loss: 0.02458980493247509 | Val. Loss: 0.09249497950077057\n",
            "Epoch: 641 | Loss: 0.03651909530162811 | Val. Loss: 0.09181121736764908\n",
            "Epoch: 642 | Loss: 0.03868475556373596 | Val. Loss: 0.09187258780002594\n",
            "Epoch: 643 | Loss: 0.04863641411066055 | Val. Loss: 0.09351348876953125\n",
            "Epoch: 644 | Loss: 0.02806527353823185 | Val. Loss: 0.0935649424791336\n",
            "Epoch: 645 | Loss: 0.029409710317850113 | Val. Loss: 0.09191454946994781\n",
            "Epoch: 646 | Loss: 0.007364590652287006 | Val. Loss: 0.09220553189516068\n",
            "Epoch: 647 | Loss: 0.022772017866373062 | Val. Loss: 0.09340454638004303\n",
            "Epoch: 648 | Loss: 0.01005538273602724 | Val. Loss: 0.09149064123630524\n",
            "Epoch: 649 | Loss: 0.04126296937465668 | Val. Loss: 0.0890858918428421\n",
            "Epoch: 650 | Loss: 0.04272809624671936 | Val. Loss: 0.09103060513734818\n",
            "Epoch: 651 | Loss: 0.04184228554368019 | Val. Loss: 0.09059397131204605\n",
            "Epoch: 652 | Loss: 0.03868080675601959 | Val. Loss: 0.08955727517604828\n",
            "Epoch: 653 | Loss: 0.023021548986434937 | Val. Loss: 0.08930234611034393\n",
            "Epoch: 654 | Loss: 0.03872517868876457 | Val. Loss: 0.08810772001743317\n",
            "Epoch: 655 | Loss: 0.03685331344604492 | Val. Loss: 0.08811106532812119\n",
            "Epoch: 656 | Loss: 0.03793877363204956 | Val. Loss: 0.08858756721019745\n",
            "Epoch: 657 | Loss: 0.02332509681582451 | Val. Loss: 0.08863814175128937\n",
            "Epoch: 658 | Loss: 0.017503324896097183 | Val. Loss: 0.08798627555370331\n",
            "Epoch: 659 | Loss: 0.037650275975465775 | Val. Loss: 0.08808119595050812\n",
            "Epoch: 660 | Loss: 0.022670257836580276 | Val. Loss: 0.08803294599056244\n",
            "Epoch: 661 | Loss: 0.03414931148290634 | Val. Loss: 0.0869659036397934\n",
            "Epoch: 662 | Loss: 0.09043193608522415 | Val. Loss: 0.08760794997215271\n",
            "Epoch: 663 | Loss: 0.06360074132680893 | Val. Loss: 0.08748830854892731\n",
            "Epoch: 664 | Loss: 0.0375371091067791 | Val. Loss: 0.08515473455190659\n",
            "Epoch: 665 | Loss: 0.023455236107110977 | Val. Loss: 0.08619461208581924\n",
            "Epoch: 666 | Loss: 0.016060467809438705 | Val. Loss: 0.08580616861581802\n",
            "Epoch: 667 | Loss: 0.036466583609580994 | Val. Loss: 0.08536480367183685\n",
            "Epoch: 668 | Loss: 0.06287519633769989 | Val. Loss: 0.08644440770149231\n",
            "Epoch: 669 | Loss: 0.08464358001947403 | Val. Loss: 0.08451932668685913\n",
            "Epoch: 670 | Loss: 0.04184392839670181 | Val. Loss: 0.0842420682311058\n",
            "Epoch: 671 | Loss: 0.08882535994052887 | Val. Loss: 0.08563707768917084\n",
            "Epoch: 672 | Loss: 0.024509785696864128 | Val. Loss: 0.08460283279418945\n",
            "Epoch: 673 | Loss: 0.02703934907913208 | Val. Loss: 0.08286569267511368\n",
            "Epoch: 674 | Loss: 0.026545045897364616 | Val. Loss: 0.08382092416286469\n",
            "Epoch: 675 | Loss: 0.016758369281888008 | Val. Loss: 0.0829041600227356\n",
            "Epoch: 676 | Loss: 0.05348483473062515 | Val. Loss: 0.0836348682641983\n",
            "Epoch: 677 | Loss: 0.003573938272893429 | Val. Loss: 0.08300036191940308\n",
            "Epoch: 678 | Loss: 0.061365436762571335 | Val. Loss: 0.08236831426620483\n",
            "Epoch: 679 | Loss: 0.019915150478482246 | Val. Loss: 0.08079137653112411\n",
            "Epoch: 680 | Loss: 0.03289993479847908 | Val. Loss: 0.08214632421731949\n",
            "Epoch: 681 | Loss: 0.044093333184719086 | Val. Loss: 0.08046860992908478\n",
            "Epoch: 682 | Loss: 0.03682422265410423 | Val. Loss: 0.07935361564159393\n",
            "Epoch: 683 | Loss: 0.03491048514842987 | Val. Loss: 0.08100847154855728\n",
            "Epoch: 684 | Loss: 0.013282527215778828 | Val. Loss: 0.07870714366436005\n",
            "Epoch: 685 | Loss: 0.044550880789756775 | Val. Loss: 0.07705456018447876\n",
            "Epoch: 686 | Loss: 0.058003608137369156 | Val. Loss: 0.07852368801832199\n",
            "Epoch: 687 | Loss: 0.05893988534808159 | Val. Loss: 0.08011043071746826\n",
            "Epoch: 688 | Loss: 0.05973917990922928 | Val. Loss: 0.07936282455921173\n",
            "Epoch: 689 | Loss: 0.024980328977108 | Val. Loss: 0.07871885597705841\n",
            "Epoch: 690 | Loss: 0.025454632937908173 | Val. Loss: 0.07743595540523529\n",
            "Epoch: 691 | Loss: 0.030937327072024345 | Val. Loss: 0.07791157066822052\n",
            "Epoch: 692 | Loss: 0.023931529372930527 | Val. Loss: 0.07818102091550827\n",
            "Epoch: 693 | Loss: 0.03073681890964508 | Val. Loss: 0.07769438624382019\n",
            "Epoch: 694 | Loss: 0.040771495550870895 | Val. Loss: 0.07668401300907135\n",
            "Epoch: 695 | Loss: 0.026958486065268517 | Val. Loss: 0.07443264871835709\n",
            "Epoch: 696 | Loss: 0.02699517272412777 | Val. Loss: 0.07495548576116562\n",
            "Epoch: 697 | Loss: 0.009760373272001743 | Val. Loss: 0.0757722333073616\n",
            "Epoch: 698 | Loss: 0.026373732835054398 | Val. Loss: 0.07404862344264984\n",
            "Epoch: 699 | Loss: 0.016805220395326614 | Val. Loss: 0.07348649203777313\n",
            "Epoch: 700 | Loss: 0.02036590687930584 | Val. Loss: 0.07418539375066757\n",
            "Epoch: 701 | Loss: 0.014662019908428192 | Val. Loss: 0.0754774734377861\n",
            "Epoch: 702 | Loss: 0.05384122207760811 | Val. Loss: 0.07455168664455414\n",
            "Epoch: 703 | Loss: 0.034410085529088974 | Val. Loss: 0.07202675938606262\n",
            "Epoch: 704 | Loss: 0.011878563091158867 | Val. Loss: 0.07053951174020767\n",
            "Epoch: 705 | Loss: 0.028177423402667046 | Val. Loss: 0.07171521335840225\n",
            "Epoch: 706 | Loss: 0.03686554357409477 | Val. Loss: 0.07293407618999481\n",
            "Epoch: 707 | Loss: 0.0059389034286141396 | Val. Loss: 0.07114546000957489\n",
            "Epoch: 708 | Loss: 0.06151457503437996 | Val. Loss: 0.07015363872051239\n",
            "Epoch: 709 | Loss: 0.01950409635901451 | Val. Loss: 0.07040433585643768\n",
            "Epoch: 710 | Loss: 0.02006964571774006 | Val. Loss: 0.07205918431282043\n",
            "Epoch: 711 | Loss: 0.023653680458664894 | Val. Loss: 0.07105787098407745\n",
            "Epoch: 712 | Loss: 0.01465982012450695 | Val. Loss: 0.06956316530704498\n",
            "Epoch: 713 | Loss: 0.012747496366500854 | Val. Loss: 0.07012350857257843\n",
            "Epoch: 714 | Loss: 0.01649313233792782 | Val. Loss: 0.06846888363361359\n",
            "Epoch: 715 | Loss: 0.029057692736387253 | Val. Loss: 0.0671938955783844\n",
            "Epoch: 716 | Loss: 0.01681802049279213 | Val. Loss: 0.06690932810306549\n",
            "Epoch: 717 | Loss: 0.02411499060690403 | Val. Loss: 0.0688888430595398\n",
            "Epoch: 718 | Loss: 0.0413518063724041 | Val. Loss: 0.06755151599645615\n",
            "Epoch: 719 | Loss: 0.03907332941889763 | Val. Loss: 0.06767547875642776\n",
            "Epoch: 720 | Loss: 0.028683142736554146 | Val. Loss: 0.06768853962421417\n",
            "Epoch: 721 | Loss: 0.015295243822038174 | Val. Loss: 0.06425173580646515\n",
            "Epoch: 722 | Loss: 0.037114229053258896 | Val. Loss: 0.0640997514128685\n",
            "Epoch: 723 | Loss: 0.051137182861566544 | Val. Loss: 0.06712116301059723\n",
            "Epoch: 724 | Loss: 0.04390082135796547 | Val. Loss: 0.06551237404346466\n",
            "Epoch: 725 | Loss: 0.030066175386309624 | Val. Loss: 0.06417078524827957\n",
            "Epoch: 726 | Loss: 0.014158697798848152 | Val. Loss: 0.06399508565664291\n",
            "Epoch: 727 | Loss: 0.018506426364183426 | Val. Loss: 0.06576106697320938\n",
            "Epoch: 728 | Loss: 0.024268392473459244 | Val. Loss: 0.06534869968891144\n",
            "Epoch: 729 | Loss: 0.014694003388285637 | Val. Loss: 0.06356491893529892\n",
            "Epoch: 730 | Loss: 0.025679774582386017 | Val. Loss: 0.06343116611242294\n",
            "Epoch: 731 | Loss: 0.028007561340928078 | Val. Loss: 0.0640759989619255\n",
            "Epoch: 732 | Loss: 0.02422190085053444 | Val. Loss: 0.06382575631141663\n",
            "Epoch: 733 | Loss: 0.03152765706181526 | Val. Loss: 0.061781786382198334\n",
            "Epoch: 734 | Loss: 0.059088874608278275 | Val. Loss: 0.05971560627222061\n",
            "Epoch: 735 | Loss: 0.016748977825045586 | Val. Loss: 0.06139415502548218\n",
            "Epoch: 736 | Loss: 0.019872069358825684 | Val. Loss: 0.06125909835100174\n",
            "Epoch: 737 | Loss: 0.04803910851478577 | Val. Loss: 0.06149245426058769\n",
            "Epoch: 738 | Loss: 0.014579502865672112 | Val. Loss: 0.06089968606829643\n",
            "Epoch: 739 | Loss: 0.0381944514811039 | Val. Loss: 0.0587940439581871\n",
            "Epoch: 740 | Loss: 0.04276014864444733 | Val. Loss: 0.05926969647407532\n",
            "Epoch: 741 | Loss: 0.029510416090488434 | Val. Loss: 0.06026234105229378\n",
            "Epoch: 742 | Loss: 0.03273943439126015 | Val. Loss: 0.06040055304765701\n",
            "Epoch: 743 | Loss: 0.017146918922662735 | Val. Loss: 0.058280158787965775\n",
            "Epoch: 744 | Loss: 0.04162999242544174 | Val. Loss: 0.05838543921709061\n",
            "Epoch: 745 | Loss: 0.044608719646930695 | Val. Loss: 0.05944100022315979\n",
            "Epoch: 746 | Loss: 0.03247477486729622 | Val. Loss: 0.058163005858659744\n",
            "Epoch: 747 | Loss: 0.030615419149398804 | Val. Loss: 0.05685124546289444\n",
            "Epoch: 748 | Loss: 0.05133838951587677 | Val. Loss: 0.05794888734817505\n",
            "Epoch: 749 | Loss: 0.0050119091756641865 | Val. Loss: 0.05644848197698593\n",
            "Epoch: 750 | Loss: 0.01325785182416439 | Val. Loss: 0.05826937407255173\n",
            "Epoch: 751 | Loss: 0.042981814593076706 | Val. Loss: 0.05617469549179077\n",
            "Epoch: 752 | Loss: 0.02232346497476101 | Val. Loss: 0.055290210992097855\n",
            "Epoch: 753 | Loss: 0.026677493005990982 | Val. Loss: 0.055584896355867386\n",
            "Epoch: 754 | Loss: 0.02078060433268547 | Val. Loss: 0.054212283343076706\n",
            "Epoch: 755 | Loss: 0.04486114904284477 | Val. Loss: 0.0525960698723793\n",
            "Epoch: 756 | Loss: 0.047345519065856934 | Val. Loss: 0.05411937087774277\n",
            "Epoch: 757 | Loss: 0.036542296409606934 | Val. Loss: 0.05479934811592102\n",
            "Epoch: 758 | Loss: 0.02746249921619892 | Val. Loss: 0.0527067668735981\n",
            "Epoch: 759 | Loss: 0.007883604615926743 | Val. Loss: 0.05319228768348694\n",
            "Epoch: 760 | Loss: 0.049222178757190704 | Val. Loss: 0.05250959470868111\n",
            "Epoch: 761 | Loss: 0.04182897135615349 | Val. Loss: 0.05260307714343071\n",
            "Epoch: 762 | Loss: 0.016752174124121666 | Val. Loss: 0.05230001360177994\n",
            "Epoch: 763 | Loss: 0.029666362330317497 | Val. Loss: 0.051632147282361984\n",
            "Epoch: 764 | Loss: 0.01661699265241623 | Val. Loss: 0.05221287161111832\n",
            "Epoch: 765 | Loss: 0.028091754764318466 | Val. Loss: 0.051310569047927856\n",
            "Epoch: 766 | Loss: 0.019638782367110252 | Val. Loss: 0.05032826215028763\n",
            "Epoch: 767 | Loss: 0.03520860895514488 | Val. Loss: 0.050982873886823654\n",
            "Epoch: 768 | Loss: 0.00930644478648901 | Val. Loss: 0.051179371774196625\n",
            "Epoch: 769 | Loss: 0.058345090597867966 | Val. Loss: 0.04904501512646675\n",
            "Epoch: 770 | Loss: 0.029772071167826653 | Val. Loss: 0.04989691823720932\n",
            "Epoch: 771 | Loss: 0.0340682715177536 | Val. Loss: 0.05079343169927597\n",
            "Epoch: 772 | Loss: 0.00940852053463459 | Val. Loss: 0.049841318279504776\n",
            "Epoch: 773 | Loss: 0.029866598546504974 | Val. Loss: 0.04939509928226471\n",
            "Epoch: 774 | Loss: 0.026733752340078354 | Val. Loss: 0.04888911917805672\n",
            "Epoch: 775 | Loss: 0.03762589022517204 | Val. Loss: 0.04656629636883736\n",
            "Epoch: 776 | Loss: 0.017737578600645065 | Val. Loss: 0.04525666683912277\n",
            "Epoch: 777 | Loss: 0.03290049359202385 | Val. Loss: 0.0472906194627285\n",
            "Epoch: 778 | Loss: 0.01140017993748188 | Val. Loss: 0.046101316809654236\n",
            "Epoch: 779 | Loss: 0.016306240111589432 | Val. Loss: 0.047457851469516754\n",
            "Epoch: 780 | Loss: 0.020385783165693283 | Val. Loss: 0.047780103981494904\n",
            "Epoch: 781 | Loss: 0.01753269135951996 | Val. Loss: 0.044500961899757385\n",
            "Epoch: 782 | Loss: 0.036608364433050156 | Val. Loss: 0.043688300997018814\n",
            "Epoch: 783 | Loss: 0.013722056522965431 | Val. Loss: 0.04542328044772148\n",
            "Epoch: 784 | Loss: 0.020873405039310455 | Val. Loss: 0.0437856987118721\n",
            "Epoch: 785 | Loss: 0.01570526696741581 | Val. Loss: 0.04623236507177353\n",
            "Epoch: 786 | Loss: 0.010607768781483173 | Val. Loss: 0.0420563742518425\n",
            "Epoch: 787 | Loss: 0.011660215444862843 | Val. Loss: 0.03856935352087021\n",
            "Epoch: 788 | Loss: 0.010927913710474968 | Val. Loss: 0.04306814447045326\n",
            "Epoch: 789 | Loss: 0.017108654603362083 | Val. Loss: 0.042379654943943024\n",
            "Epoch: 790 | Loss: 0.03544551134109497 | Val. Loss: 0.03660384565591812\n",
            "Epoch: 791 | Loss: 0.020252659916877747 | Val. Loss: 0.036717481911182404\n",
            "Epoch: 792 | Loss: 0.02964748628437519 | Val. Loss: 0.03816314786672592\n",
            "Epoch: 793 | Loss: 0.01326163113117218 | Val. Loss: 0.03618883341550827\n",
            "Epoch: 794 | Loss: 0.03466560319066048 | Val. Loss: 0.037034519016742706\n",
            "Epoch: 795 | Loss: 0.02710931934416294 | Val. Loss: 0.03989294916391373\n",
            "Epoch: 796 | Loss: 0.0135418102145195 | Val. Loss: 0.03701014071702957\n",
            "Epoch: 797 | Loss: 0.009106431156396866 | Val. Loss: 0.03471405431628227\n",
            "Epoch: 798 | Loss: 0.017810216173529625 | Val. Loss: 0.03704122453927994\n",
            "Epoch: 799 | Loss: 0.031222239136695862 | Val. Loss: 0.03520151972770691\n",
            "Epoch: 800 | Loss: 0.015831725671887398 | Val. Loss: 0.03410664573311806\n",
            "Epoch: 801 | Loss: 0.02708156406879425 | Val. Loss: 0.03373120352625847\n",
            "Epoch: 802 | Loss: 0.028847679495811462 | Val. Loss: 0.03390892595052719\n",
            "Epoch: 803 | Loss: 0.015049724839627743 | Val. Loss: 0.033389613032341\n",
            "Epoch: 804 | Loss: 0.01589195616543293 | Val. Loss: 0.033246107399463654\n",
            "Epoch: 805 | Loss: 0.02212424948811531 | Val. Loss: 0.032640475779771805\n",
            "Epoch: 806 | Loss: 0.013326439075171947 | Val. Loss: 0.03201265260577202\n",
            "Epoch: 807 | Loss: 0.014066998846828938 | Val. Loss: 0.03178572282195091\n",
            "Epoch: 808 | Loss: 0.02775077521800995 | Val. Loss: 0.03180953487753868\n",
            "Epoch: 809 | Loss: 0.027448037639260292 | Val. Loss: 0.031458303332328796\n",
            "Epoch: 810 | Loss: 0.02953634411096573 | Val. Loss: 0.029950251802802086\n",
            "Epoch: 811 | Loss: 0.008249999955296516 | Val. Loss: 0.029707351699471474\n",
            "Epoch: 812 | Loss: 0.00831730104982853 | Val. Loss: 0.030101995915174484\n",
            "Epoch: 813 | Loss: 0.02036297880113125 | Val. Loss: 0.029552092775702477\n",
            "Epoch: 814 | Loss: 0.018604079261422157 | Val. Loss: 0.029249519109725952\n",
            "Epoch: 815 | Loss: 0.00958342757076025 | Val. Loss: 0.02978062629699707\n",
            "Epoch: 816 | Loss: 0.009321155957877636 | Val. Loss: 0.028804048895835876\n",
            "Epoch: 817 | Loss: 0.02149912156164646 | Val. Loss: 0.028147825971245766\n",
            "Epoch: 818 | Loss: 0.020047862082719803 | Val. Loss: 0.02784930169582367\n",
            "Epoch: 819 | Loss: 0.03335246816277504 | Val. Loss: 0.027012716978788376\n",
            "Epoch: 820 | Loss: 0.005339959170669317 | Val. Loss: 0.027123872190713882\n",
            "Epoch: 821 | Loss: 0.03169843181967735 | Val. Loss: 0.026941126212477684\n",
            "Epoch: 822 | Loss: 0.018186364322900772 | Val. Loss: 0.026705065742135048\n",
            "Epoch: 823 | Loss: 0.01846444606781006 | Val. Loss: 0.0267360620200634\n",
            "Epoch: 824 | Loss: 0.0075516789220273495 | Val. Loss: 0.026149556040763855\n",
            "Epoch: 825 | Loss: 0.029039189219474792 | Val. Loss: 0.025819510221481323\n",
            "Epoch: 826 | Loss: 0.008434869349002838 | Val. Loss: 0.02571391500532627\n",
            "Epoch: 827 | Loss: 0.03974581137299538 | Val. Loss: 0.02570577897131443\n",
            "Epoch: 828 | Loss: 0.015127245336771011 | Val. Loss: 0.025397058576345444\n",
            "Epoch: 829 | Loss: 0.029505852609872818 | Val. Loss: 0.024971822276711464\n",
            "Epoch: 830 | Loss: 0.008930647745728493 | Val. Loss: 0.024982739239931107\n",
            "Epoch: 831 | Loss: 0.016155127435922623 | Val. Loss: 0.024808144196867943\n",
            "Epoch: 832 | Loss: 0.011618595570325851 | Val. Loss: 0.024710312485694885\n",
            "Epoch: 833 | Loss: 0.024104051291942596 | Val. Loss: 0.024794626981019974\n",
            "Epoch: 834 | Loss: 0.032284319400787354 | Val. Loss: 0.023792212828993797\n",
            "Epoch: 835 | Loss: 0.006773194298148155 | Val. Loss: 0.023881789296865463\n",
            "Epoch: 836 | Loss: 0.01754830591380596 | Val. Loss: 0.023812854662537575\n",
            "Epoch: 837 | Loss: 0.03468228876590729 | Val. Loss: 0.02391449734568596\n",
            "Epoch: 838 | Loss: 0.016855880618095398 | Val. Loss: 0.023123305290937424\n",
            "Epoch: 839 | Loss: 0.009661106392741203 | Val. Loss: 0.02339095249772072\n",
            "Epoch: 840 | Loss: 0.01679721474647522 | Val. Loss: 0.023763667792081833\n",
            "Epoch: 841 | Loss: 0.028776194900274277 | Val. Loss: 0.023136598989367485\n",
            "Epoch: 842 | Loss: 0.011722220107913017 | Val. Loss: 0.023504819720983505\n",
            "Epoch: 843 | Loss: 0.02203856222331524 | Val. Loss: 0.02298079989850521\n",
            "Epoch: 844 | Loss: 0.014707020483911037 | Val. Loss: 0.02286563441157341\n",
            "Epoch: 845 | Loss: 0.008552801795303822 | Val. Loss: 0.022594308480620384\n",
            "Epoch: 846 | Loss: 0.005963719449937344 | Val. Loss: 0.02247758023440838\n",
            "Epoch: 847 | Loss: 0.02521899715065956 | Val. Loss: 0.022114861756563187\n",
            "Epoch: 848 | Loss: 0.02532842569053173 | Val. Loss: 0.022037018090486526\n",
            "Epoch: 849 | Loss: 0.023383954539895058 | Val. Loss: 0.022663693875074387\n",
            "Epoch: 850 | Loss: 0.010305759496986866 | Val. Loss: 0.022134020924568176\n",
            "Epoch: 851 | Loss: 0.020080508664250374 | Val. Loss: 0.021853875368833542\n",
            "Epoch: 852 | Loss: 0.00479941675439477 | Val. Loss: 0.021362775936722755\n",
            "Epoch: 853 | Loss: 0.031644467264413834 | Val. Loss: 0.022223979234695435\n",
            "Epoch: 854 | Loss: 0.027083612978458405 | Val. Loss: 0.021914491429924965\n",
            "Epoch: 855 | Loss: 0.007580796722322702 | Val. Loss: 0.021235886961221695\n",
            "Epoch: 856 | Loss: 0.024952486157417297 | Val. Loss: 0.021244486793875694\n",
            "Epoch: 857 | Loss: 0.019355682656168938 | Val. Loss: 0.02128729596734047\n",
            "Epoch: 858 | Loss: 0.0035168142057955265 | Val. Loss: 0.0213560052216053\n",
            "Epoch: 859 | Loss: 0.018803659826517105 | Val. Loss: 0.02072252705693245\n",
            "Epoch: 860 | Loss: 0.020608894526958466 | Val. Loss: 0.020595747977495193\n",
            "Epoch: 861 | Loss: 0.005156938452273607 | Val. Loss: 0.021020878106355667\n",
            "Epoch: 862 | Loss: 0.005263384431600571 | Val. Loss: 0.020646875724196434\n",
            "Epoch: 863 | Loss: 0.02109949290752411 | Val. Loss: 0.020190006121993065\n",
            "Epoch: 864 | Loss: 0.037908852100372314 | Val. Loss: 0.020360847935080528\n",
            "Epoch: 865 | Loss: 0.014576858840882778 | Val. Loss: 0.01979243941605091\n",
            "Epoch: 866 | Loss: 0.010939895175397396 | Val. Loss: 0.020104028284549713\n",
            "Epoch: 867 | Loss: 0.013486947864294052 | Val. Loss: 0.019864803180098534\n",
            "Epoch: 868 | Loss: 0.020299410447478294 | Val. Loss: 0.019693724811077118\n",
            "Epoch: 869 | Loss: 0.01297734770923853 | Val. Loss: 0.020043756812810898\n",
            "Epoch: 870 | Loss: 0.015275003388524055 | Val. Loss: 0.019965123385190964\n",
            "Epoch: 871 | Loss: 0.030811063945293427 | Val. Loss: 0.019768457859754562\n",
            "Epoch: 872 | Loss: 0.0135521674528718 | Val. Loss: 0.018040480092167854\n",
            "Epoch: 873 | Loss: 0.006987539120018482 | Val. Loss: 0.017762916162610054\n",
            "Epoch: 874 | Loss: 0.01612739823758602 | Val. Loss: 0.017093120142817497\n",
            "Epoch: 875 | Loss: 0.015865463763475418 | Val. Loss: 0.016719236969947815\n",
            "Epoch: 876 | Loss: 0.00893871858716011 | Val. Loss: 0.016439754515886307\n",
            "Epoch: 877 | Loss: 0.006739608943462372 | Val. Loss: 0.016817951574921608\n",
            "Epoch: 878 | Loss: 0.011384829878807068 | Val. Loss: 0.01637391559779644\n",
            "Epoch: 879 | Loss: 0.004626759793609381 | Val. Loss: 0.016071626916527748\n",
            "Epoch: 880 | Loss: 0.013575958088040352 | Val. Loss: 0.016124537214636803\n",
            "Epoch: 881 | Loss: 0.0064105200581252575 | Val. Loss: 0.01596023328602314\n",
            "Epoch: 882 | Loss: 0.006066932342946529 | Val. Loss: 0.015948768705129623\n",
            "Epoch: 883 | Loss: 0.024407852441072464 | Val. Loss: 0.015682684257626534\n",
            "Epoch: 884 | Loss: 0.004769823048263788 | Val. Loss: 0.015587719157338142\n",
            "Epoch: 885 | Loss: 0.0169526319950819 | Val. Loss: 0.015469293110072613\n",
            "Epoch: 886 | Loss: 0.0044504557736217976 | Val. Loss: 0.015387440100312233\n",
            "Epoch: 887 | Loss: 0.005189056508243084 | Val. Loss: 0.015443453565239906\n",
            "Epoch: 888 | Loss: 0.006033728364855051 | Val. Loss: 0.014911937527358532\n",
            "Epoch: 889 | Loss: 0.008508636616170406 | Val. Loss: 0.015254192054271698\n",
            "Epoch: 890 | Loss: 0.03101757913827896 | Val. Loss: 0.015345381572842598\n",
            "Epoch: 891 | Loss: 0.021301623433828354 | Val. Loss: 0.015041934326291084\n",
            "Epoch: 892 | Loss: 0.021697094663977623 | Val. Loss: 0.014867712743580341\n",
            "Epoch: 893 | Loss: 0.016479844227433205 | Val. Loss: 0.014837878756225109\n",
            "Epoch: 894 | Loss: 0.020899660885334015 | Val. Loss: 0.014378051273524761\n",
            "Epoch: 895 | Loss: 0.00800213124603033 | Val. Loss: 0.01450425386428833\n",
            "Epoch: 896 | Loss: 0.0009457369451411068 | Val. Loss: 0.0146062346175313\n",
            "Epoch: 897 | Loss: 0.01243350375443697 | Val. Loss: 0.014050928875803947\n",
            "Epoch: 898 | Loss: 0.0066279880702495575 | Val. Loss: 0.014933465048670769\n",
            "Epoch: 899 | Loss: 0.0015098961303010583 | Val. Loss: 0.014491965062916279\n",
            "Epoch: 900 | Loss: 0.01198616810142994 | Val. Loss: 0.014063837938010693\n",
            "Epoch: 901 | Loss: 0.007220094557851553 | Val. Loss: 0.013996034860610962\n",
            "Epoch: 902 | Loss: 0.004519267939031124 | Val. Loss: 0.014025745913386345\n",
            "Epoch: 903 | Loss: 0.005366150755435228 | Val. Loss: 0.014031323604285717\n",
            "Epoch: 904 | Loss: 0.00896706897765398 | Val. Loss: 0.013638673350214958\n",
            "Epoch: 905 | Loss: 0.007820249535143375 | Val. Loss: 0.01388336718082428\n",
            "Epoch: 906 | Loss: 0.011816457845270634 | Val. Loss: 0.01357884518802166\n",
            "Epoch: 907 | Loss: 0.014047384262084961 | Val. Loss: 0.013745857402682304\n",
            "Epoch: 908 | Loss: 0.01644781231880188 | Val. Loss: 0.01336555927991867\n",
            "Epoch: 909 | Loss: 0.012002266943454742 | Val. Loss: 0.013396991416811943\n",
            "Epoch: 910 | Loss: 0.003922425676137209 | Val. Loss: 0.013219401240348816\n",
            "Epoch: 911 | Loss: 0.0010204233694821596 | Val. Loss: 0.013203981332480907\n",
            "Epoch: 912 | Loss: 0.005561382044106722 | Val. Loss: 0.01322769932448864\n",
            "Epoch: 913 | Loss: 0.005605841986835003 | Val. Loss: 0.01320735551416874\n",
            "Epoch: 914 | Loss: 0.003427431918680668 | Val. Loss: 0.012984859757125378\n",
            "Epoch: 915 | Loss: 0.015522425062954426 | Val. Loss: 0.012905238196253777\n",
            "Epoch: 916 | Loss: 0.00684307049959898 | Val. Loss: 0.012876403518021107\n",
            "Epoch: 917 | Loss: 0.004656091332435608 | Val. Loss: 0.012721767649054527\n",
            "Epoch: 918 | Loss: 0.009446006268262863 | Val. Loss: 0.012845161370933056\n",
            "Epoch: 919 | Loss: 0.00989504810422659 | Val. Loss: 0.012640859000384808\n",
            "Epoch: 920 | Loss: 0.01716163381934166 | Val. Loss: 0.012544828467071056\n",
            "Epoch: 921 | Loss: 0.007311264052987099 | Val. Loss: 0.012432378716766834\n",
            "Epoch: 922 | Loss: 0.022960679605603218 | Val. Loss: 0.012209907174110413\n",
            "Epoch: 923 | Loss: 0.019802071154117584 | Val. Loss: 0.012457609176635742\n",
            "Epoch: 924 | Loss: 0.009002909995615482 | Val. Loss: 0.012228274717926979\n",
            "Epoch: 925 | Loss: 0.010820938274264336 | Val. Loss: 0.0121231060475111\n",
            "Epoch: 926 | Loss: 0.021854005753993988 | Val. Loss: 0.011896597221493721\n",
            "Epoch: 927 | Loss: 0.011558650992810726 | Val. Loss: 0.012145815417170525\n",
            "Epoch: 928 | Loss: 0.01706610433757305 | Val. Loss: 0.011820008046925068\n",
            "Epoch: 929 | Loss: 0.006405321881175041 | Val. Loss: 0.011989581398665905\n",
            "Epoch: 930 | Loss: 0.007007997948676348 | Val. Loss: 0.011629270389676094\n",
            "Epoch: 931 | Loss: 0.010829603299498558 | Val. Loss: 0.01197779644280672\n",
            "Epoch: 932 | Loss: 0.009170453064143658 | Val. Loss: 0.011630958877503872\n",
            "Epoch: 933 | Loss: 0.012598540633916855 | Val. Loss: 0.011528237722814083\n",
            "Epoch: 934 | Loss: 0.01049824059009552 | Val. Loss: 0.011705690063536167\n",
            "Epoch: 935 | Loss: 0.012438233941793442 | Val. Loss: 0.011443897150456905\n",
            "Epoch: 936 | Loss: 0.005809490103274584 | Val. Loss: 0.011654363013803959\n",
            "Epoch: 937 | Loss: 0.010632188990712166 | Val. Loss: 0.011372738517820835\n",
            "Epoch: 938 | Loss: 0.005233675707131624 | Val. Loss: 0.011083929799497128\n",
            "Epoch: 939 | Loss: 0.003419378073886037 | Val. Loss: 0.011354103684425354\n",
            "Epoch: 940 | Loss: 0.009738289751112461 | Val. Loss: 0.011144743300974369\n",
            "Epoch: 941 | Loss: 0.0043803430162370205 | Val. Loss: 0.011018183082342148\n",
            "Epoch: 942 | Loss: 0.011929195374250412 | Val. Loss: 0.010829860344529152\n",
            "Epoch: 943 | Loss: 0.020000385120511055 | Val. Loss: 0.011031548492610455\n",
            "Epoch: 944 | Loss: 0.014046268537640572 | Val. Loss: 0.010893287137150764\n",
            "Epoch: 945 | Loss: 0.004179473500698805 | Val. Loss: 0.010876656509935856\n",
            "Epoch: 946 | Loss: 0.005539229605346918 | Val. Loss: 0.010548814199864864\n",
            "Epoch: 947 | Loss: 0.009149441495537758 | Val. Loss: 0.010595081374049187\n",
            "Epoch: 948 | Loss: 0.007859615609049797 | Val. Loss: 0.010450487956404686\n",
            "Epoch: 949 | Loss: 0.01658567599952221 | Val. Loss: 0.010540622286498547\n",
            "Epoch: 950 | Loss: 0.0013233405770733953 | Val. Loss: 0.01052645780146122\n",
            "Epoch: 951 | Loss: 0.005434670951217413 | Val. Loss: 0.01044929027557373\n",
            "Epoch: 952 | Loss: 0.006612241268157959 | Val. Loss: 0.010210531763732433\n",
            "Epoch: 953 | Loss: 0.010082703083753586 | Val. Loss: 0.0101053137332201\n",
            "Epoch: 954 | Loss: 0.02165060117840767 | Val. Loss: 0.01025697123259306\n",
            "Epoch: 955 | Loss: 0.003472680225968361 | Val. Loss: 0.00991077721118927\n",
            "Epoch: 956 | Loss: 0.006328791845589876 | Val. Loss: 0.009999118745326996\n",
            "Epoch: 957 | Loss: 0.007494404911994934 | Val. Loss: 0.01011937391012907\n",
            "Epoch: 958 | Loss: 0.01847732625901699 | Val. Loss: 0.01009434089064598\n",
            "Epoch: 959 | Loss: 0.009242051281034946 | Val. Loss: 0.009697149507701397\n",
            "Epoch: 960 | Loss: 0.011707624420523643 | Val. Loss: 0.00981544703245163\n",
            "Epoch: 961 | Loss: 0.005238722078502178 | Val. Loss: 0.00979434885084629\n",
            "Epoch: 962 | Loss: 0.01788727566599846 | Val. Loss: 0.009585924446582794\n",
            "Epoch: 963 | Loss: 0.00397734297439456 | Val. Loss: 0.009576095268130302\n",
            "Epoch: 964 | Loss: 0.014015440829098225 | Val. Loss: 0.009684288874268532\n",
            "Epoch: 965 | Loss: 0.00393916480243206 | Val. Loss: 0.009541965089738369\n",
            "Epoch: 966 | Loss: 0.008613089099526405 | Val. Loss: 0.009691799990832806\n",
            "Epoch: 967 | Loss: 0.006132080685347319 | Val. Loss: 0.009548080153763294\n",
            "Epoch: 968 | Loss: 0.006812563166022301 | Val. Loss: 0.009310761466622353\n",
            "Epoch: 969 | Loss: 0.004182334523648024 | Val. Loss: 0.00925770215690136\n",
            "Epoch: 970 | Loss: 0.00536427553743124 | Val. Loss: 0.009391430765390396\n",
            "Epoch: 971 | Loss: 0.005242813378572464 | Val. Loss: 0.00919347070157528\n",
            "Epoch: 972 | Loss: 0.010850145481526852 | Val. Loss: 0.009006882086396217\n",
            "Epoch: 973 | Loss: 0.004239477217197418 | Val. Loss: 0.009005235508084297\n",
            "Epoch: 974 | Loss: 0.003582898061722517 | Val. Loss: 0.00887150876224041\n",
            "Epoch: 975 | Loss: 0.006616548635065556 | Val. Loss: 0.00896927434951067\n",
            "Epoch: 976 | Loss: 0.0064027514308691025 | Val. Loss: 0.008806831203401089\n",
            "Epoch: 977 | Loss: 0.005735434591770172 | Val. Loss: 0.008724348619580269\n",
            "Epoch: 978 | Loss: 0.02077329531311989 | Val. Loss: 0.008918728679418564\n",
            "Epoch: 979 | Loss: 0.003616342321038246 | Val. Loss: 0.008707000873982906\n",
            "Epoch: 980 | Loss: 0.0058049303479492664 | Val. Loss: 0.008651548065245152\n",
            "Epoch: 981 | Loss: 0.007314963266253471 | Val. Loss: 0.008539893664419651\n",
            "Epoch: 982 | Loss: 0.005959327332675457 | Val. Loss: 0.008677099831402302\n",
            "Epoch: 983 | Loss: 0.012463211081922054 | Val. Loss: 0.008534232154488564\n",
            "Epoch: 984 | Loss: 0.010159957222640514 | Val. Loss: 0.008483687415719032\n",
            "Epoch: 985 | Loss: 0.00387275917455554 | Val. Loss: 0.008387060835957527\n",
            "Epoch: 986 | Loss: 0.007184543646872044 | Val. Loss: 0.008408503606915474\n",
            "Epoch: 987 | Loss: 0.002368827350437641 | Val. Loss: 0.008367556147277355\n",
            "Epoch: 988 | Loss: 0.005275430157780647 | Val. Loss: 0.008269970305263996\n",
            "Epoch: 989 | Loss: 0.015501655638217926 | Val. Loss: 0.008118522353470325\n",
            "Epoch: 990 | Loss: 0.005415666848421097 | Val. Loss: 0.008034543134272099\n",
            "Epoch: 991 | Loss: 0.004597675986588001 | Val. Loss: 0.008185931481420994\n",
            "Epoch: 992 | Loss: 0.003642213996499777 | Val. Loss: 0.008078273385763168\n",
            "Epoch: 993 | Loss: 0.006578208412975073 | Val. Loss: 0.008055528625845909\n",
            "Epoch: 994 | Loss: 0.009223060682415962 | Val. Loss: 0.008021744899451733\n",
            "Epoch: 995 | Loss: 0.0024963465984910727 | Val. Loss: 0.007848708890378475\n",
            "Epoch: 996 | Loss: 0.005505397450178862 | Val. Loss: 0.007896524854004383\n",
            "Epoch: 997 | Loss: 0.016101090237498283 | Val. Loss: 0.007794289384037256\n",
            "Epoch: 998 | Loss: 0.002813743194565177 | Val. Loss: 0.0077965883538126945\n",
            "Epoch: 999 | Loss: 0.011667708866298199 | Val. Loss: 0.007785686291754246\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Loss')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGyCAYAAAAYveVYAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAbbZJREFUeJzt3Xd0VGXixvHvnUknJKGlQSD0Ir0KWECiiIhiZdUV7BUVWRs/FCyr2EVdFDu6FhC7wlIVpAnSew+EkoSaXmfm/v4IGTIkgQRmMinP55yck7nz3nvfuUDy8FbDNE0TERERkWrC4u0KiIiIiLiTwo2IiIhUKwo3IiIiUq0o3IiIiEi1onAjIiIi1YrCjYiIiFQrCjciIiJSrSjciIiISLWicCMiIiLVio+3K1DRHA4HBw8epHbt2hiG4e3qiIiISBmYpkl6ejrR0dFYLGdomzG9aOHCheaVV15pRkVFmYD5448/lvncxYsXm1ar1ezUqVO57rlv3z4T0Je+9KUvfelLX1Xwa9++fWf8Xe/VlpvMzEw6derEHXfcwbXXXlvm81JSUhg+fDgDBgwgOTm5XPesXbs2APv27SMkJKRc54qIiIh3pKWlERMT4/w9fjpeDTeDBg1i0KBB5T7vvvvu4+abb8ZqtfLTTz+V69zCrqiQkBCFGxERkSqmLENKqtyA4s8++4zdu3czfvz4MpXPzc0lLS3N5UtERESqryoVbnbs2MFTTz3Fl19+iY9P2RqdJkyYQGhoqPMrJibGw7UUERERb6oy4cZut3PzzTfz3HPP0apVqzKfN2bMGFJTU51f+/bt82AtRURExNuqzFTw9PR0Vq5cyZo1axg5ciRQMK3bNE18fHyYM2cOl1xySbHz/P398ff3r+jqiohIJWG328nPz/d2NaQM/Pz8zjzNuwyqTLgJCQlhw4YNLsfee+89fv/9d7777juaNm3qpZqJiEhlZJomSUlJpKSkeLsqUkYWi4WmTZvi5+d3TtfxarjJyMhg586dztfx8fGsXbuWunXr0rhxY8aMGcOBAwf44osvsFgstG/f3uX88PBwAgICih0XEREpDDbh4eEEBQVp4dZKrnCR3cTERBo3bnxOf15eDTcrV66kf//+ztejR48GYMSIEUyZMoXExEQSEhK8VT0REami7Ha7M9jUq1fP29WRMmrQoAEHDx7EZrPh6+t71tcxTNM03VivSi8tLY3Q0FBSU1O1zo2ISDWVk5NDfHw8sbGxBAYGers6UkbZ2dns2bOHpk2bEhAQ4PJeeX5/V5nZUiIiIuWlrqiqxV1/Xgo3IiIiUq0o3IiIiEi1onAjIiJSidx2220MHTrU29Wo0hRu3Cw7z+7tKoiIiNRoCjduNP7njbQdN4u1+1K8XRUREamGFi5cSM+ePfH39ycqKoqnnnoKm83mfP+7776jQ4cOBAYGUq9ePeLi4sjMzARgwYIF9OzZk1q1ahEWFkbfvn3Zu3evtz6KR1WZFYqrgs+XFfwleWvudj6/o6eXayMiIkWZpkl2vnda1wN9rec8E+jAgQNcccUV3HbbbXzxxRds3bqVu+++m4CAAJ599lkSExO56aabePXVV7nmmmtIT09n0aJFmKaJzWZj6NCh3H333XzzzTfk5eWxYsWKajubTOFGRERqhOx8O+3GzfbKvTc/P5Agv3P7lfvee+8RExPDf/7zHwzDoE2bNhw8eJAnn3yScePGkZiYiM1m49prr6VJkyYAdOjQAYBjx46RmprKlVdeSfPmzQFo27btuX2oSkzdUiIiIlXAli1b6N27t0trS9++fcnIyGD//v106tSJAQMG0KFDB2644QY++ugjjh8/DkDdunW57bbbGDhwIEOGDOHtt98mMTHRWx/F49RyIyIiNUKgr5XNzw/02r09zWq1MnfuXJYuXcqcOXN49913GTt2LMuXL6dp06Z89tlnPPzww8yaNYtp06bx9NNPM3fuXM4//3yP162iqeVGRERqBMMwCPLz8cqXO8a2tG3blmXLllF016QlS5ZQu3ZtGjVq5PyMffv25bnnnmPNmjX4+fnx448/Ost36dKFMWPGsHTpUtq3b8/XX399zvWqjNRyIyIiUsmkpqaydu1al2P33HMPEydO5KGHHmLkyJFs27aN8ePHM3r0aCwWC8uXL2f+/PlcdtllhIeHs3z5cg4fPkzbtm2Jj4/nww8/5KqrriI6Oppt27axY8cOhg8f7p0P6GEKNyIiIpXMggUL6NKli8uxO++8k5kzZ/L444/TqVMn6taty5133snTTz8NQEhICH/++ScTJ04kLS2NJk2a8MYbbzBo0CCSk5PZunUrn3/+OUePHiUqKooHH3yQe++91xsfz+MUbjygms6sExGRCjBlyhSmTJlS6vsrVqwo8Xjbtm2ZNWtWie9FRES4dE9Vdxpz4wFFukNFRESkginciIiISLWicCMiIiLVisKNiIiIVCsKNyIiIlKtKNyIiIhItaJw4wGaCi4iIuI9CjciIiJSrSjceIDWuREREfEehRsREZFqpl+/fowaNcrb1fAahRsREZFKYsiQIVx++eUlvrdo0SIMw2D9+vUeubdhGPz0008euXZFU7gRERGpJO68807mzp3L/v37i7332Wef0b17dzp27OiFmlUtCjcekJ6T7+0qiIhIFXTllVfSoEGDYhtnZmRkMH36dO68806OHj3KTTfdRMOGDQkKCqJDhw588803Hq2Xw+Hg+eefp1GjRvj7+9O5c2eXTTrz8vIYOXIkUVFRBAQE0KRJEyZMmACAaZo8++yzNG7cGH9/f6Kjo3n44Yc9Wl/tCu4BB1NyvF0FERE5lWmCzUs/n30CyrROiI+PD8OHD2fKlCmMHTsW48Q506dPx263c9NNN5GRkUG3bt148sknCQkJYcaMGdx66600b96cnj17eqT6b7/9Nm+88QYffPABXbp04dNPP+Wqq65i06ZNtGzZknfeeYdffvmFb7/9lsaNG7Nv3z727dsHwPfff89bb73F1KlTOe+880hKSmLdunUeqWchhRsPMNF0KRGRSseWA5+WPJ7F4+6YBb6BZSt6xx289tprLFy4kH79+gEFXVLXXXcdoaGhhIaG8thjjznLP/TQQ8yePZtvv/3WY+Hm9ddf58knn+Qf//gHAK+88gp//PEHEydOZNKkSSQkJNCyZUsuuOACDMOgSZMmznMTEhKIjIwkLi4OX19fGjdu7LF6FlK3lAdoKriIiJytNm3a0KdPHz799FMAdu7cyaJFi7jzzjsBsNvtvPDCC3To0IG6desSHBzM7NmzSUhI8Eh90tLSOHjwIH379nU53rdvX7Zs2QLAbbfdxtq1a2ndujUPP/wwc+bMcZa74YYbyM7OplmzZtx99938+OOP2Gw2j9S1kFpuPEDZRkSkEvIJKGhB8da9y+HOO+/koYceYtKkSXz22Wc0b96ciy++GIDXXnuNt99+m4kTJ9KhQwdq1arFqFGjyMvL80TNy6Rr167Ex8fzv//9j3nz5nHjjTcSFxfHd999R0xMDNu2bWPevHnMnTuXBx54wNky5evr65H6qOXGA0w13YiIVD6GUdA15I2vcu7Lc+ONN2KxWPj666/54osvuOOOO5zjb5YsWcLVV1/NP//5Tzp16kSzZs3Yvn27J54YACEhIURHR7NkyRKX40uWLKFdu3Yu5YYNG8ZHH33EtGnT+P777zl27BgAgYGBDBkyhHfeeYcFCxawbNkyNmzY4LE6q+VGRESkkgkODmbYsGGMGTOGtLQ0brvtNud7LVu25LvvvmPp0qXUqVOHN998k+TkZJegcaoxY8Zw4MABvvjii9PeNz4+nrVr17oca9myJY8//jjjx4+nefPmdO7cmc8++4y1a9fy1VdfAfDmm28SFRVFly5dsFgsTJ8+ncjISMLCwpgyZQp2u51evXoRFBTEl19+SWBgoMu4HHdTuPEANdyIiMi5uvPOO/nkk0+44ooriI6Odh5/+umn2b17NwMHDiQoKIh77rmHoUOHkpqaWuq1EhMTyzQmZ/To0cWOLVq0iIcffpjU1FT+9a9/cejQIdq1a8cvv/xCy5YtAahduzavvvoqO3bswGq10qNHD2bOnInFYiEsLIyXX36Z0aNHY7fb6dChA7/++iv16tU7i6dSNoZZw/pQ0tLSCA0NJTU1lZCQELdeO/apGQDUq+XHqmcudeu1RUSk7HJycoiPj6dp06YEBJRvvIt4z+n+3Mrz+1tjbjygRqVFERGRSkbhxk1sdofz+xrWGCYiIlKpKNy4yfyth5zfK9qIiIh4j8KNm+Tk253fq+FGRETEe7wabv7880+GDBlCdHR0mbZa/+GHH7j00ktp0KABISEh9O7dm9mzZ1dMZctB3VIiIpWDfh5XLe768/JquMnMzKRTp05MmjSpTOX//PNPLr30UmbOnMmqVavo378/Q4YMYc2aNR6u6ZkV/fPQPyUREe8qXPk2KyvLyzWR8ihcZdlqtZ7Tdby6zs2gQYMYNGhQmctPnDjR5fVLL73Ezz//zK+//kqXLl3cXLvycdksU+lGRMSrrFYrYWFhHDpUMB4yKCjIucKvVE4Oh4PDhw8TFBSEj8+5xZMqvYifw+EgPT2dunXrllomNzeX3Nxc5+u0tDSP1EUtnyIilUtkZCSAM+BI5WexWGjcuPE5B9EqHW5ef/11MjIyuPHGG0stM2HCBJ577rkKrJUabkREKgPDMIiKiiI8PJz8/HxvV0fKwM/PD4vl3EfMVNlw8/XXX/Pcc8/x888/Ex4eXmq5MWPGuCwnnZaWRkxMjNvr4zLmRs04IiKVhtVqPecxHFK1VMlwM3XqVO666y6mT59OXFzcacv6+/vj7+/v8TopzoiIiFQOVW6dm2+++Ybbb7+db775hsGDB3u7OiVS0BEREfEer7bcZGRksHPnTufrwq3W69atS+PGjYtt0f71118zYsQI3n77bXr16kVSUhIAgYGBhIaGeuUzFCraFeVQt5SIiIjXeLXlZuXKlXTp0sU5jXv06NF06dKFcePGAcW3aP/www+x2Ww8+OCDREVFOb8eeeQRr9S/qKJxJiffUWo5ERER8Syvttz069fvtINvp0yZ4vJ6wYIFnq3QuVBjjYiISKVQ5cbcVFbtokO8XQURERFB4cZtAnw1zVBERKQyULgRERGRakXhxk20ZYmIiEjloHAjIiIi1YrCjYiIiFQrCjciIiJSrSjciIiISLWicOMmGk8sIiJSOSjciIiISLWicOMmhuaCi4iIVAoKNyIiIlKtKNy4idptREREKgeFGzexpu7hRZ9PeMxnmrerIiIiUqP5eLsC1YWRn00Hy26SzTreroqIiEiNppYbNzEsBTnRisPLNREREanZFG7cxDQKHqXCjYiIiHcp3LiJYS1oubEo3IiIiHiVwo2bmIYVUMuNiIiItyncuIlhORFuDIUbERERb1K4cRONuREREakcFG7cRN1SIiIilYPCjZucnApuB0zvVkZERKQGU7hxk8KWGwBD4UZERMRrFG7cpHBAMYCPuqZERES8RuHGXSwnH6XWuhEREfEehRs3KdotpUHFIiIi3qNw4y6Wk3uQKtyIiIh4j8KNuxgWTAxA3VIiIiLepHDjRo4Tj/PuvjFeromIiEjNpXDjRvYTjzPUX49VRETEW/Rb2E0M42S4MUx1S4mIiHiLwo0b2c3CcGP3ck1ERERqLoUbNzEMw9lyY0HhRkRExFsUbtyocEDx8YwcL9dERESk5lK4caPClpsfVu31ck1ERERqLoUbNyoMN1rET0RExHsUbtzEAOwUbMGgcCMiIuI9CjduZNcKxSIiIl7n1XDz559/MmTIEKKjozEMg59++umM5yxYsICuXbvi7+9PixYtmDJlisfrWVYOdUuJiIh4nVfDTWZmJp06dWLSpEllKh8fH8/gwYPp378/a9euZdSoUdx1113Mnj3bwzU9M8MAm7qlREREvM7nzEU8Z9CgQQwaNKjM5SdPnkzTpk154403AGjbti2LFy/mrbfeYuDAgZ6qZpmp5UZERMT7qtSYm2XLlhEXF+dybODAgSxbtqzUc3Jzc0lLS3P58pTCFYp9DC3iJyIi4i1VKtwkJSURERHhciwiIoK0tDSys7NLPGfChAmEhoY6v2JiPLdjt825QrFabkRERLylSoWbszFmzBhSU1OdX/v27fPIfQwMdUuJiIhUAl4dc1NekZGRJCcnuxxLTk4mJCSEwMDAEs/x9/fH39+/IqpXZG8phRsRERFvqVItN71792b+/Pkux+bOnUvv3r29VKOTDOPkgGIfhRsRERGv8Wq4ycjIYO3ataxduxYomOq9du1aEhISgIIupeHDhzvL33fffezevZsnnniCrVu38t577/Htt9/y6KOPeqP6xWj7BREREe/zarhZuXIlXbp0oUuXLgCMHj2aLl26MG7cOAASExOdQQegadOmzJgxg7lz59KpUyfeeOMNPv7440oxDRxOrnOjbikRERHv8eqYm379+mGaZqnvl7T6cL9+/VizZo0Ha3X2NKBYRETE+6rUmJvKTgOKRUREvE/hxk0Mig4o1iJ+IiIi3qJw40aFKxRbDbXciIiIeIvCjbsYJ1co1pgbERER71G4cSO7dgUXERHxOoUbNzEwNKBYRESkElC4cRMTUwOKRUREKgGFGzcqOuYmJ18BR0RExBsUbtzEwMB2Yk1EX+xMnLfDyzUSERGpmRRu3Cj/xIBiH2ws3XXEy7URERGpmRRu3MQwTq5z42OoS0pERMRbFG7cqLBbSgOKRUREvEfhxk0MinZL2TG8Wx0REZEaS+HGTSyGge1EuPFVy42IiIjXKNy4SZ1afs6WG6vCjYiIiNco3LiRvUjLjenluoiIiNRUCjduZCsy5iYlK9/LtREREamZFG7cKN880S2lqeAiIiJeo3DjRkVXKHaY6pgSERHxBoUbNyo6FVxERES8Q+HGjYoOKDa00I2IiIhXKNy4UdG9pURERMQ7FG7cyOZc58bh5ZqIiIjUXAo3bmQzi6xzo/HEIiIiXqFw40Z9W0cC4GPYSM9R15SIiIg3KNy40UWto4CClpvUbC3iJyIi4g0KN27kMArWubHgwNC4GxEREa9QuHEj0+rj/N4HB7sPZ3ixNiIiIjWTwo0bmYav83srdi59608v1kZERKRmUrhxI39/f+f3vtixOzRlSkREpKIp3LjRpedF4aBgaWIt5CciIuIdCjdu5GO1OLdg0P5SIiIi3qFw42a2IvtLiYiISMVTuHGz/BOrFPsYCjciIiLeoHDjZnkUzJjyQ4v4iYiIeIPCjZsVhht/hRsRERGvULhxs1yFGxEREa9SuHGzXPNEuDEUbkRERLzB6+Fm0qRJxMbGEhAQQK9evVixYsVpy0+cOJHWrVsTGBhITEwMjz76KDk5ORVU2zPLdY650To3IiIi3uDVcDNt2jRGjx7N+PHjWb16NZ06dWLgwIEcOnSoxPJff/01Tz31FOPHj2fLli188sknTJs2jf/7v/+r4JqX7uSYmzwv10RERKRm8mq4efPNN7n77ru5/fbbadeuHZMnTyYoKIhPP/20xPJLly6lb9++3HzzzcTGxnLZZZdx0003nbG1pyKp5UZERMS7vBZu8vLyWLVqFXFxcScrY7EQFxfHsmXLSjynT58+rFq1yhlmdu/ezcyZM7niiisqpM5lkUfBzuCaCi4iIuIdPt668ZEjR7Db7URERLgcj4iIYOvWrSWec/PNN3PkyBEuuOACTNPEZrNx3333nbZbKjc3l9zcXOfrtLQ093yA0u5n+gEaUCwiIuItXh9QXB4LFizgpZde4r333mP16tX88MMPzJgxgxdeeKHUcyZMmEBoaKjzKyYmxqN1LGy50VRwERER7/Bay039+vWxWq0kJye7HE9OTiYyMrLEc5555hluvfVW7rrrLgA6dOhAZmYm99xzD2PHjsViKZ7VxowZw+jRo52v09LSPBpwtM6NiIiId3mt5cbPz49u3boxf/585zGHw8H8+fPp3bt3iedkZWUVCzBWa8FeTqZplniOv78/ISEhLl+elKvtF0RERLzKay03AKNHj2bEiBF0796dnj17MnHiRDIzM7n99tsBGD58OA0bNmTChAkADBkyhDfffJMuXbrQq1cvdu7cyTPPPMOQIUOcIcfbtIifiIiId3k13AwbNozDhw8zbtw4kpKS6Ny5M7NmzXIOMk5ISHBpqXn66acxDIOnn36aAwcO0KBBA4YMGcKLL77orY9QjKaCi4iIeJdhltafU02lpaURGhpKamqqR7qobv+/f/Ooz3escbRgvO129rw82O33EBERqWnK8/u7Ss2WqgrUciMiIuJdCjdulmeemAquMTciIiJeoXDjZtn4AxBI7hlKioiIiCco3LhZFgEABCnciIiIeIXCjZtln9h+IdBQuBEREfEGhRs3yzrRLRVAHgYOL9dGRESk5lG4cbPsE91SAIHkebEmIiIiNZPCjZvl44ONgtWSNe5GRESk4inceEC2eWLGlJHj5ZqIiIjUPAo3HlA47kYtNyIiIhVP4cYDNB1cRETEexRuPCDL2S2VSw3buktERMTrFG48IIeCtW6CyOX/ftzg5dqIiIjULGcVbvbt28f+/fudr1esWMGoUaP48MMP3VaxqqzomJtvVuzzcm1ERERqlrMKNzfffDN//PEHAElJSVx66aWsWLGCsWPH8vzzz7u1glVRplkw5qaWke3lmoiIiNQ8ZxVuNm7cSM+ePQH49ttvad++PUuXLuWrr75iypQp7qxflZROEADBKNyIiIhUtLMKN/n5+fj7F3S9zJs3j6uuugqANm3akJiY6L7aVVHpZkG4qW1kebkmIiIiNc9ZhZvzzjuPyZMns2jRIubOncvll18OwMGDB6lXr55bK1gVZRAIQG213IiIiFS4swo3r7zyCh988AH9+vXjpptuolOnTgD88ssvzu6qmiztRMtNiFpuREREKpzP2ZzUr18/jhw5QlpaGnXq1HEev+eeewgKCnJb5aqqwjE3tVG4ERERqWhn1XKTnZ1Nbm6uM9js3buXiRMnsm3bNsLDw91awaroZLgp6JbSQn4iIiIV56zCzdVXX80XX3wBQEpKCr169eKNN95g6NChvP/++26tYFWUbhaMuallZGPgIM/u8HKNREREao6zCjerV6/mwgsvBOC7774jIiKCvXv38sUXX/DOO++4tYJVUWHLDRS03qTn2LxYGxERkZrlrMJNVlYWtWvXBmDOnDlce+21WCwWzj//fPbu3evWClY1jwxoiQOLc/PM2mTx8DdrvFwrERGRmuOswk2LFi346aef2LdvH7Nnz+ayyy4D4NChQ4SEhLi1glXNo5e2Ak52TdU2slm666g3qyQiIlKjnFW4GTduHI899hixsbH07NmT3r17AwWtOF26dHFrBauqVIIBCCXTyzURERGpWc5qKvj111/PBRdcQGJionONG4ABAwZwzTXXuK1yVVmKWQsMCDPSQZOlREREKsxZhRuAyMhIIiMjnbuDN2rUSAv4FXHcLBiTFGao5UZERKQinVW3lMPh4Pnnnyc0NJQmTZrQpEkTwsLCeOGFF3A4NO0ZIJVaAISR4eWaiIiI1Cxn1XIzduxYPvnkE15++WX69u0LwOLFi3n22WfJycnhxRdfdGslq6LClps6RkG4+WxJPBbDYESfWC/WSkREpPo7q3Dz+eef8/HHHzt3Awfo2LEjDRs25IEHHlC4AVJOtNyEnmi5ee7XzQBc360RtfzPujdQREREzuCsuqWOHTtGmzZtih1v06YNx44dO+dKVQcpZsFsqcKWm0I2u0YXi4iIeNJZhZtOnTrxn//8p9jx//znP3Ts2PGcK1UdnAw36V6uiYiISM1yVv0jr776KoMHD2bevHnONW6WLVvGvn37mDlzplsrWFWlnFjnJpBcfLGRf/YT00RERKQczqrl5uKLL2b79u1cc801pKSkkJKSwrXXXsumTZv473//6+46VkmZBGDDCkAYar0RERGpKGfdnBAdHV1s4PC6dev45JNP+PDDD8+5YlVZ72b1WLb7KClmMPWNVMKMTA6bdbxdLRERkRrhrFpu5PT6t2kAwPETXVN11HIjIiJSYRRuPCj1xKDiMEML+YmIiFQUhRsPMDCAkzOmQrUFg4iISIUp15iba6+99rTvp6SklLsCkyZN4rXXXiMpKYlOnTrx7rvvnnaPqpSUFMaOHcsPP/zAsWPHaNKkCRMnTuSKK64o9709xSjINuqWEhER8YJyhZvQ0NAzvj98+PAyX2/atGmMHj2ayZMn06tXLyZOnMjAgQPZtm0b4eHhxcrn5eVx6aWXEh4eznfffUfDhg3Zu3cvYWFh5fkYFeZkt5RabkRERCpKucLNZ5995tabv/nmm9x9993cfvvtAEyePJkZM2bw6aef8tRTTxUr/+mnn3Ls2DGWLl2Kr68vALGxsW6tkzsVttxoKriIiEjF8dqYm7y8PFatWkVcXNzJylgsxMXFsWzZshLP+eWXX+jduzcPPvggERERtG/fnpdeegm73V5R1S6XFLXciIiIVDivLZt75MgR7HY7ERERLscjIiLYunVriefs3r2b33//nVtuuYWZM2eyc+dOHnjgAfLz8xk/fnyJ5+Tm5pKbm+t8nZaW5r4PUQrjxKCbo2YIAPWMNMCEEwONRURExHOq1Gwph8NBeHg4H374Id26dWPYsGGMHTuWyZMnl3rOhAkTCA0NdX7FxMR4vJ7WExnmKAXhJpBcapHj8fuKiIiIF8NN/fr1sVqtJCcnuxxPTk4mMjKyxHOioqJo1aoVVqvVeaxt27YkJSWRl5dX4jljxowhNTXV+bVv3z73fYhSXNiqYBG/XPxIM4MAaGCkePy+IiIi4sVw4+fnR7du3Zg/f77zmMPhYP78+c7NOE/Vt29fdu7cicPhcB7bvn07UVFR+Pn5lXiOv78/ISEhLl+e5mc9+VgPEwZAA1I9fl8RERHxcrfU6NGj+eijj/j888/ZsmUL999/P5mZmc7ZU8OHD2fMmDHO8vfffz/Hjh3jkUceYfv27cyYMYOXXnqJBx980Fsf4YwOm2GAWm5EREQqitcGFAMMGzaMw4cPM27cOJKSkujcuTOzZs1yDjJOSEjAYjmZv2JiYpg9ezaPPvooHTt2pGHDhjzyyCM8+eST3voIZ3TYLFgbKFzhRkREpEJ4NdwAjBw5kpEjR5b43oIFC4od6927N3/99ZeHa+U+ySd2A480jnu5JiIiIjVDlZotVRUlmvUAiDKOerkmIiIiNYPCjYclUheASOMYBWvdiIiIiCcp3HhYslkXE4NAcglFKxWLiIh4msKNh+Xjw5ETg4qjjGNapFhERMTDFG4qQJJZ2DV1lMU7jni5NiIiItWbwo0HhAT4urw+eGJQcbRxjAe/Xs2dU/7GNDX+RkRExBMUbjwgNMiXL+7o6XxdtOUGYP7WQyzbpdlTIiIinqBw4yEXtWpA18ZhABzkZMtNoYxcmzeqJSIiUu0p3HhQVGggUKTlBrXWiIiIeJrCjQeZJ9a1STwRbkKMLGqR7c0qiYiIVHsKNx5UOGY4B39SzGCgcDE/ERER8RSFGw8qOiGqsGsq6kS4iT+iBf1EREQ8QeHGg8wi2y0k4rrH1IT/bfVKnURERKo7hRsPKtpyUzjuJgp1S4mIiHiSwo0HFV2mr3B38Mgiu4Pf+sly7A4t5iciIuJOCjceVGLLTZEBxYt2HOGv3RUzPXzfsSzmbErSysgiIlLtKdx4VJExNydabuoZafiT5zxuK6HlJivPxj1frOTHNfvdVpMLX/2De/67ijmbk912TRERkcpI4caDijaSpBNIplmwqN+ZpoN/siieOZuTeXTaOrfX6e94jfkREZHqTeHGg7rF1inyymC/WR+AxsahIkeLO56V79mKiYiIVGMKNx501wXNeHxga+frPWYkAE0MdQ2JiIh4isKNB/n5WHiwfwvn68JwE2skeatKIiIi1Z7CTQXaY0YAZw43JprRJCIicrYUbirQ3hPhJtxIIZAcAGwOR4VOz1ZsEhGR6k7hpgJlEMRRMwSAJicGFd8xZSUjv1njUs4oMsw4I9dWIXXbcySTjxftJjvPXiH3ExER8RSFmwpW0ribGesTSy3//K+bPF4ngH6vL+DfM7bwxpxtFXI/ERERT1G4qWB7yzDupuiYm6W7ClYwNk2TfceyzrkLq6Sp50X9vff4OV1fRETE2xRuKljhoOJTp4NP+zsBm91R6nkfLdrNha/+wb9nbPFo/URERKo6hZsKtvdEt1RTSxJFh/c++f0GPl+2t9TzXpq5FYBPFsef0/01oFhERKo7hZsKtt9sgAMLQeQQTorLeyVtjVDR+1yeqdtKRESkslO4qWD5+LDLjAKgrSWhxDK7D2dWZJVERESqFYWbCtAuKsTl9WZHEwDaGK7hxjjRbLJw++Fix0RERKRsFG4qwM8j+7q83mo2BqCtpfQxNoUqultKRESkqlO4qQC+Vguj4lo6X291FISbWCOJAHKdx3NtxWdLHUjJZvfhjHLf0zRNPl0cz6q9xcfxnM725PRy30tERKQyUbipIFd2jHJ+f5RQDplhWDBpbexzHv996yE+LWE21CVvLCz3/eZsTub53zZz3fvLynVellYoFhGRKk7hpoK0CK/t8vpk15TruJvnf9vslvtpULKIiNRUCjdesuXEoOK2xpnH3YiIiEjZKdx4yUYzFoDzLHvxI/+crnUgJZsjGbkux0wt1yciIjWUwo2X7DUjOGKG4kc+HYzdZ32dFfHH6Pvy73T/97xSyzgcCjoiIlJzKNx4jcFKR2sAelrKtxN30VaaGz8484DhouN4yjq1fE3CcX5ac6Bc9RIREakMKkW4mTRpErGxsQQEBNCrVy9WrFhRpvOmTp2KYRgMHTrUsxX0kBUnwk13yzbKs+vT6VppSjJl6Z5ylQe45r2ljJq2ltUJ2iVcRESqFq+Hm2nTpjF69GjGjx/P6tWr6dSpEwMHDuTQoUOnPW/Pnj089thjXHjhhRVUU/fbYDYjHx8aGCnFdgn3lPKueLzniGZdiYhI1eL1cPPmm29y9913c/vtt9OuXTsmT55MUFAQn376aann2O12brnlFp577jmaNWtWgbV1r1z8WO8oqH+PcnZNfbFsDwu2uQbAX9YddH6vlY1FRKSm8mq4ycvLY9WqVcTFxTmPWSwW4uLiWLas9LEkzz//POHh4dx5551nvEdubi5paWkuX5XJCkcbALob5Qs3437exG2f/e1y7OFv1nD0lFlTIiIiNY1Xw82RI0ew2+1ERES4HI+IiCApKanEcxYvXswnn3zCRx99VKZ7TJgwgdDQUOdXTEzMOdfbnQoHFbexJFCbrHO+Xkau7ZyvUVR5WoBWxB9j1sZEt95fRESkvLzeLVUe6enp3HrrrXz00UfUr1+/TOeMGTOG1NRU59e+ffvOfFIFOkwYCWYEFky6Wnac8/UueWMhb84pvRXoeGYe01fuI7NICPp7z5n3n8q12bni7UWM+WFDqWVu/GAZ9325mr1HNU5HRES8x6vhpn79+litVpKTXQfTJicnExkZWaz8rl272LNnD0OGDMHHxwcfHx+++OILfvnlF3x8fNi1a1exc/z9/QkJCXH58pbRl7Yq8XjhrKkelq3nfA+7w+Sd33eW+v4Paw7w+Hfrefqnjc5jX/5V+irJhQOQf99yiM2JaXyzIqHEct+t2u/8PjE1p5y1FhERcR+vhhs/Pz+6devG/PnzncccDgfz58+nd+/excq3adOGDRs2sHbtWufXVVddRf/+/Vm7dm2l63I61cMDWpZ4vLBrqquxAwvFdwb3hBnry9d9dLp1AFcnHOex6eucrzWYWUREvMnH2xUYPXo0I0aMoHv37vTs2ZOJEyeSmZnJ7bffDsDw4cNp2LAhEyZMICAggPbt27ucHxYWBlDseFWy1WxMhhlIsJFNW2Mvm8ymHr+nO7dnSDh67mOFRERE3MXr4WbYsGEcPnyYcePGkZSUROfOnZk1a5ZzkHFCQgIWS5UaGlRuDiwsd7RlgHU1F1o2sMnu+XCTbzfJyLUR7H/6vwKFrTBFw9CiHYe5sGUDT1ZPRETkrHk93ACMHDmSkSNHlvjeggULTnvulClT3F8hL1jqOI8B1tX0sGxjst0Eyrna3ln4YfV+hveOLfd5t36ygj0vD3Z/hURERNygejeJVCHrvbBacVmUd0VjKFuXl6mBOSIi4iEKN5VELn6scbQAoL9lrXcrUwLjNC1J5R2/M+mPnfSe8DsHU7LPtVoiIiLFKNxUIvMdXQG4xLIGK/Zzupa7WkZKGnNz5pNO//Zrs7eRlJbDm3O3n33FRERESqFwU4mscLQhxQwmzMig5zmuefP6nDMHh7mbkzmemVem683aWPKK0XD6Vp3TcahrSkREPEDhpoJ1bRxW6nt2rM7Wm8ssKz1el0U7jnD1pCVsPJBaapnCMTe/lXNdHBEREW9RuKlgH4/ocdr359q7AdDVsoMGpHi8PgnHsth1uPzbJfzn97JvFZFrs3Pvf1cWW914W1I6ebaKWbRQRERqDoWbCla3lt9p3z9IfTY6mmJgEmddVUG1Kt3ny/ayZOeRYseLdnudOh7n1M6maX/vY/am5GL7Um06mMbdX3i+hUpERGoWhZtKaLajoHUnzrIao4K2YyiqaGvKun0p3PLx8hLLjflhA0t2HmHBtsPF3juakev8Pi07v9R7Ldxe/FwREZFzUSkW8RNXSx3nca8ZSAMjhW7GdlaabSr0/rd9tqJM5b5ZkVDiRpqfLdnDvC3J9G5Wj+NZecTUDTrjteKPZBJTJxAfq/K2iIicG/0mqYTy8XEOLL7T53/4YKvQ+y/ddfSczp+3pWARwmW7j7I1KZ25m0+/KOEv6w7S//UF3OWmLqr3Fuzkgld+JzlNu5OLiNRECjeV1FR7f1LMYBoaR7jYsu7MJ1RhnyyOByixe+tsvDprG/uPZzNxXunT4U3T5MGvVvPqrHObci8iIpWPwo0XXNIm/IxlMgnkZ3tfAG63ziKUDE9Xq9qxO0pfR2d1wnFmbEjkvQW7KrBGIiJSERRuvOC+i5uXqdwvjj7sMSMJMbIYYZ3j4VpVjKkljNHxhlxNQRcRqbYUbrygZ9O6/DKy7xnL5ePDe7arAYizrqKLUfa1ZSqrp06ZDi4iIuJuCjde0rFRWJnKbTUbOxf2G+P7dYUs7FddHM8qfQq6iIhUXwo3VcBk+1VsdTQmgDxG+XyPbwXPnvK0bUlpxY4dy8wjJ//cNg890yytssrOs+M4zfgdERGpXBRuqoB8fPiPfSjZ+NPBspvRPtO9srifp+Tku36Woxm5dH1hLj3+Pa9c15m5IZFxP290Z9U4nJ5L23Gz+MeHf7n1uiIi4jkKN15UJ8i3zGUTzAheyr8ZO1b6Wjbys98ztDEqx+Bcd1u59zgA6bnla6F64KvVfLFsr1vrMmtTwW7oK/Ycc+t1RUTEcxRuqpB1ZgvesN3gfP2q7wf8y+dbQsik+I5OVdOkP3Zy73/PvKfWZ0vimblBO5WLiEhx2n6hilns6MDR/BCe8vmGOkY6F1vWcbHfOjY4mvGKbRhpBHu7iufktdnbzlhmW1I6z/26GYA9Lw/2dJVERKSKUbipgraYTbgt/wkusGzkLutM6hjpdLDs5ku/CeTjw2pHS+bYu7PSbIVZDRvnim7K6Q5/bj/MRa0alPymWT1axEREahKFGy8yDOOszzWxsMjRkb8drRlkWcEN1oUEG9n4YqOXZQu9LFtwYBBvRrHK0YrDZhirHK04QqgbP4Hn5eTbOZ6Vh6/VQv1gf8D9HXDDP12hFiARkWpE4caLTDe0CuTgz4+OC/nJ0ZdGxmEusawl1kiinWUvgeTS3DhIc+tBZ/m/HW1Y5WjJPjOcDWZT4OwDVkV4ffY2Pj6x91RhAPFEY8rYHzfQp3l9BneMcn3jHAJoZWKzO1i04whdm9QhNLDsA9lFRKoihZtqwsTCPjOCz+0DAfAjn47GLuoZ6Zxv2Uw3S8Emkj0sW+lhKdgs8oBZn1x8WelozUx7L44R4rX6l+b3bYcq5D5fLU/gq+UJDO5YPVtwJi/cxetzttM2KoT/PXKht6sjIuJRCjfVVB6+rDTbgAmzHT0Ak67GDvpZ11KbbDpY4mloHAGgmTWRG60LSDGDOUoIWxyNWeDoTBgZrDDb4M3WnZLubFbkzLAizUSPT1/HzsMZTL+3Nz7WqjWW6cc1BwDYklh8wUQRkepG4caLzmXMzVncjdVmK1bbWgFQhzS6WnbQ0RLPxZZ1WHAQZmQQRgbNrQe50lqwaN1RM4SNZlPWOFqw1HEeOfhXYJ1dd/aetTGJy9tHcjjdvQOKi8rJtxPgay3xvemr9gMFa970aV7fY3UQEZFzo3DjRe4Yc3O2jhPCfEc35ju68R5XEW4cp7Oxi86WXc5uK4B6RhoXG+u42LKOUXxPghmBHQuHzVD+dHRkg6Mpxz3YnbXnaJbz+/u+XMXCx/vx0swtZT7f4TCxWIqHSKOU1qgPFu7mkbiWgPtnZYmISMVQuBFy8WOfGcE+M4JfHX0IIJdwI4UwMrjAsoEGRiqNjMNEGMdpbBTs19TUSKTniRC0wtGGbWYMs+w9SSfIo3VNTM0p14aYj367lrf/0YWUrDyW7jrKgLbh+PtYS+3amr81mf9tTCQsyJe/dh8jyK94K46lhBa30kKUiIhUPIUbL6rYbqmyy8GfBDOCBCJYb28OgIGDLsZOGhpHuNCygTaWk1s/9LRspSdbudn6O4sd7fnAdiUZHgo5N3/0F0X3sJy/JZkXZ27hrRs7l1j+57UHefsfXbjl4+VsOpjG3Rc2ZezgdqVef/3+VJfXWXnFN+/8Y9shOseEObuv3luwkw8W7ub7+/vQIrxqL6IoIlIdKNxUEgG+lmIbSFYmJpaCMTtmK3519AGgLmn0sWyii2UnLY39hBkZXGxZRx+/Tcywn8/n9suwU/L4lbN16ubcd36+EoDbp/x92vM2HSwYSPvLuoOnDTdl8cHC3RxOy+XNYZ0BeHVWwarKL/y2mc/v6HlO1xYRkXNXtaZ8VGPT7ulNp5gwb1ejXI4Rwm+O3rxgu5Xh+WN4Nn8EB836+GJjqHUx3/j9m0stKytkB/PMcm6y+cXSc9tg84cTs49ERKTyUbipJDrFhPHzg329XY1zstpsxQP5jzDDfj4AAeTxkM+PfOj7Fv0tawjAcwN0Tzc0+8M/dzm/LxxIXLjbd000auoa9hzJ9HY1REQ8RuFG3MqBhQ/sQ7gr7zFm2XuQjT8RxjEe9fmOqX4v8LTPf+lj2Yi7N1HIs5XeOvTSzK2lvlfZOE7td/OAn9YeZMRnKzx+HxERb9GYGy+qHeDDscw8b1fDIw5Rh/fsQ/nZ3pd7fX6jtWUfgeQWDD62bCXNDCLEyCLZrMNrtmFsN2MqpF5JaTnc9fnpx+d4S67NzqCJi2gbFcKkW7q6vHcsM4/QQF+sbpqRtfdoFqnZ+dqKQUSqJbXceNHkf3ajTWRtPh7e3dtV8ZgDNGCc7XaG5Y3j3/n/ZJGjAzashBgF69dEGMd51fcDvvCdwGM+04jmiMfrNG+LZ7Z0OHXy2/r9Kdz80V/M3JDI0p0FnysxNZuhk5bwUwljdhbvOMLuI5nM2JDoPJaZa2PzwTS6vjCXmz/6y631fXveDrdeT0SkslDLjRe1jQph1qiLvF2NCrPCbMsKW1vqk0qskUhTSxK3WudiwSTMyOAiYz0X+G3ge/tFbHLEstps5e0ql0tqdsH6O4Vr3lz3/lLy7SZLdx0F4JeRfXl/wS7W7kth1LS1DO3S0OX8U9d0fGnmFj78czfNGtQCYHn8sXLVx+4wee+PnfRsWrfE94+cZpHCPJuDpbuO0CO2LrX89WNCRKoW/dSSCneEUI6Yoay0t+E7+0X0MLYRYxwmzrqKhsYRbrAu5AbrQgBesw1jkaOjl2tcNmsSUpg4bzufLo7nl5EXkG93TSsbDqSSnlO2WV3//m2zczf03YfPbvDvD6v388bcgg1Tm58ISGX1xpxtfPDnbvo0r8fXd59/VvcXEfEWhRvxKhNLQYuO2ZZZjh4Msq6gl7GF1pZ9ADzuM43HmcYRM5TfHV3IMAP53dGFNMr3y9oT8u0O1iSkuBybeKKr57U524qV35KY5rIy8rvzd/DQgJbO10WjUGGwORd7jp4MRbvKGZC+Xl6wSGNhq5OISFWicCOVRiaBfGe/mO+5kD6WTfzLZzo+FKwQXN9I5UbrAgCuNpfwuu1G/Mlnu9nIY6shn8mzv2ziq+UJZy54wpd/uZZ9Y+52HhrQkm1J6ZiYXt1rTESkOqkUA4onTZpEbGwsAQEB9OrVixUrSp+m+tFHH3HhhRdSp04d6tSpQ1xc3GnLS9VjYmGJowM35I3n//LvZLGjg8v79Yw0Jvh+zLO+n/O134s8YP2ZCI7hQ/kW8jtXpws2e4+WraUkz+Zg4MQ/uXziIrLzi2/1UBb2U6aPvzJrK2+e6I7ytBd+28wlry8go5yLKIqIeJLXW26mTZvG6NGjmTx5Mr169WLixIkMHDiQbdu2ER4eXqz8ggULuOmmm+jTpw8BAQG88sorXHbZZWzatImGDRuWcIeqpX6wH0cyquf08PKyY2Wj2YyNtma8yj9oaiRygWUD11n/xFKkE+dy6wout67AjpVNjib4YWO/2YA1Zgv+crQjHx8KOn0qbi+vjQfSylQuu8jeVWnZZd8QtNBbc7fzyeJ4fh7Zl+YNgjmcnsv7CwoWLbyjb9PTnuuOdqJPTnSfTV+5j9vPcD8RkYri9XDz5ptvcvfdd3P77bcDMHnyZGbMmMGnn37KU089Vaz8V1995fL6448/5vvvv2f+/PkMHz68QursSZ/f0ZPB7yz2djUqpXgzinh7FF/a46hLOhdb1nGZdRXRRsE0ayt2Olp2A9CGBOJY5XJ+slmHCOM4v9j7sMmMZZWjFRHGMQ6ZdcjFr8I/D+CSt85m/b635xeM8Xnlf1v5cHh38uwnFzN0VGA3V+GtMnJtBPlatUO6iHiVV8NNXl4eq1atYsyYMc5jFouFuLg4li1bVqZrZGVlkZ+fT926JU93zc3NJTf35JTXtLSy/Y/aW86LDmXHi4NoOfZ/3q5KpWVi4Sih/OC4iB8cFxFKBj0s24g0jtHNsp3mxsESz4swjgNwlXUpV7G0xDJHzFBMDDaYTckzffE38vAnn2n2/mSYARymjls/S9G1ceLPYUuEkoJMeTadX7brKIt2HObRS1vha7WUu1Vn7uZk3vl9BylZ+VzYsj7/vbNXOa8gIuI+Xg03R44cwW63ExER4XI8IiKCrVvLtmT+k08+SXR0NHFxcSW+P2HCBJ577rlzrmtF8rVauLpzND+vLfmXtLhKJZh5jm4AfGm/FAADB/7k082ynWusS6hDOsFGNoFn2N+qvpEKwCXGGpfjfSybnN8/lz+cTWYsOfi782MwZemesz7XVkKzT3kabm46sUDgewt28cYNnco9hmbZ7pOzqhbt8PxCjCIip+P1bqlz8fLLLzN16lQWLFhAQEBAiWXGjBnD6NGjna/T0tKIiamYpf7PxSvXdeTG7jHc8vFyb1elSjKxkIM/SxwdWFJkQLKBg47GbnLxw4qDfKw0Ng7R1thLrCWZw2YowWSz02zItdZFJV57vO8Xzu8dGHxgG8I8Rzd8sZFFyX8PS/PmnPIP/M2zOdhwIIVOjcKcxwoHFRedcXWmlpvSZmf9a/q6ctdJRKQy8Wq4qV+/PlarleTkZJfjycnJREZGnvbc119/nZdffpl58+bRsWPpi7z5+/vj7+/e/2FXhABfK31b1OfiVg1YuP2wt6tTbZhYWGe2cDm23YxhHt3glMlKU+wDaWIkc9CsT1MjkR6WbVxo2eAc4wNgweR+n1+4n18ASDGD8cXOarMFNqwcM0NY4jiPnWZDCvYkd2AWmaR4Nq01rZ4u3mVZkeNrREQqO6+GGz8/P7p168b8+fMZOnQoAA6Hg/nz5zNy5MhSz3v11Vd58cUXmT17Nt27V999mQD6t1a48R6DvWZByN5uxrDdHsNX9jgaG8kMsKymrZFAHSPdOZYHIMzIAOBCY4Pz2HXWP4td+bhZm3/b/kmKGUyUcZQUgkkwI4qVO9Urs0rurrXZSxhzU4Gzw0REKhOvd0uNHj2aESNG0L17d3r27MnEiRPJzMx0zp4aPnw4DRs2ZMKECQC88sorjBs3jq+//prY2FiSkpIACA4OJjg42GufQ2qOBDOCz+yDnK+DyOESyxrCjAzOM/bQ1rKXMw3JrWOk84bv+6ct87ejDXvNcH6y98XEQjqBzmnepypsuCnagHOmbqnf1idyRYdErugQdfqCReTbHUz7ex+9m9ejeQP9exORysnr4WbYsGEcPnyYcePGkZSUROfOnZk1a5ZzkHFCQgIWy8lm/Pfff5+8vDyuv/56l+uMHz+eZ599tiKrLgJAFgH85ujtcsyKHX/y6WXZQqpZi6ZGEo0tyfQwthFsZJfpuj0sW+nBVq4/peUnHx+s2MnFjzn2bmwxmzAoIwmORWDkBjm7vsrSU/XAV6vZ8/LgMn/Wz5fu4d8ztgCU6zwRkYrk9XADMHLkyFK7oRYsWODyes+ePZ6vUCUSGVq+AapSOdixkoWVPxxdAAp2OD+5BA2B5BBELiFGFplmADdaF9DUkkiskYzvGVZaLnw/kFyuti7lapZSK8eH458vJ8Bh8rNfNj/b+7J4aXsGWA6zwNEZO9az/iwZuTY27E+lZ9O6rNxzsgvuX99q4LGIVE6GWcM2tElLSyM0NJTU1FRCQkK8XZ0zcjhMXp+zjfdK6Y6Q6qdwGnsO/lhw0MXYQRq1uNk6HysO8vChp8V17I2f1eKygF9J/nB0JsUMZqnjPOoa6Wx2NCGVYDo2CmX9/tRSzyt8/+nBbVm19zj/25h0xs+w6In+xNT1zp5fIlI9lef3t8JNFTF1RQJP/bDhzAWlhjGpRxrtLXv4l8+37DAb0tI4UK4rHDDr0/DEDLAZ9vO50LIeCyY2rPzq6M1+swG7HNG0CA/G5jDZcsRGOkHFZn6dSt1WIuJOCjenUVXDjWmabElMZ0tiGrM3JTFnc/KZT5IayYqdIHJpbhwk2MjmH9bfaWwcOufrBvhaySllc8/CgJRiBvOVPY4Acnl7+IXQ5gowTSbPWsWf+218fkdPfK0WZm9Kws9qoX+b4vvHiYiUROHmNKpquDlV7FMzvF0FqWJ8sdHTsoX+lrVkEEgfyyYCKPsmracLNyVpGBZIvVp+2BwmmxPTWOtoQb/IXCKim/D6KjtrHC345rkH8fULIN/uwNdaeiuQiIjCzWnUxHAzb/RFxL1ZfK0VEQALDs63bGajoym5+GLBQaRxHBsWfLFzyAwj0jhG/fqRpB3ZT5RxlEd8fmC7GUNTI/G0A6DbRYdgs5tsT04HCgJS7QAfDqcXbIPRPjqUVfWu5OM1GTzSpz7tLroeQopPTTdNk9mbkmjfMJRGdTSWR6QmKs/v70oxW0o8q0V4bW9XQSoxBxaWOtq7HIs3XQPGTjOInYcBYtlsxjI/r1ux6wSTRS5+hHOc9pZ4GhipBB324+7zo8hM+pZ8rITlZxRr/fHf8BUP+oB9BZC5AppeBPVbQbOLnS06v6w7yCNT1wIayyMiZ6ZwU8UN7hDFjA2JHr/PIwNa8vb8HR6/j1RdGRS0qBygAQccDQoOZoItpymf5rdylvMjnxCy+KfPXPyST04nNwEzJQFjzZcApGTn8/mRVjS/4UX+2nmYQHLIPrF3V57NwVPfr+fCVvW5pkujivmAIlJlqJO7imsXHcLTg9uW+n7P2LrnfI/b+sQyvHeTc76O1EyfLol3eZ2HL0cIZaLtekbl3M02x8mNbBOOZbl839+ylsbfD+am9bcxze8FHrL+AEkbmLZyHz+sOcCj00pea+fXdQdZurNgBtjmg2l8tiQe2xmmyotI9aGWmyqqV9O6LI8/xtWdo2lUJ4jtyel8u3J/sXK1/AsWb5v8z27c9+Wqct/nwpb1GT+knXPXaRF32mo25nHbfYSRTh6+DLb/xWv1VmKaJQeRS62r4OeRXJaey8/GDRyiDqviD/Hz2gM8EteWerUD2Hs0k4e+WQMUdGFd8U7B7u5Wi8Hw3rElXtdmd/DAV6vpFBPGg/1blFhGRKoOhZsq6pu7zycr306wf8Ef4dgr2jFjfSKZeSXPZunQKNTl9cqn4+j+73mlXv/HB/rQpXGdIkfKFm4ubFmfRTuOnLmgSBEpFIwLm27vx2v3vMauQxnc+uYMLrKsJ9ZIKgg1RZkwzve/Bd9/9g7XAwfWQL1L/sHBbQfoYcRSx0jnpc9tFP6YG/fzJhJTcziakcut58e6/JuYt+UQczYnM2dzssKNSDWgcFNFWSyGM9gAhAb5snb8ZXywcBevz9lerHzDsECX1/WD/U97feOUXRdPfV2aoZ0bKtzIOfnP7zvo3yacVIL51dEHgHft1wLgTx6bGi0jefmcEs/N3zKTWolpPONb0FrDrp+4wDcYX+zsMSPYvKgJnYx9/LXWnw53P0R+3Va8MHs3SVme76F/a+52flt/kB/u70tokO85XcvhMF3+cyMirvQvoxrxtVoYeUlLRl7SkkvfXMiOQxkM7dLwjOdFhPjz2GWtefy79c5jp64QULZoc+adqEXO5PU526lXSvjOxY+nc/7J1LyLCSWDWCOJrpYdBJGLv5HHoYN7qW0UbFvhTz4AYUYGAOcZeziPPScvNmsMaZl5XJ+SzXGzNjf5GvzlaMem31M574KrITcdaoVDkY17D6XlcCg9l/YNXVtCS5KSlUctfx/ybA6ufW8p205Mh/9saTyj4lqd4ezT++cny1m666i2uRAphcJNNfXTg33ZcSiDTo1K/yH85+P9+XjxbkZe0oLw2gEu4eZsKdyIO4w5zVYjhV2vqQSzzmzBOntp3UgmoWTSwjjAeZY91DPSaW3sw5886hlpAOSfGGRcxygIHoOtf2Ff8Bfs+KDgEvWaY6sVScr+LRwJacfTa+tyjBAmj/oHjeoEsm5fwYaiVkvBX3y7w+SRqWuIDAng48XxNK1fi1vPb+IMNoVlztXSXUcB+HHNAR4e0PKcrydS3SjcVFO1/H3oHBNW4nuFAaRxvSCev/rk+ib/Htqep3/aeNpzziTY/9ya20XOZPPB0jf5dGWQSjCrzNassrcu9m4fM4QujXOYuXYvLY39nGfZSztjDxHGyZ3Pk+I3cSh99YlX8Yw78dfbMmM5H6W3ZHdyCtntwrhkwCCoE8vcHen8tv7k0gzxRzI9Ohi/aGgSkZMUbmqg0pa5/+f5TU4Tboqnm5J2kx6gvYLEw3YdznTLdZbuSWPpHoAo4s0o5jh6AAXbVOz4RzdMiy+vTXiTZpaDXGhxbUkyEtfSP3cl/X3Ad5cFMmcC0DPbxgs+kSx1nMcqRyvSCcTX5lrfPLuD39YfpFOjMGeX0so9x1i19zjXdG1IeO2AEuu7ZOcRvl+9n/FXnuc8NmN9IpNuPlkmJ9+OwzQJ8vPB4TD5K/4o50WHEhqo/3RIzaJwU4N8cGs3xv28kXdv6nrGsmUZQPzxiO70fHG+yzGLpeTznh3Sjmd/3Vy2iop4UT4+xE4oXD/nInDAa/wDAAMHHYx4Wtv30cXYQWvLfqxArk8w8QcSMTDoZNlFJ8su5/Wi1gTQ1s9klyOKRY6OrPxzG0vwJYXaLH7sQgiqx/WTlwEw4X9bmX5fb3qUsD7VLR8vByDQ11pivR0Ok/PGz8buMNn+70F8v3o/Y37YQNP6tfjjsX4kHM1izuYkbu7VmCA//eiX6k1/w2uQgedFMvC8yNOWuapTNAdSsulQhgGTpf0P89XrOjLul43k5J9cq6R38/rlq6xIJWRiYb3ZnPX25kynHwC1/Kxk7rARQB51SecS6xr6WjbS0CiYNegwIZBc2lv20N6yx+V69umfkpPvYIJPfbII4KhZm90zltHj0gshuivkpEKdWDBPLvFwMCW7xLpl5dudXWCH0nP4dd1BoKBrDODStxaSa3Ow71gWz13dvsRriFQXCjfi4p2bupSpXN8W9Up978YeMVzYqj69J/xerntf1i6COZuTy3WOiLcVDHA2yMGfg/jzpf1SvrRfii82rNgJOZJFR8tuOljiucCywWWj0fgjWWTl2TjPkuE8FprmC38scr1JQAgDLd1oZ9lLq+M+tLJa2WVGk2jW5ao3/sfIS8+jV8viG44WlWsr+M/GX7uPue2zi1RWCjdSbr+M7Ots2bmxe6MSV0a2ltCtFdc2gnlbSg8vA9qGs35/KklpOe6rrIiX5ONDPj7k4M88RzfmObrxFtcTQibpBGLBpEN+PHf5zGC/2YDjZm36W9cSYrouxJnvMElKTOZBn58AMFKgYdGeqVTgO4jHwsPWLsxw9CJhwRRq50YTSRpJuP5HxCzjgpwiVZnCjZRboK/VOSYnoJT+/1PH3hgGPHl5a/7cfpi7L2rKZ0v2kHXKasrnN6vHv4f6c9cXKz1TcZFKII1aANiBtWYLRuY/4nzvA/sQ4pqH8/E1DcHiAxi8/vn3xGV9QuHKPyYF+3P5YMNSJKhYcBBnXUWcdRWsgQcB/E68uTGHSAq6pw4npzHk3cVMuLaDc72e9Jx8gvx8nFPaRao6hRsps4cvacHhjFxahAc7jz10SUu+WLa3WNlaJQxYbBlRm83PD8THauEfPRqzYNshFu88wuxNBa05TerV4nhWvuc+gEgV4DCBkGjn61/TWjIt71/4YeMQdfDFRv6JH90hZDLIugKbaSXYyGag5W+CjeJjcmyL3uZDvzTn67RDQTg+yIILhnCM2oxaYMM/qi0fPXSNWz9LalY+eXYHDWqffkV0EXdTuJEyG31Z8bVCGtT256u7ejlnchQK9LPy9d29uPmjguMxdQqmvPqcmIYeUzeIW3vH8le8+v9FinKYxbuNCvfeApzBBgpagabZ+ztf/9d+KZEc4yghhJDJzdbf6WzZyebENJfrhRgFu687dv/J/oOpPOYDHAY+/RAi2oFvELQYAKExUK+5y7k5+XaOZ+URFRrI0YxcDqXn0jYqpMTP0un5gm0yNj43UFtFSIXS3zY5Z92a1CG8tj/NGtRyOd6neX02Pz8Qm8Mk0K/k7qszbfPQvmEIGw+c/ME8oE0487cecinTpF4QnWPC+HntwbP/ECKVhMMsWDn58ol/lntNHwcWDlIwM/Ewfrxtv66g/wsIIYNc/Ig0jtHJ2MXF1nVE+4VQMGinwN7kIzTM/RsfiwHxf568cGAYZoN22MKacMPvddiQHsy9Fzfjg4W7AZj76EW0jDgZwACmr9zn/D7haBbtoksOQCKeoHAj5yzA18rSpy4psb/+TOtpnLp6a1iRDQXfuakLcW3DybebdHqu4H+ALSKCXcLNmEFtuL1vU9buS1G4kWrhz+2HOf+l+RzNzHPrddMo6E7ea0ay14zkF0df2A5g4oudLsYOatlyuL5eDpeYK8jLySDQ14qv1YKRncKuVXPIyXfwomli87Oyd2kEL/v4MtPRi7VbImjZoCuHMvIIDylYIuLNucU38BWpKAo34hY+pax6fCZxbSOYvSnZuYJqk3q1eO6q8wgL8uWqTtHFylsMg/v7Nef9BQWLpDWo7Y+fj4WujcPo0jiM2Hq1+HHNAZfrlzRDq6TVlUUqC3cHm9MzyMeHFWZbMGFBPJhmb+qQxr0+Mwgkh3qt+7I3ZzVtjH00MFLwwU5z4yAY0M6yF37/lrW/G6SYtTnSqBltBwwnxJFGEj6YuP5sSMnKIzTQt0wLhYqcLYUb8arrujaiQW1/l12WR/SJLbW8n9XCo5e2coabwl4tH6uFHx/oC+ASbuoEuS47P6BNOFd3aciUJfFu+gQi1Uvhv6njhPCy7aaCF5sAWmHgIM6ymhjjMPWNVC4osi2FBZO6Rhq2A2vZ+tV6XrM7wA+OmiFEL/wJOlzOlgPHueGPUK7t3oTnr+/pPPdwei61/K3lXjnZNE1SsvKpU8vvzIWlRlG4Ea+yWAz6tT7zflT/urQVMzYkcscFTc9Y9qu7evHegp28dE0HdiRnMH3VyXV4PhreHYvF4LMi4WbpU5fQ7/UF5J1Y5Ozarg35YfWBYtct6tSxQCI1gYmFuY7uztdvcAMXW9bhwKCBkUotcrjWusi52zpAPSMN/6NbYMl2bAdS+doPctf7You+HZ/w1hyu04lLX5tHvl8Im56/olz1eXjqWn5dd5Cp95zP+c1KX1hUah6FG6kSHhrQkocGtCx2vKSW7b4t6tO3RcGgysYnNiY8VdGhPtFhgQT7+3DMVtAV8OaNnUsMN8H+PmTkFqwu+9aNnbl60pJia/WI1CR2rPzucN2r7nP7ZXQ1dmBgcql1Fb0tmzmQko1p4lyVx598Ns/6kHq1/Kjl78MXflnk4os543eMkCiy7fDsvq5c1r0dA9pGFLvvrI2J2B04t5j4YOEumjWoxc5DGXRoGEpGrg2b3XRuTCo1j8KNVGuGYXBhy/os2nHE5bif1TUVfTKiOyO/XsMzV7Yt8Trf39+Hbk3qcNfnf3M8K5/mDYLZ9NxAjmTk8cqsrXx3onVoxf8NIMjfB4dpkpvvYMG2Q1x2XqRzQHRJWoYHs+NQRqnvi1QlJhZWmQXLRqy0tSGAXHLy/AgmGwsOYo1k7vH5jcbGIY5m5jnHF/mTz4bl8wgN9CXP5uCW/P/CDqBrH4juAm0GQ+0ocmwO7vtytcs9DcOgz4TfsZ0yQUFT0Gsu/alLldSneT1W7jnOgDbF/1d3qlFxrZzhprClZ8K1Hbljyt+MvKQFAF0a12HJU5eUeo1uTeoA8PGIHpim6RwM2aC2P9d3a+QMNw1q+58cKBkAN3SPAWB47ybFFjvc8/JgktNyaBDsT7P/m+ny3j96xDD1732IVHU5J9ZWzqCgFWW9Gcyj+Q8SSA75+NDNsoMjZig9LFtpayTQPtt1PNyx+DXYdq2mzqr/YgCZ2fl84BtEqJHJAnsn/nB0BrMBdocdThm8vPNQBp0ahXI4Ixdfi0Vjc2oQhRupkr66qxd5dgf+PiWvn1NULf/iZVqEB/PnE/1LKF3guavOY/wvm0p879RZHkX/Z1jaDJDnr27P81e3J/apGS7HI05Mmw30tZKdf7KLa8K1HRRupNoq2HerYGr6YkcHALbaGzvfDySHa62LGWj5G44XtGompZ7ccy7qxCrMV1iXc4V1OcTDKD/Y6IjlMGHkmr58ZY9j6KQlLvfd/dIVxbaGkepJ4UaqJMMwyhRsztaIPrFEhwVydxn2uWobFULbqBDqB5/9/wpPXSOotJDkazXIt59seo+tF8Seo1lnfV+RyiibAL6yx/GVPQ4AAwd+2GhuHKSHZRt9LRuJNI6Rj4/LLuvtLXuc319u/Zup9kv42j6AgtE+Bk/9sJ5Xr++Ew2GSnJ5DSIAvtU7TbbXxQCp/7T7KbX1iz3q5C/EOhRupUcqztkZZw4rVYjDjoQtKHNxcVpe2i3BOYY+pG1himSGdonn9ho60fnoWUNAFtuDx/rw4YzMfLSo+tX3XS1cw9e8EesbWZfwvm1i662iZ61O3lh/HTllr5bY+sUxZuqfM1xBxFxMLufix2Yxlsz2Wz+0DMXDgi51/+3xKG0tCief9w/o7V1j+IsTIYo2jBZ+uGkTsyv10b1KHlXuPA/DmjZ0Y0CaCQD8rianZXPzaAm7qGcOEazty5buLgYJ/47f3LZip6XCYrNl3nHZRoQT4Ws7YgjxrYyIxdYM4Lzq01DKF5RJTc5z3kXOjcCPiBufa1P3C0PbOcPOPHo1LLPPWjZ3wsVqcA6RvPb8JAA8PaMneo1n0bl6P537d7CxvtRjc0qugzJTbe9Lq6f853zu/WV3+2l3yvl7xE67AMAx+XLOfR6etcx6/vlsjBrQN59ZPVpzx80SGBJCUlnPGciJny8RCHhaesN3rPFabLCKNY7zh+77zWOE+Wl0sO3nX8i5/ODqzfl8zYoxG7DMjGP3tumLX/mbFPjo0DHO+fu7XzQT4WhnUPpLOz88tVn7tuEsJCyr4z5DN7sBqMXj+t818tmSPs8yelweX+lnG/7yRz0+MyevdvB5N69fyaMt0TaBwI9WebyVqTn40rhVvzdvO6EtbuRwP9vdh5dNxrIg/xqXtSh4kXdgs/uGt3dlwINU5yLl2gC8fDi9Ye6RouCnKz8fCA/2ak55j48qOUXRoFEq7cbOd7781rJMzyBS2bl3TpRGT/tjFzhMzuQwDLmzZgK0vXM6EmVucP4xLMmvUhSX+EijNt/f25sYPlpW5vEhJ0gki3QxiTP5d9LFsYoi1+N+p/pa19LesBSAXXxLMcFY6WpNpBrDTbMhmswlg8H8/bnA5b8wPGxjzw4Zi1wO4/8vV/PfOnrQY+78S3wdIzc7n27/3kZVn56JW9WkXHYJpQm6+w+Xf0uUTFwEF4/CWjx1ASIBvaZeU01C4kWqvWf1aXNe1EXVref+HxMMDWnB990ZEhwYUe69+sD9XdIhyvl70RH/6vb6g2P5bgX5WejatW+57P3F5G5fXha0rMXUD8bGUHABnj7qI5idmcjWpV7AxaoCvlXFDzisx3Lx/S1f6tKjv3E6jJHdf2LRYN1rHRqHMffQiLn3rz1LOEim7TWZTNtmb8pF9MBZMrDjoZtlOKBm0NvYTYxyipWU//uTT0jhAS+vJda2OmiEkmnVJNutykHqkmbU4ZIbhi41dZjRHKd69tGz30dMGG8BlOYi35p15363sfDs9X5zH57f35OGpa/jXZa1pGxnCkP8s5pZejXnsstYczsglvLa/s9WoqHfm72BrUhr/ualrhQ2i/mnNAeKPZDIqrqXXt9cwzFO3Za7m0tLSCA0NJTU1lZAQ7VIrpTuemUeXFwpaH07XpOxJv647yEPfrCE00Jd14y87Y/lbP1nunPZ+pjrvPpzBBwt3c3+/5mw8mMrIr9eUeF5aTj75Ngf1gv1djh9Oz6XHi/OAghWbv723t8vy+b1emkdyWq7LOff3a86Tl7ch/kgmQyctITU7n1YRwcx59GIA1iQc56NFu5m5IanUevv7WMi1OUp8780bO5XYzSByqgByiTWSOd+ymTpGOtEcJdaShD/5pz3PgUGmGYgNK8cJJsmsxxEzBB/sZOOPgckeM5LDZih2rBwza3OYUEwMDMxie2250+19Y2kVUZuDKdm8+/tO5/Hv7+9DLX8rbSJDyLM5yM6z838/bqBlRDAXtmxAYmo2cW0j+Hp5As//tpl2USH8+5r2dG1ch7ScfEICfHE4TPIdDlKy8p2zPAvjQ2GQKZwNWrgumLuV5/e3wo3Iaew+nEGgn5Wo0JIH+XqaaZos23WU1pG1i4WLkmw8kOocBFmeQLZq73Gue39puc8r/GHWoWEovz50gct76/enMGrqWh4a0MLZ5TVmUBvuvbg5APl2Bwu3HaZH07rFWnp+WXeQsT9uYOo957PxQCpPfl/QHTC4YxRvD+uM1WKw71g2jeoEOtcIKroE/6lT7kXKwhcb7Yw9hBmZRBlHiTKOEUY6dY10rDhoZBw+62s7MLBgssuM5if7BWx1xJCDHxkEYqdyjq/p0jiMNQkpWC2GSwvy9/f3oUPDUAa/swhfq4UW4cHc3Ksx//jwLwAmDuvM0C4N3V4fhZvTULiR6m7JziNEhwXStH6tcp33yeJ4mtWvRf82Z97rq1BhiGjfMITfHrqw1HIPfLWKuZuTWfrUABrUPnNIO1VGrq3UlWaX7DxCYmoO13drVKxeZzL1nvOdP5BvPb8J//2roKvthwcKfngPeXcxW5PSz3idxy5rxfL4Y8VWwi6rC1rUZ/HOsztXKk4wWYQZGdQnDROoZeRQnzTqGmlYcWDFjg92GhmHaWCkYmBSz0jHyum3acnDlyzTnywCcGCQYQaSQjDZ+FGLHBLNetiwkoU/uaYvaQSRjw+5pi+5+JGLL3n4kGsW+R4/8rECnukeqh/sx5GM0nevn3J7jzLtG1geVS7cTJo0iddee42kpCQ6derEu+++S8+ePUstP336dJ555hn27NlDy5YteeWVV7jiirJtuKZwI+I+ZQ03pmmSbzfx86mYwd2/rDvIw9+s4ZI24fRv3YBbe8cWCzx7Xh6MaZq8MmsbLcKDGdo52jluYt7oi2gRXhs4c1D6deQFtG8YQr7dJO7NhSQcK/+6Q3teHuzR1qYesXWwOUzWJKR47B5SMgMHdcjAwCTUyORiyzq6WXYQZRzFBzsGnvsV7MAgFz/yTB9y8T0RfHxPhKKCYJR3IiTl4UsmAWSZ/tjwwYaFPHyxYSUPH/JNH2dosmMpCFumP+aJ8JSHD1kEkMfJVlh3d+eX5/e31wcUT5s2jdGjRzN58mR69erFxIkTGThwINu2bSM8vHjqW7p0KTfddBMTJkzgyiuv5Ouvv2bo0KGsXr2a9u3be+ETiNRcI3o34fNle3l8YJvTljMMAz+fihtgeFWnaK7qFO1y7OnBbfn3jC1YLQZTbu/hrNdTgwrqXrTZveh/+R7s35w/th7mu/t78+Vfe7mkTThD3l1Cdr6dAF8LHRoVDDD18zGcq17P3pRE/JFMjmfl8cHC3bSOqM1/7+rJun2pXNImnCMZufR6aT4Ab/+jMwCbnx/I1qR0cvMdZObaiA4LZHXCcQ6l5fDOifETW1+4nDbPzCrXs2gRHsz0+/oA0OaZ/5GTX/J4JfEMEwvHKPhFfNQMZbc9ms/sgwDwwUYAeQSTTYCRRyiZAAQZuYSRQaiRiR0LIWQBJsHk4GfkE0IWPtjxM/Lxp8iXkY8/eVhOBCYLJoHkEmi4jn3zUGOOkwODbY7GgHfGKkIlaLnp1asXPXr04D//+Q8ADoeDmJgYHnroIZ566qli5YcNG0ZmZia//fab89j5559P586dmTx58hnvp5YbEfcxTZO0HNtpZ0dVFaZpMvzTFaRm5/PjA32LrRpd1MYDqbz8v608NagN7RuWvjhbrs3O3M3J9Glen7qn7Gu0+WAatQN8zrhzda7NztvzdtCvdbhzltzeo5nUqeXH0Yw8/tx+mDfmbCMtx0aD2v4E+FpoWj+YP7cXjA9Z+XQc9U8Zr/Xp4nie/63kZQMKzXn0Ii47w+y1Px7rR1RogDNw/evSVrwx9+RMoPv7NadhWCBP/7TR5bxTV9oWdzILgg82/MnDn3z8DNuJAJRXcPxECHINRfnUIocgI9fZveaHDV9sRYKUzRmefLERYBR8b5y4Z9FWqK2Oxtz471/c+smqTLdUXl4eQUFBfPfddwwdOtR5fMSIEaSkpPDzzz8XO6dx48aMHj2aUaNGOY+NHz+en376iXXris+SyM3NJTf3ZGpNS0sjJiZG4UZEijl19kdVkW93YHeYBPieHJi6/3gW9YP9XY4VMk2T6av20zYyhInztvP7tkN8f38fvlmewPRV+xl3ZTvuuKAp01fu49XZ25j8z658ungP/Vo34HhWHi/N3MrQztG8NawzhmHwx7ZDpOfYuKpTNBsPpHIwJZtezeo5Q++Pa/bz/oJdfHhrd2JPjAXLybcXDEYdO9PZUlY47inIz8rm5y9n1sYkvvxrr8t4pJ0vDuLt+TtoVCeQYT0ac/37S1m597jLGJAbujWiR2xdnvh+vfO8uLYRPDawFWN/3EhooC+39YnlX9PXMaBNuPZxcwMDB4EnOrIMTBxYWP/yjW69R5UJNwcPHqRhw4YsXbqU3r17O48/8cQTLFy4kOXLlxc7x8/Pj88//5ybbrrJeey9997jueeeIzk5uVj5Z599lueee67YcYUbEZHi0nPyqV2BC8fl2Rz8uu4gfVrUK3VWosNh8uv6g3RtXKfElq74I5k0qhPI7sOZ/Lz2APde3NylNdE0zdMG1iMZuaRk5RMS4MMf2w7Rv3U44SemO+9ITif+SCaXtotwXmPczwUB6V+XtQYKFujzs1p4fc42WoYHc1XnaP758XKu7xbD1yv2svNQBiufvpTD6bkYQGz9WuTbC7oHZ25IpGV4bdpFh5CZa+Pe/67ijgti2XMki9mbkmhSL4gRfWKpE+THK7O2Mrx3E16bvY36wf5c3bkhCceyiA4N4P6vVrt8poWP92PZrqMcSMlm9KWtOJCSzbzNyTSsE1SmPfM6Ngrlh/v7sG5/Ct+vPsDXywu2uHiwf3O+W7WfqNBAujauw45D6SUOpF/61CVEh7l3lqnCTRFquREREan6qsyA4vr162O1WouFkuTkZCIjI0s8JzIyslzl/f398fcv/9RTERERqZq8uumOn58f3bp1Y/78+c5jDoeD+fPnu7TkFNW7d2+X8gBz584ttbyIiIjULF6fCj569GhGjBhB9+7d6dmzJxMnTiQzM5Pbb78dgOHDh9OwYUMmTJgAwCOPPMLFF1/MG2+8weDBg5k6dSorV67kww8/9ObHEBERkUrC6+Fm2LBhHD58mHHjxpGUlETnzp2ZNWsWEREFOyMnJCRgKbKpX58+ffj66695+umn+b//+z9atmzJTz/9pDVuREREBKgE69xUNK1zIyIiUvWU5/e3V8fciIiIiLibwo2IiIhUKwo3IiIiUq0o3IiIiEi1onAjIiIi1YrCjYiIiFQrCjciIiJSrSjciIiISLWicCMiIiLVite3X6hohQsyp6WlebkmIiIiUlaFv7fLsrFCjQs36enpAMTExHi5JiIiIlJe6enphIaGnrZMjdtbyuFwcPDgQWrXro1hGG69dlpaGjExMezbt0/7VnmQnnPF0HOuGHrOFUfPumJ46jmbpkl6ejrR0dEuG2qXpMa13FgsFho1auTRe4SEhOgfTgXQc64Yes4VQ8+54uhZVwxPPOcztdgU0oBiERERqVYUbkRERKRaUbhxI39/f8aPH4+/v7+3q1Kt6TlXDD3niqHnXHH0rCtGZXjONW5AsYiIiFRvarkRERGRakXhRkRERKoVhRsRERGpVhRu3GTSpEnExsYSEBBAr169WLFihberVKlNmDCBHj16ULt2bcLDwxk6dCjbtm1zKZOTk8ODDz5IvXr1CA4O5rrrriM5OdmlTEJCAoMHDyYoKIjw8HAef/xxbDabS5kFCxbQtWtX/P39adGiBVOmTPH0x6uUXn75ZQzDYNSoUc5jesbuc+DAAf75z39Sr149AgMD6dChAytXrnS+b5om48aNIyoqisDAQOLi4tixY4fLNY4dO8Ytt9xCSEgIYWFh3HnnnWRkZLiUWb9+PRdeeCEBAQHExMTw6quvVsjnqwzsdjvPPPMMTZs2JTAwkObNm/PCCy+4LMev51x+f/75J0OGDCE6OhrDMPjpp59c3q/IZzp9+nTatGlDQEAAHTp0YObMmWf3oUw5Z1OnTjX9/PzMTz/91Ny0aZN59913m2FhYWZycrK3q1ZpDRw40Pzss8/MjRs3mmvXrjWvuOIKs3HjxmZGRoazzH333WfGxMSY8+fPN1euXGmef/75Zp8+fZzv22w2s3379mZcXJy5Zs0ac+bMmWb9+vXNMWPGOMvs3r3bDAoKMkePHm1u3rzZfPfdd02r1WrOmjWrQj+vt61YscKMjY01O3bsaD7yyCPO43rG7nHs2DGzSZMm5m233WYuX77c3L17tzl79mxz586dzjIvv/yyGRoaav7000/munXrzKuuusps2rSpmZ2d7Sxz+eWXm506dTL/+usvc9GiRWaLFi3Mm266yfl+amqqGRERYd5yyy3mxo0bzW+++cYMDAw0P/jggwr9vN7y4osvmvXq1TN/++03Mz4+3pw+fboZHBxsvv32284yes7lN3PmTHPs2LHmDz/8YALmjz/+6PJ+RT3TJUuWmFar1Xz11VfNzZs3m08//bTp6+trbtiwodyfSeHGDXr27Gk++OCDztd2u92Mjo42J0yY4MVaVS2HDh0yAXPhwoWmaZpmSkqK6evra06fPt1ZZsuWLSZgLlu2zDTNgn+QFovFTEpKcpZ5//33zZCQEDM3N9c0TdN84oknzPPOO8/lXsOGDTMHDhzo6Y9UaaSnp5stW7Y0586da1588cXOcKNn7D5PPvmkecEFF5T6vsPhMCMjI83XXnvNeSwlJcX09/c3v/nmG9M0TXPz5s0mYP7999/OMv/73/9MwzDMAwcOmKZpmu+9955Zp04d57MvvHfr1q3d/ZEqpcGDB5t33HGHy7Frr73WvOWWW0zT1HN2h1PDTUU+0xtvvNEcPHiwS3169epl3nvvveX+HOqWOkd5eXmsWrWKuLg45zGLxUJcXBzLli3zYs2qltTUVADq1q0LwKpVq8jPz3d5rm3atKFx48bO57ps2TI6dOhARESEs8zAgQNJS0tj06ZNzjJFr1FYpib92Tz44IMMHjy42HPQM3afX375he7du3PDDTcQHh5Oly5d+Oijj5zvx8fHk5SU5PKcQkND6dWrl8uzDgsLo3v37s4ycXFxWCwWli9f7ixz0UUX4efn5ywzcOBAtm3bxvHjxz39Mb2uT58+zJ8/n+3btwOwbt06Fi9ezKBBgwA9Z0+oyGfqzp8lCjfn6MiRI9jtdpcf/gAREREkJSV5qVZVi8PhYNSoUfTt25f27dsDkJSUhJ+fH2FhYS5liz7XpKSkEp974XunK5OWlkZ2drYnPk6lMnXqVFavXs2ECROKvadn7D67d+/m/fffp2XLlsyePZv777+fhx9+mM8//xw4+axO93MiKSmJ8PBwl/d9fHyoW7duuf48qrOnnnqKf/zjH7Rp0wZfX1+6dOnCqFGjuOWWWwA9Z0+oyGdaWpmzeeY1buNMqXwefPBBNm7cyOLFi71dlWpl3759PPLII8ydO5eAgABvV6daczgcdO/enZdeegmALl26sHHjRiZPnsyIESO8XLvq49tvv+Wrr77i66+/5rzzzmPt2rWMGjWK6OhoPWdxoZabc1S/fn2sVmuxGSbJyclERkZ6qVZVx8iRI/ntt9/4448/XHZrj4yMJC8vj5SUFJfyRZ9rZGRkic+98L3TlQkJCSEwMNDdH6dSWbVqFYcOHaJr1674+Pjg4+PDwoULeeedd/Dx8SEiIkLP2E2ioqJo166dy7G2bduSkJAAnHxWp/s5ERkZyaFDh1zet9lsHDt2rFx/HtXZ448/7my96dChA7feeiuPPvqos2VSz9n9KvKZllbmbJ65ws058vPzo1u3bsyfP995zOFwMH/+fHr37u3FmlVupmkycuRIfvzxR37//XeaNm3q8n63bt3w9fV1ea7btm0jISHB+Vx79+7Nhg0bXP5RzZ07l5CQEOcvmt69e7tco7BMTfizGTBgABs2bGDt2rXOr+7du3PLLbc4v9czdo++ffsWW8pg+/btNGnSBICmTZsSGRnp8pzS0tJYvny5y7NOSUlh1apVzjK///47DoeDXr16Ocv8+eef5OfnO8vMnTuX1q1bU6dOHY99vsoiKysLi8X115bVasXhcAB6zp5Qkc/UrT9Lyj0EWYqZOnWq6e/vb06ZMsXcvHmzec8995hhYWEuM0zE1f3332+GhoaaCxYsMBMTE51fWVlZzjL33Xef2bhxY/P33383V65cafbu3dvs3bu38/3CacqXXXaZuXbtWnPWrFlmgwYNSpym/Pjjj5tbtmwxJ02aVOOmKRdVdLaUaeoZu8uKFStMHx8f88UXXzR37NhhfvXVV2ZQUJD55ZdfOsu8/PLLZlhYmPnzzz+b69evN6+++uoSp9N26dLFXL58ubl48WKzZcuWLtNpU1JSzIiICPPWW281N27caE6dOtUMCgqqtlOUTzVixAizYcOGzqngP/zwg1m/fn3ziSeecJbRcy6/9PR0c82aNeaaNWtMwHzzzTfNNWvWmHv37jVNs+Ke6ZIlS0wfHx/z9ddfN7ds2WKOHz9eU8G97d133zUbN25s+vn5mT179jT/+usvb1epUgNK/Prss8+cZbKzs80HHnjArFOnjhkUFGRec801ZmJiost19uzZYw4aNMgMDAw069evb/7rX/8y8/PzXcr88ccfZufOnU0/Pz+zWbNmLveoaU4NN3rG7vPrr7+a7du3N/39/c02bdqYH374ocv7DofDfOaZZ8yIiAjT39/fHDBggLlt2zaXMkePHjVvuukmMzg42AwJCTFvv/12Mz093aXMunXrzAsuuMD09/c3GzZsaL788sse/2yVRVpamvnII4+YjRs3NgMCAsxmzZqZY8eOdZlerOdcfn/88UeJP49HjBhhmmbFPtNvv/3WbNWqlenn52eed9555owZM87qM2lXcBEREalWNOZGREREqhWFGxEREalWFG5ERESkWlG4ERERkWpF4UZERESqFYUbERERqVYUbkRERKRaUbgRERGRakXhRkRqtClTphAWFubtaoiIGynciEilcNttt2EYhvOrXr16XH755axfv77M13j22Wfp3Lmz5yopIlWCwo2IVBqXX345iYmJJCYmMn/+fHx8fLjyyiu9XS0RqWIUbkSk0vD39ycyMpLIyEg6d+7MU089xb59+zh8+DAATz75JK1atSIoKIhmzZrxzDPPkJ+fDxR0Lz333HOsW7fO2fozZcoUAFJSUrj33nuJiIggICCA9u3b89tvv7nce/bs2bRt25bg4GBnyBKRqsnH2xUQESlJRkYGX375JS1atKBevXoA1K5dmylTphAdHc2GDRu4++67qV27Nk888QTDhg1j48aNzJo1i3nz5gEQGhqKw+Fg0KBBpKen8+WXX9K8eXM2b96M1Wp13isrK4vXX3+d//73v1gsFv75z3/y2GOP8dVXX3nls4vIuVG4EZFK47fffiM4OBiAzMxMoqKi+O2337BYChqZn376aWfZ2NhYHnvsMaZOncoTTzxBYGAgwcHB+Pj4EBkZ6Sw3Z84cVqxYwZYtW2jVqhUAzZo1c7lvfn4+kydPpnnz5gCMHDmS559/3qOfVUQ8R+FGRCqN/v378/777wNw/Phx3nvvPQYNGsSKFSto0qQJ06ZN45133mHXrl1kZGRgs9kICQk57TXXrl1Lo0aNnMGmJEFBQc5gAxAVFcWhQ4fc86FEpMJpzI2IVBq1atWiRYsWtGjRgh49evDxxx+TmZnJRx99xLJly7jlllu44oor+O2331izZg1jx44lLy/vtNcMDAw84319fX1dXhuGgWma5/RZRMR71HIjIpWWYRhYLBays7NZunQpTZo0YezYsc739+7d61Lez88Pu93ucqxjx47s37+f7du3n7b1RkSqD4UbEak0cnNzSUpKAgq6pf7zn/+QkZHBkCFDSEtLIyEhgalTp9KjRw9mzJjBjz/+6HJ+bGws8fHxzq6o2rVrc/HFF3PRRRdx3XXX8eabb9KiRQu2bt2KYRhcfvnl3viYIuJh6pYSkUpj1qxZREVFERUVRa9evfj777+ZPn06/fr146qrruLRRx9l5MiRdO7cmaVLl/LMM8+4nH/ddddx+eWX079/fxo0aMA333wDwPfff0+PHj246aabaNeuHU888USxFh4RqT4MUx3LIiIiUo2o5UZERESqFYUbERERqVYUbkRERKRaUbgRERGRakXhRkRERKoVhRsRERGpVhRuREREpFpRuBEREZFqReFGREREqhWFGxEREalWFG5ERESkWlG4ERERkWrl/wFCkmHPF3fFxAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testando o modelo\n",
        "\n",
        "# Converte os dados de teste em arrays numpy\n",
        "x_teste_np = data_test.iloc[:, :-1].to_numpy()\n",
        "d_teste_np = data_test.iloc[:, -1].to_numpy()\n",
        "\n",
        "# Aplica a normalização no conjunto de teste\n",
        "x_teste_np_norm = (x_teste_np - media_treino) / desvio_padrao_treino\n",
        "\n",
        "# Converte os arrays numpy em tensores PyTorch\n",
        "x_teste_tensor = torch.tensor(x_teste_np_norm, dtype=torch.float32).to(device=device)\n",
        "d_teste_tensor = torch.tensor(d_teste_np, dtype=torch.long).to(device=device)\n",
        "\n",
        "# Testa o modelo com os dados de teste\n",
        "y_teste_tensor = model(x_teste_tensor)\n",
        "y_teste_np = y_teste_tensor.cpu().detach().numpy()\n",
        "\n",
        "predicoes = np.argmax(y_teste_np, axis=1)\n",
        "acuracia = np.mean(predicoes == d_teste_np)\n",
        "\n",
        "# Taxa de erros\n",
        "\n",
        "Taxa_de_erro = (1 - acuracia) * 100\n",
        "\n",
        "print(f\"Acurácia: {acuracia*100:.2f}%\")\n",
        "print(f\"Taxa de erro: {Taxa_de_erro:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-3zun2OMPZd",
        "outputId": "a94d2732-301a-4005-9298-f83924aa6560"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia: 98.67%\n",
            "Taxa de erro: 1.33%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "pB6QkHw9PF4Z",
        "outputId": "0cd5a3c0-4ca6-4285-a6eb-f37e16881698"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        0    1    2      3      4    5    6      7    8    9   10   11   12  \\\n",
              "0    67.0  1.0  0.0  120.0  229.0  0.0  0.0  129.0  1.0  2.6  1.0  2.0  3.0   \n",
              "1    63.0  1.0  3.0  145.0  233.0  1.0  0.0  150.0  0.0  2.3  0.0  0.0  1.0   \n",
              "2    63.0  0.0  0.0  124.0  197.0  0.0  1.0  136.0  1.0  0.0  1.0  0.0  2.0   \n",
              "3    52.0  1.0  0.0  112.0  230.0  0.0  1.0  160.0  0.0  0.0  2.0  1.0  2.0   \n",
              "4    58.0  0.0  0.0  130.0  197.0  0.0  1.0  131.0  0.0  0.6  1.0  0.0  2.0   \n",
              "..    ...  ...  ...    ...    ...  ...  ...    ...  ...  ...  ...  ...  ...   \n",
              "220  59.0  1.0  1.0  140.0  221.0  0.0  1.0  164.0  1.0  0.0  2.0  0.0  2.0   \n",
              "221  60.0  1.0  0.0  125.0  258.0  0.0  0.0  141.0  1.0  2.8  1.0  1.0  3.0   \n",
              "222  47.0  1.0  0.0  110.0  275.0  0.0  0.0  118.0  1.0  1.0  1.0  1.0  2.0   \n",
              "223  50.0  0.0  0.0  110.0  254.0  0.0  0.0  159.0  0.0  0.0  2.0  0.0  2.0   \n",
              "224  54.0  1.0  0.0  120.0  188.0  0.0  1.0  113.0  0.0  1.4  1.0  1.0  3.0   \n",
              "\n",
              "      13  \n",
              "0    0.0  \n",
              "1    1.0  \n",
              "2    0.0  \n",
              "3    0.0  \n",
              "4    1.0  \n",
              "..   ...  \n",
              "220  1.0  \n",
              "221  0.0  \n",
              "222  0.0  \n",
              "223  1.0  \n",
              "224  0.0  \n",
              "\n",
              "[225 rows x 14 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c823cfec-4a87-4288-822e-fa0af0152d46\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>67.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>129.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>63.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>145.0</td>\n",
              "      <td>233.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>63.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>124.0</td>\n",
              "      <td>197.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>136.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>52.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>230.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>160.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>58.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>197.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>131.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220</th>\n",
              "      <td>59.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>140.0</td>\n",
              "      <td>221.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>164.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>221</th>\n",
              "      <td>60.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>125.0</td>\n",
              "      <td>258.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>141.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>222</th>\n",
              "      <td>47.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>275.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>118.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223</th>\n",
              "      <td>50.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>254.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>159.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>224</th>\n",
              "      <td>54.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>188.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>113.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>225 rows × 14 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c823cfec-4a87-4288-822e-fa0af0152d46')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c823cfec-4a87-4288-822e-fa0af0152d46 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c823cfec-4a87-4288-822e-fa0af0152d46');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-7caedaba-c3e7-46e3-95f4-8ad82679c406\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7caedaba-c3e7-46e3-95f4-8ad82679c406')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-7caedaba-c3e7-46e3-95f4-8ad82679c406 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_c64ed7d8-73f5-4af3-9e2b-c6f347bed1c6\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('data_test')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_c64ed7d8-73f5-4af3-9e2b-c6f347bed1c6 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('data_test');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data_test",
              "summary": "{\n  \"name\": \"data_test\",\n  \"rows\": 225,\n  \"fields\": [\n    {\n      \"column\": \"0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8.499474773688751,\n        \"min\": 35.0,\n        \"max\": 76.0,\n        \"num_unique_values\": 37,\n        \"samples\": [\n          55.0,\n          42.0,\n          53.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4383862904819672,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0750046142388958,\n        \"min\": 0.0,\n        \"max\": 3.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          3.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 16.271752230012602,\n        \"min\": 94.0,\n        \"max\": 180.0,\n        \"num_unique_values\": 43,\n        \"samples\": [\n          152.0,\n          132.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"4\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 45.49341832739227,\n        \"min\": 131.0,\n        \"max\": 417.0,\n        \"num_unique_values\": 111,\n        \"samples\": [\n          249.0,\n          268.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"5\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.33082388735465307,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"6\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5361162513118173,\n        \"min\": 0.0,\n        \"max\": 2.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"7\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 22.799549215816615,\n        \"min\": 95.0,\n        \"max\": 194.0,\n        \"num_unique_values\": 74,\n        \"samples\": [\n          131.0,\n          142.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"8\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.47696960070847183,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"9\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.1418913577989258,\n        \"min\": 0.0,\n        \"max\": 5.6,\n        \"num_unique_values\": 35,\n        \"samples\": [\n          2.4,\n          1.4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"10\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6281428914375227,\n        \"min\": 0.0,\n        \"max\": 2.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"11\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.9614184296264536,\n        \"min\": 0.0,\n        \"max\": 4.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.0,\n          3.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"12\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6164735862465499,\n        \"min\": 0.0,\n        \"max\": 3.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"13\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5009910812500195,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_dAL6ph3K_d"
      },
      "source": [
        "# Exercício 2\n",
        "\n",
        "Aplique o PCA nos dados de entrada e obtenha uma matriz de dados transformados representando os 12 componentes principais. Mostre o valor da porcentagem de variância explicada acumulada à medida que você considera um maior número de componentes principais.\n",
        "\n",
        "## Resolução"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kz4BFhRx3K_e",
        "outputId": "f072805a-9fc1-496f-d529-cccc1330d31e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2.86740774 1.46122368 1.25520296 1.15314482 1.04667089 0.9734824\n",
            " 0.87201375 0.73645678 0.72308199 0.6074035  0.52547499 0.40691905\n",
            " 0.35120494]\n",
            "M     | Variância acumulada (%)\n",
            "------------------------------\n",
            "1     | 22.09%\n",
            "2     | 33.35%\n",
            "3     | 43.02%\n",
            "4     | 51.90%\n",
            "5     | 59.97%\n",
            "6     | 67.47%\n",
            "7     | 74.19%\n",
            "8     | 79.86%\n",
            "9     | 85.43%\n",
            "10    | 90.11%\n",
            "11    | 94.16%\n",
            "12    | 97.29%\n",
            "13    | 100.00%\n"
          ]
        }
      ],
      "source": [
        "# Vamos aplicar a PCA sobre os dados normalizados\n",
        "\n",
        "# x_treino_np_norm\n",
        "# d_treino_np\n",
        "# x_val_np_norm\n",
        "# d_val_np\n",
        "\n",
        "# D: número de características\n",
        "# N: número de exemplos\n",
        "# M: número de características no espaço projetado\n",
        "\n",
        "# P = UX (M X N)\n",
        "# U = matriz com os autovetores da matriz de covariância dos dados (M X D)\n",
        "# X = matriz com os dados normalizados (D X N)\n",
        "\n",
        "X_treino = x_treino_np_norm.T\n",
        "N_treino = X_treino.shape[1]\n",
        "\n",
        "# Matriz de covariância dos dados de treino\n",
        "S_treino = (X_treino @ X_treino.T) / N_treino # (D X D)\n",
        "\n",
        "autovalores, autovetores = np.linalg.eig(S_treino)\n",
        "\n",
        "# Ordenar os autovalores em ordem decrescente\n",
        "indices_ordenados = np.argsort(autovalores)\n",
        "indices_decrescente = indices_ordenados[::-1]\n",
        "\n",
        "autovalores = autovalores[indices_decrescente]\n",
        "autovetores = autovetores[:, indices_decrescente]\n",
        "\n",
        "# Variância explicada\n",
        "var_explicada = (autovalores / np.sum(autovalores)) * 100\n",
        "\n",
        "# Variância acumulada\n",
        "var_acumulada = np.cumsum(var_explicada)\n",
        "\n",
        "print(f\"{'M':<5} | {'Variância acumulada (%)'}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "for i, variancia in enumerate(var_acumulada):\n",
        "  M = i + 1\n",
        "  print(f\"{M:<5} | {variancia:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Projetar dados com M = 12 componentes principais\n",
        "\n",
        "M = 12\n",
        "U = autovetores[:, :M].T\n",
        "\n",
        "# Transformação linear P = UX\n",
        "P_treino = U @ X_treino\n",
        "print(\"Matriz dos dados transformados no subespaço de dimensão M=12\")\n",
        "print(P_treino)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSZM2FJCzXix",
        "outputId": "d2800f63-9137-4376-9157-6c28553ecf9b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matriz dos dados transformados no subespaço de dimensão M=12\n",
            "[[-0.07935227 -0.89318144  0.69595694 ... -0.01517568  1.68807942\n",
            "   3.03084608]\n",
            " [ 0.1416988   1.1531973   0.45705237 ...  0.04659808  1.32358092\n",
            "  -0.11026535]\n",
            " [-1.61382313  2.04073754 -1.23264821 ...  0.00375707 -0.10833941\n",
            "   0.55461342]\n",
            " ...\n",
            " [ 0.15169618  0.01428737  0.04966463 ... -0.51195944  0.17841599\n",
            "   0.62290624]\n",
            " [-0.81245546 -1.61807682 -0.09708913 ...  0.77008586 -0.71805743\n",
            "  -0.70736993]\n",
            " [ 1.17086285  0.39181812  0.25373326 ...  0.70881926 -0.42611976\n",
            "  -0.59610936]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMRKZ9gP3K_g"
      },
      "source": [
        "# Exercício 3\n",
        "\n",
        "Implemente uma segunda rede neural para fazer a classificação usando o número de componentes principais necessário para incluir 90% da variância explicada. Calcule a acurácia obtida nos dados de teste.\n",
        "\n",
        "## Resolução"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como vimos na tabela do exercício 2, o número mínimo de componentes principais necessário para incluir 90% da variância explicada é M=10."
      ],
      "metadata": {
        "id": "6vbKHGrG0Wdl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "5TYxZCc03K_h"
      },
      "outputs": [],
      "source": [
        "M = 10\n",
        "U = autovetores[:, :M].T\n",
        "\n",
        "# Transformação linear P = UX\n",
        "P_treino = U @ X_treino\n",
        "\n",
        "# Projetando também nos dados de validação\n",
        "X_val = x_val_np_norm.T\n",
        "P_val = U @ X_val\n",
        "\n",
        "# Convertendo em tensores Pytorch\n",
        "x_treino_pca = torch.tensor(P_treino.T, dtype=torch.float32)\n",
        "x_val_pca = torch.tensor(P_val.T, dtype=torch.float32)\n",
        "\n",
        "# Vou definir d_treino_tensor e d_val_tensor de novo mas é o mesmo\n",
        "d_treino_tensor = torch.tensor(d_treino_np, dtype=torch.long)\n",
        "d_val_tensor = torch.tensor(d_val_np, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelPCA(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.model = nn.Sequential(\n",
        "        nn.Linear(10, 8),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.Linear(8, 4),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.Linear(4, 4),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.Linear(4, 2),\n",
        "    )\n",
        "\n",
        "    self._init_weights()\n",
        "\n",
        "  def _init_weights(self):\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Linear):\n",
        "        nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
        "\n",
        "        # Inicializa os bias com zero\n",
        "        if m.bias is not None:\n",
        "          nn.init.constant_(m.bias, 0)\n",
        "\n",
        "  def forward(self, x):\n",
        "    output = self.model(x)\n",
        "    return output"
      ],
      "metadata": {
        "id": "Q7ylvxlY2iTK"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Aqui muda\n",
        "model_pca = ModelPCA().to(device=device)\n",
        "\n",
        "# Taxa de aprendizado\n",
        "eta = 0.001\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_pca.parameters(), lr=eta)\n",
        "\n",
        "Nb = 64 # Tamanho do mini-batch\n",
        "Ne = 1000 # Número de épocas\n"
      ],
      "metadata": {
        "id": "DWBODm3E24ES"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_set_pca = TensorDataset(x_treino_pca, d_treino_tensor)\n",
        "train_loader_pca = torch.utils.data.DataLoader(train_set_pca, batch_size=Nb, shuffle=True)"
      ],
      "metadata": {
        "id": "YICaAKhS3W02"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Treinamento\n",
        "losses = []\n",
        "val_losses = []\n",
        "\n",
        "x_val_pca = x_val_pca.to(device=device)\n",
        "d_val_tensor = d_val_tensor.to(device=device)\n",
        "\n",
        "for epoch in range(Ne):\n",
        "  for n, (X, d) in enumerate(train_loader_pca):\n",
        "\n",
        "    X = X.to(device=device)\n",
        "    d = d.to(device=device)\n",
        "\n",
        "    # Treinamento\n",
        "    model_pca.train()\n",
        "    model_pca.zero_grad()\n",
        "    y = model_pca(X)\n",
        "    loss = loss_function(y, d)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Validação\n",
        "    model_pca.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      y_val = model_pca(x_val_pca)\n",
        "      val_loss = loss_function(y_val, d_val_tensor)\n",
        "\n",
        "    losses.append(loss.item())\n",
        "    val_losses.append(val_loss.item())\n",
        "\n",
        "    if epoch % 1 == 0 and n == x_treino_pca.shape[0]//Nb - 1:\n",
        "      print(f\"Epoch: {epoch} | Loss: {loss} | Val. Loss: {val_loss}\")\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(losses)\n",
        "plt.plot(val_losses, alpha=0.8)\n",
        "plt.legend([\"Loss\", \"Val. Loss\"])\n",
        "plt.xlabel(\"Batch\")\n",
        "plt.ylabel(\"Loss\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PavHmvie3hym",
        "outputId": "3cc86ca2-aaf7-4104-e910-d28f4738b5c3"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | Loss: 1.181028127670288 | Val. Loss: 1.2386881113052368\n",
            "Epoch: 1 | Loss: 1.1394513845443726 | Val. Loss: 1.1205092668533325\n",
            "Epoch: 2 | Loss: 1.1473580598831177 | Val. Loss: 1.0223414897918701\n",
            "Epoch: 3 | Loss: 1.088344931602478 | Val. Loss: 0.9397552609443665\n",
            "Epoch: 4 | Loss: 1.1020853519439697 | Val. Loss: 0.8724657893180847\n",
            "Epoch: 5 | Loss: 0.6793640851974487 | Val. Loss: 0.816271960735321\n",
            "Epoch: 6 | Loss: 0.6559228301048279 | Val. Loss: 0.7740383744239807\n",
            "Epoch: 7 | Loss: 0.7019074559211731 | Val. Loss: 0.7408673167228699\n",
            "Epoch: 8 | Loss: 0.6546685695648193 | Val. Loss: 0.713732123374939\n",
            "Epoch: 9 | Loss: 0.7610123157501221 | Val. Loss: 0.691959798336029\n",
            "Epoch: 10 | Loss: 0.6438171863555908 | Val. Loss: 0.6736057996749878\n",
            "Epoch: 11 | Loss: 0.6120687127113342 | Val. Loss: 0.6566123366355896\n",
            "Epoch: 12 | Loss: 0.6841654777526855 | Val. Loss: 0.6429302096366882\n",
            "Epoch: 13 | Loss: 0.737618088722229 | Val. Loss: 0.6316598653793335\n",
            "Epoch: 14 | Loss: 0.6382169127464294 | Val. Loss: 0.6202307939529419\n",
            "Epoch: 15 | Loss: 0.6026009917259216 | Val. Loss: 0.6105293035507202\n",
            "Epoch: 16 | Loss: 0.6125006079673767 | Val. Loss: 0.6010080575942993\n",
            "Epoch: 17 | Loss: 0.5761602520942688 | Val. Loss: 0.5918701887130737\n",
            "Epoch: 18 | Loss: 0.6163094639778137 | Val. Loss: 0.583503246307373\n",
            "Epoch: 19 | Loss: 0.6299599409103394 | Val. Loss: 0.5751103162765503\n",
            "Epoch: 20 | Loss: 0.6074880957603455 | Val. Loss: 0.5675851106643677\n",
            "Epoch: 21 | Loss: 0.6040428280830383 | Val. Loss: 0.5605979561805725\n",
            "Epoch: 22 | Loss: 0.5530714392662048 | Val. Loss: 0.5537036657333374\n",
            "Epoch: 23 | Loss: 0.5527486801147461 | Val. Loss: 0.5472612977027893\n",
            "Epoch: 24 | Loss: 0.4970511794090271 | Val. Loss: 0.5412808060646057\n",
            "Epoch: 25 | Loss: 0.5236401557922363 | Val. Loss: 0.5354177355766296\n",
            "Epoch: 26 | Loss: 0.5738426446914673 | Val. Loss: 0.5296612977981567\n",
            "Epoch: 27 | Loss: 0.476955771446228 | Val. Loss: 0.5236800909042358\n",
            "Epoch: 28 | Loss: 0.42891162633895874 | Val. Loss: 0.5174151659011841\n",
            "Epoch: 29 | Loss: 0.528389573097229 | Val. Loss: 0.5108556151390076\n",
            "Epoch: 30 | Loss: 0.5352158546447754 | Val. Loss: 0.504263162612915\n",
            "Epoch: 31 | Loss: 0.5267060399055481 | Val. Loss: 0.4978271424770355\n",
            "Epoch: 32 | Loss: 0.5376996397972107 | Val. Loss: 0.4916306138038635\n",
            "Epoch: 33 | Loss: 0.4880906343460083 | Val. Loss: 0.4860093593597412\n",
            "Epoch: 34 | Loss: 0.48167359828948975 | Val. Loss: 0.4805074632167816\n",
            "Epoch: 35 | Loss: 0.4281519949436188 | Val. Loss: 0.4752795696258545\n",
            "Epoch: 36 | Loss: 0.4518539309501648 | Val. Loss: 0.470954030752182\n",
            "Epoch: 37 | Loss: 0.4667139947414398 | Val. Loss: 0.4663800299167633\n",
            "Epoch: 38 | Loss: 0.40336328744888306 | Val. Loss: 0.46251922845840454\n",
            "Epoch: 39 | Loss: 0.4343830943107605 | Val. Loss: 0.4585857391357422\n",
            "Epoch: 40 | Loss: 0.5391595959663391 | Val. Loss: 0.45402294397354126\n",
            "Epoch: 41 | Loss: 0.41781288385391235 | Val. Loss: 0.4497585892677307\n",
            "Epoch: 42 | Loss: 0.4537459909915924 | Val. Loss: 0.4451792240142822\n",
            "Epoch: 43 | Loss: 0.5039129257202148 | Val. Loss: 0.44041362404823303\n",
            "Epoch: 44 | Loss: 0.389385849237442 | Val. Loss: 0.4350174069404602\n",
            "Epoch: 45 | Loss: 0.4446386992931366 | Val. Loss: 0.43022027611732483\n",
            "Epoch: 46 | Loss: 0.40433451533317566 | Val. Loss: 0.4260721802711487\n",
            "Epoch: 47 | Loss: 0.38509663939476013 | Val. Loss: 0.42165452241897583\n",
            "Epoch: 48 | Loss: 0.45141589641571045 | Val. Loss: 0.41770139336586\n",
            "Epoch: 49 | Loss: 0.4301590919494629 | Val. Loss: 0.41406235098838806\n",
            "Epoch: 50 | Loss: 0.40793147683143616 | Val. Loss: 0.4103991389274597\n",
            "Epoch: 51 | Loss: 0.4043769836425781 | Val. Loss: 0.4068654179573059\n",
            "Epoch: 52 | Loss: 0.4041701555252075 | Val. Loss: 0.4037966728210449\n",
            "Epoch: 53 | Loss: 0.4516585171222687 | Val. Loss: 0.4006325304508209\n",
            "Epoch: 54 | Loss: 0.42166200280189514 | Val. Loss: 0.397500604391098\n",
            "Epoch: 55 | Loss: 0.4539472460746765 | Val. Loss: 0.39438849687576294\n",
            "Epoch: 56 | Loss: 0.38283008337020874 | Val. Loss: 0.391896516084671\n",
            "Epoch: 57 | Loss: 0.3956811726093292 | Val. Loss: 0.38930144906044006\n",
            "Epoch: 58 | Loss: 0.3595627546310425 | Val. Loss: 0.38630470633506775\n",
            "Epoch: 59 | Loss: 0.41700783371925354 | Val. Loss: 0.3834095597267151\n",
            "Epoch: 60 | Loss: 0.5257585644721985 | Val. Loss: 0.3808823823928833\n",
            "Epoch: 61 | Loss: 0.29564744234085083 | Val. Loss: 0.3774718642234802\n",
            "Epoch: 62 | Loss: 0.4265475869178772 | Val. Loss: 0.37535762786865234\n",
            "Epoch: 63 | Loss: 0.3499613404273987 | Val. Loss: 0.37305063009262085\n",
            "Epoch: 64 | Loss: 0.3781823515892029 | Val. Loss: 0.3703703284263611\n",
            "Epoch: 65 | Loss: 0.2794512212276459 | Val. Loss: 0.3680352568626404\n",
            "Epoch: 66 | Loss: 0.35517460107803345 | Val. Loss: 0.3661220967769623\n",
            "Epoch: 67 | Loss: 0.43846800923347473 | Val. Loss: 0.3644561171531677\n",
            "Epoch: 68 | Loss: 0.31328147649765015 | Val. Loss: 0.3628120720386505\n",
            "Epoch: 69 | Loss: 0.347067654132843 | Val. Loss: 0.360574334859848\n",
            "Epoch: 70 | Loss: 0.41087812185287476 | Val. Loss: 0.3592853248119354\n",
            "Epoch: 71 | Loss: 0.38701021671295166 | Val. Loss: 0.3565030097961426\n",
            "Epoch: 72 | Loss: 0.2167384922504425 | Val. Loss: 0.35469523072242737\n",
            "Epoch: 73 | Loss: 0.3085747957229614 | Val. Loss: 0.3529393672943115\n",
            "Epoch: 74 | Loss: 0.4206470549106598 | Val. Loss: 0.3519018888473511\n",
            "Epoch: 75 | Loss: 0.40509578585624695 | Val. Loss: 0.3499806225299835\n",
            "Epoch: 76 | Loss: 0.3090592622756958 | Val. Loss: 0.3477899730205536\n",
            "Epoch: 77 | Loss: 0.27887991070747375 | Val. Loss: 0.34705033898353577\n",
            "Epoch: 78 | Loss: 0.29388654232025146 | Val. Loss: 0.3450395464897156\n",
            "Epoch: 79 | Loss: 0.31958338618278503 | Val. Loss: 0.3436906039714813\n",
            "Epoch: 80 | Loss: 0.4192798137664795 | Val. Loss: 0.3419509828090668\n",
            "Epoch: 81 | Loss: 0.26791930198669434 | Val. Loss: 0.3402301073074341\n",
            "Epoch: 82 | Loss: 0.44161468744277954 | Val. Loss: 0.3390123248100281\n",
            "Epoch: 83 | Loss: 0.4861069619655609 | Val. Loss: 0.3381110429763794\n",
            "Epoch: 84 | Loss: 0.3431107997894287 | Val. Loss: 0.33679693937301636\n",
            "Epoch: 85 | Loss: 0.41685032844543457 | Val. Loss: 0.33576536178588867\n",
            "Epoch: 86 | Loss: 0.29221591353416443 | Val. Loss: 0.33463332056999207\n",
            "Epoch: 87 | Loss: 0.33816614747047424 | Val. Loss: 0.33316895365715027\n",
            "Epoch: 88 | Loss: 0.4152040183544159 | Val. Loss: 0.3319817781448364\n",
            "Epoch: 89 | Loss: 0.28276893496513367 | Val. Loss: 0.33086854219436646\n",
            "Epoch: 90 | Loss: 0.33933696150779724 | Val. Loss: 0.3300250470638275\n",
            "Epoch: 91 | Loss: 0.29596057534217834 | Val. Loss: 0.3291375935077667\n",
            "Epoch: 92 | Loss: 0.39340516924858093 | Val. Loss: 0.3278821110725403\n",
            "Epoch: 93 | Loss: 0.27966979146003723 | Val. Loss: 0.32691338658332825\n",
            "Epoch: 94 | Loss: 0.3761807084083557 | Val. Loss: 0.32603269815444946\n",
            "Epoch: 95 | Loss: 0.3970676064491272 | Val. Loss: 0.32485431432724\n",
            "Epoch: 96 | Loss: 0.39067894220352173 | Val. Loss: 0.32416868209838867\n",
            "Epoch: 97 | Loss: 0.25330260396003723 | Val. Loss: 0.3229043483734131\n",
            "Epoch: 98 | Loss: 0.2756192982196808 | Val. Loss: 0.32208094000816345\n",
            "Epoch: 99 | Loss: 0.23457764089107513 | Val. Loss: 0.3207221031188965\n",
            "Epoch: 100 | Loss: 0.30155977606773376 | Val. Loss: 0.3198191523551941\n",
            "Epoch: 101 | Loss: 0.28111332654953003 | Val. Loss: 0.3187974691390991\n",
            "Epoch: 102 | Loss: 0.33565282821655273 | Val. Loss: 0.3177514374256134\n",
            "Epoch: 103 | Loss: 0.2303241342306137 | Val. Loss: 0.31706780195236206\n",
            "Epoch: 104 | Loss: 0.23709814250469208 | Val. Loss: 0.3157942593097687\n",
            "Epoch: 105 | Loss: 0.2761448621749878 | Val. Loss: 0.3145189881324768\n",
            "Epoch: 106 | Loss: 0.25613921880722046 | Val. Loss: 0.3134969472885132\n",
            "Epoch: 107 | Loss: 0.3346533477306366 | Val. Loss: 0.3125748932361603\n",
            "Epoch: 108 | Loss: 0.2849671542644501 | Val. Loss: 0.31194061040878296\n",
            "Epoch: 109 | Loss: 0.42257562279701233 | Val. Loss: 0.3107496201992035\n",
            "Epoch: 110 | Loss: 0.29488345980644226 | Val. Loss: 0.30981776118278503\n",
            "Epoch: 111 | Loss: 0.27139943838119507 | Val. Loss: 0.30870920419692993\n",
            "Epoch: 112 | Loss: 0.3944112956523895 | Val. Loss: 0.30802085995674133\n",
            "Epoch: 113 | Loss: 0.24218638241291046 | Val. Loss: 0.3065089285373688\n",
            "Epoch: 114 | Loss: 0.254107803106308 | Val. Loss: 0.3060770630836487\n",
            "Epoch: 115 | Loss: 0.27574077248573303 | Val. Loss: 0.30560925602912903\n",
            "Epoch: 116 | Loss: 0.3807520270347595 | Val. Loss: 0.30433759093284607\n",
            "Epoch: 117 | Loss: 0.2538354992866516 | Val. Loss: 0.30388665199279785\n",
            "Epoch: 118 | Loss: 0.3172459900379181 | Val. Loss: 0.30323266983032227\n",
            "Epoch: 119 | Loss: 0.3436329662799835 | Val. Loss: 0.3024412989616394\n",
            "Epoch: 120 | Loss: 0.20893408358097076 | Val. Loss: 0.30107542872428894\n",
            "Epoch: 121 | Loss: 0.24423952400684357 | Val. Loss: 0.3004860281944275\n",
            "Epoch: 122 | Loss: 0.3551751673221588 | Val. Loss: 0.30020153522491455\n",
            "Epoch: 123 | Loss: 0.2964683175086975 | Val. Loss: 0.2997308373451233\n",
            "Epoch: 124 | Loss: 0.44542795419692993 | Val. Loss: 0.299237459897995\n",
            "Epoch: 125 | Loss: 0.2663291394710541 | Val. Loss: 0.2977517247200012\n",
            "Epoch: 126 | Loss: 0.3210342526435852 | Val. Loss: 0.29743218421936035\n",
            "Epoch: 127 | Loss: 0.19758345186710358 | Val. Loss: 0.2971805930137634\n",
            "Epoch: 128 | Loss: 0.3920862376689911 | Val. Loss: 0.29615506529808044\n",
            "Epoch: 129 | Loss: 0.231354221701622 | Val. Loss: 0.29540199041366577\n",
            "Epoch: 130 | Loss: 0.23248803615570068 | Val. Loss: 0.2945716381072998\n",
            "Epoch: 131 | Loss: 0.19503237307071686 | Val. Loss: 0.29455384612083435\n",
            "Epoch: 132 | Loss: 0.31210264563560486 | Val. Loss: 0.2934199869632721\n",
            "Epoch: 133 | Loss: 0.2560194432735443 | Val. Loss: 0.2929815649986267\n",
            "Epoch: 134 | Loss: 0.30406999588012695 | Val. Loss: 0.29272156953811646\n",
            "Epoch: 135 | Loss: 0.1861266791820526 | Val. Loss: 0.29183000326156616\n",
            "Epoch: 136 | Loss: 0.32591569423675537 | Val. Loss: 0.2916889786720276\n",
            "Epoch: 137 | Loss: 0.3035222887992859 | Val. Loss: 0.29154130816459656\n",
            "Epoch: 138 | Loss: 0.32745984196662903 | Val. Loss: 0.2907666563987732\n",
            "Epoch: 139 | Loss: 0.2497638463973999 | Val. Loss: 0.2898823916912079\n",
            "Epoch: 140 | Loss: 0.3888203203678131 | Val. Loss: 0.28901422023773193\n",
            "Epoch: 141 | Loss: 0.2884673774242401 | Val. Loss: 0.2887076735496521\n",
            "Epoch: 142 | Loss: 0.250362753868103 | Val. Loss: 0.287720263004303\n",
            "Epoch: 143 | Loss: 0.3041905164718628 | Val. Loss: 0.2866678833961487\n",
            "Epoch: 144 | Loss: 0.28386083245277405 | Val. Loss: 0.28669899702072144\n",
            "Epoch: 145 | Loss: 0.24612557888031006 | Val. Loss: 0.2858597934246063\n",
            "Epoch: 146 | Loss: 0.3604808449745178 | Val. Loss: 0.2862563133239746\n",
            "Epoch: 147 | Loss: 0.18145594000816345 | Val. Loss: 0.28482556343078613\n",
            "Epoch: 148 | Loss: 0.3420591652393341 | Val. Loss: 0.2848138213157654\n",
            "Epoch: 149 | Loss: 0.2511846423149109 | Val. Loss: 0.2843460738658905\n",
            "Epoch: 150 | Loss: 0.2903823256492615 | Val. Loss: 0.28331121802330017\n",
            "Epoch: 151 | Loss: 0.19847357273101807 | Val. Loss: 0.28315696120262146\n",
            "Epoch: 152 | Loss: 0.37312379479408264 | Val. Loss: 0.28322339057922363\n",
            "Epoch: 153 | Loss: 0.25062522292137146 | Val. Loss: 0.28201109170913696\n",
            "Epoch: 154 | Loss: 0.39574965834617615 | Val. Loss: 0.2812705934047699\n",
            "Epoch: 155 | Loss: 0.23447050154209137 | Val. Loss: 0.2813188135623932\n",
            "Epoch: 156 | Loss: 0.15681008994579315 | Val. Loss: 0.280836284160614\n",
            "Epoch: 157 | Loss: 0.3122309446334839 | Val. Loss: 0.27988648414611816\n",
            "Epoch: 158 | Loss: 0.19662810862064362 | Val. Loss: 0.27916452288627625\n",
            "Epoch: 159 | Loss: 0.3883952498435974 | Val. Loss: 0.27893686294555664\n",
            "Epoch: 160 | Loss: 0.2174597531557083 | Val. Loss: 0.27839791774749756\n",
            "Epoch: 161 | Loss: 0.14100895822048187 | Val. Loss: 0.27747902274131775\n",
            "Epoch: 162 | Loss: 0.2356514036655426 | Val. Loss: 0.2773284316062927\n",
            "Epoch: 163 | Loss: 0.2811383306980133 | Val. Loss: 0.276836633682251\n",
            "Epoch: 164 | Loss: 0.356715589761734 | Val. Loss: 0.2767990231513977\n",
            "Epoch: 165 | Loss: 0.426935076713562 | Val. Loss: 0.2769247889518738\n",
            "Epoch: 166 | Loss: 0.2321733981370926 | Val. Loss: 0.27588194608688354\n",
            "Epoch: 167 | Loss: 0.2508692443370819 | Val. Loss: 0.27563077211380005\n",
            "Epoch: 168 | Loss: 0.281573086977005 | Val. Loss: 0.274414598941803\n",
            "Epoch: 169 | Loss: 0.2903899550437927 | Val. Loss: 0.2738967835903168\n",
            "Epoch: 170 | Loss: 0.22283196449279785 | Val. Loss: 0.27326107025146484\n",
            "Epoch: 171 | Loss: 0.20059187710285187 | Val. Loss: 0.27316197752952576\n",
            "Epoch: 172 | Loss: 0.20278705656528473 | Val. Loss: 0.2731153964996338\n",
            "Epoch: 173 | Loss: 0.2330610156059265 | Val. Loss: 0.2727162837982178\n",
            "Epoch: 174 | Loss: 0.3129364252090454 | Val. Loss: 0.2729572653770447\n",
            "Epoch: 175 | Loss: 0.3163684904575348 | Val. Loss: 0.272421658039093\n",
            "Epoch: 176 | Loss: 0.18533504009246826 | Val. Loss: 0.27095332741737366\n",
            "Epoch: 177 | Loss: 0.2521742284297943 | Val. Loss: 0.2704983353614807\n",
            "Epoch: 178 | Loss: 0.294739693403244 | Val. Loss: 0.27045854926109314\n",
            "Epoch: 179 | Loss: 0.26358145475387573 | Val. Loss: 0.2705128788948059\n",
            "Epoch: 180 | Loss: 0.14275747537612915 | Val. Loss: 0.2689809501171112\n",
            "Epoch: 181 | Loss: 0.33589500188827515 | Val. Loss: 0.2686721682548523\n",
            "Epoch: 182 | Loss: 0.22889293730258942 | Val. Loss: 0.2694869041442871\n",
            "Epoch: 183 | Loss: 0.27932730317115784 | Val. Loss: 0.26836973428726196\n",
            "Epoch: 184 | Loss: 0.25212207436561584 | Val. Loss: 0.2674873471260071\n",
            "Epoch: 185 | Loss: 0.261940598487854 | Val. Loss: 0.26792779564857483\n",
            "Epoch: 186 | Loss: 0.31207457184791565 | Val. Loss: 0.2675374150276184\n",
            "Epoch: 187 | Loss: 0.32448869943618774 | Val. Loss: 0.2663131356239319\n",
            "Epoch: 188 | Loss: 0.2003365457057953 | Val. Loss: 0.26602938771247864\n",
            "Epoch: 189 | Loss: 0.2845938205718994 | Val. Loss: 0.26566988229751587\n",
            "Epoch: 190 | Loss: 0.22281789779663086 | Val. Loss: 0.265516459941864\n",
            "Epoch: 191 | Loss: 0.2615501284599304 | Val. Loss: 0.2647206485271454\n",
            "Epoch: 192 | Loss: 0.22145935893058777 | Val. Loss: 0.2648107409477234\n",
            "Epoch: 193 | Loss: 0.32413870096206665 | Val. Loss: 0.2640460133552551\n",
            "Epoch: 194 | Loss: 0.42999327182769775 | Val. Loss: 0.26343658566474915\n",
            "Epoch: 195 | Loss: 0.22448842227458954 | Val. Loss: 0.2631063163280487\n",
            "Epoch: 196 | Loss: 0.260999858379364 | Val. Loss: 0.2623259127140045\n",
            "Epoch: 197 | Loss: 0.35678455233573914 | Val. Loss: 0.2623981833457947\n",
            "Epoch: 198 | Loss: 0.258382111787796 | Val. Loss: 0.2612949013710022\n",
            "Epoch: 199 | Loss: 0.2048691213130951 | Val. Loss: 0.2605755925178528\n",
            "Epoch: 200 | Loss: 0.2952767610549927 | Val. Loss: 0.2600836455821991\n",
            "Epoch: 201 | Loss: 0.30629974603652954 | Val. Loss: 0.2600943148136139\n",
            "Epoch: 202 | Loss: 0.28064480423927307 | Val. Loss: 0.2598569691181183\n",
            "Epoch: 203 | Loss: 0.2564319968223572 | Val. Loss: 0.2600509226322174\n",
            "Epoch: 204 | Loss: 0.27274590730667114 | Val. Loss: 0.25858283042907715\n",
            "Epoch: 205 | Loss: 0.21895666420459747 | Val. Loss: 0.2576066553592682\n",
            "Epoch: 206 | Loss: 0.29561010003089905 | Val. Loss: 0.25812047719955444\n",
            "Epoch: 207 | Loss: 0.1950196623802185 | Val. Loss: 0.2574159502983093\n",
            "Epoch: 208 | Loss: 0.253265380859375 | Val. Loss: 0.2567443251609802\n",
            "Epoch: 209 | Loss: 0.1469622105360031 | Val. Loss: 0.2556614577770233\n",
            "Epoch: 210 | Loss: 0.24108006060123444 | Val. Loss: 0.25535404682159424\n",
            "Epoch: 211 | Loss: 0.3083091378211975 | Val. Loss: 0.25481462478637695\n",
            "Epoch: 212 | Loss: 0.27150213718414307 | Val. Loss: 0.25454604625701904\n",
            "Epoch: 213 | Loss: 0.2645125091075897 | Val. Loss: 0.2543715834617615\n",
            "Epoch: 214 | Loss: 0.3463297188282013 | Val. Loss: 0.25369808077812195\n",
            "Epoch: 215 | Loss: 0.18201859295368195 | Val. Loss: 0.2532094717025757\n",
            "Epoch: 216 | Loss: 0.18267634510993958 | Val. Loss: 0.2518641948699951\n",
            "Epoch: 217 | Loss: 0.17878679931163788 | Val. Loss: 0.25230473279953003\n",
            "Epoch: 218 | Loss: 0.2429456114768982 | Val. Loss: 0.2509821355342865\n",
            "Epoch: 219 | Loss: 0.2107449769973755 | Val. Loss: 0.2507546544075012\n",
            "Epoch: 220 | Loss: 0.29322534799575806 | Val. Loss: 0.2502157390117645\n",
            "Epoch: 221 | Loss: 0.22674694657325745 | Val. Loss: 0.2498856484889984\n",
            "Epoch: 222 | Loss: 0.1953539401292801 | Val. Loss: 0.250060498714447\n",
            "Epoch: 223 | Loss: 0.22390180826187134 | Val. Loss: 0.24963326752185822\n",
            "Epoch: 224 | Loss: 0.13663895428180695 | Val. Loss: 0.24867987632751465\n",
            "Epoch: 225 | Loss: 0.22465495765209198 | Val. Loss: 0.24847301840782166\n",
            "Epoch: 226 | Loss: 0.11423993855714798 | Val. Loss: 0.24846389889717102\n",
            "Epoch: 227 | Loss: 0.2699379324913025 | Val. Loss: 0.24813511967658997\n",
            "Epoch: 228 | Loss: 0.17153756320476532 | Val. Loss: 0.24797217547893524\n",
            "Epoch: 229 | Loss: 0.16079583764076233 | Val. Loss: 0.2474047690629959\n",
            "Epoch: 230 | Loss: 0.23645943403244019 | Val. Loss: 0.24773399531841278\n",
            "Epoch: 231 | Loss: 0.18842501938343048 | Val. Loss: 0.24638967216014862\n",
            "Epoch: 232 | Loss: 0.3994733989238739 | Val. Loss: 0.2467953860759735\n",
            "Epoch: 233 | Loss: 0.18392427265644073 | Val. Loss: 0.24785581231117249\n",
            "Epoch: 234 | Loss: 0.25463560223579407 | Val. Loss: 0.2466391623020172\n",
            "Epoch: 235 | Loss: 0.24419866502285004 | Val. Loss: 0.24576568603515625\n",
            "Epoch: 236 | Loss: 0.18600428104400635 | Val. Loss: 0.24605834484100342\n",
            "Epoch: 237 | Loss: 0.2785738408565521 | Val. Loss: 0.2456206977367401\n",
            "Epoch: 238 | Loss: 0.28562766313552856 | Val. Loss: 0.24558579921722412\n",
            "Epoch: 239 | Loss: 0.16061235964298248 | Val. Loss: 0.24702143669128418\n",
            "Epoch: 240 | Loss: 0.18165841698646545 | Val. Loss: 0.2451007068157196\n",
            "Epoch: 241 | Loss: 0.35214731097221375 | Val. Loss: 0.24533052742481232\n",
            "Epoch: 242 | Loss: 0.23282991349697113 | Val. Loss: 0.24551157653331757\n",
            "Epoch: 243 | Loss: 0.268342524766922 | Val. Loss: 0.24473223090171814\n",
            "Epoch: 244 | Loss: 0.1833927035331726 | Val. Loss: 0.24417130649089813\n",
            "Epoch: 245 | Loss: 0.27370086312294006 | Val. Loss: 0.24408206343650818\n",
            "Epoch: 246 | Loss: 0.14497330784797668 | Val. Loss: 0.24409528076648712\n",
            "Epoch: 247 | Loss: 0.3126678466796875 | Val. Loss: 0.24403421580791473\n",
            "Epoch: 248 | Loss: 0.12889379262924194 | Val. Loss: 0.2442205250263214\n",
            "Epoch: 249 | Loss: 0.19956807792186737 | Val. Loss: 0.24482616782188416\n",
            "Epoch: 250 | Loss: 0.22105205059051514 | Val. Loss: 0.24479535222053528\n",
            "Epoch: 251 | Loss: 0.29605457186698914 | Val. Loss: 0.24375756084918976\n",
            "Epoch: 252 | Loss: 0.2604890465736389 | Val. Loss: 0.244167760014534\n",
            "Epoch: 253 | Loss: 0.17464682459831238 | Val. Loss: 0.24415604770183563\n",
            "Epoch: 254 | Loss: 0.21563024818897247 | Val. Loss: 0.24392881989479065\n",
            "Epoch: 255 | Loss: 0.21964244544506073 | Val. Loss: 0.24325108528137207\n",
            "Epoch: 256 | Loss: 0.3913590610027313 | Val. Loss: 0.2432883232831955\n",
            "Epoch: 257 | Loss: 0.31350263953208923 | Val. Loss: 0.24306173622608185\n",
            "Epoch: 258 | Loss: 0.20856569707393646 | Val. Loss: 0.2431964874267578\n",
            "Epoch: 259 | Loss: 0.26707693934440613 | Val. Loss: 0.24300137162208557\n",
            "Epoch: 260 | Loss: 0.2241724729537964 | Val. Loss: 0.24282506108283997\n",
            "Epoch: 261 | Loss: 0.23006395995616913 | Val. Loss: 0.24361903965473175\n",
            "Epoch: 262 | Loss: 0.22984103858470917 | Val. Loss: 0.2442154586315155\n",
            "Epoch: 263 | Loss: 0.18388478457927704 | Val. Loss: 0.24402913451194763\n",
            "Epoch: 264 | Loss: 0.17009590566158295 | Val. Loss: 0.24250511825084686\n",
            "Epoch: 265 | Loss: 0.1815892606973648 | Val. Loss: 0.2427479773759842\n",
            "Epoch: 266 | Loss: 0.2191978096961975 | Val. Loss: 0.2418299913406372\n",
            "Epoch: 267 | Loss: 0.11469750851392746 | Val. Loss: 0.2425878793001175\n",
            "Epoch: 268 | Loss: 0.10018005222082138 | Val. Loss: 0.24266843497753143\n",
            "Epoch: 269 | Loss: 0.17136283218860626 | Val. Loss: 0.24354460835456848\n",
            "Epoch: 270 | Loss: 0.30340859293937683 | Val. Loss: 0.24305060505867004\n",
            "Epoch: 271 | Loss: 0.2464277446269989 | Val. Loss: 0.24201352894306183\n",
            "Epoch: 272 | Loss: 0.20527587831020355 | Val. Loss: 0.2428266555070877\n",
            "Epoch: 273 | Loss: 0.12020368874073029 | Val. Loss: 0.24208569526672363\n",
            "Epoch: 274 | Loss: 0.3145222067832947 | Val. Loss: 0.24398890137672424\n",
            "Epoch: 275 | Loss: 0.22211287915706635 | Val. Loss: 0.24252450466156006\n",
            "Epoch: 276 | Loss: 0.19019608199596405 | Val. Loss: 0.24164064228534698\n",
            "Epoch: 277 | Loss: 0.2265823632478714 | Val. Loss: 0.24141672253608704\n",
            "Epoch: 278 | Loss: 0.2567199170589447 | Val. Loss: 0.24275508522987366\n",
            "Epoch: 279 | Loss: 0.26121774315834045 | Val. Loss: 0.24138125777244568\n",
            "Epoch: 280 | Loss: 0.3455568552017212 | Val. Loss: 0.24172985553741455\n",
            "Epoch: 281 | Loss: 0.1969682276248932 | Val. Loss: 0.2416028082370758\n",
            "Epoch: 282 | Loss: 0.30618733167648315 | Val. Loss: 0.24085506796836853\n",
            "Epoch: 283 | Loss: 0.23421263694763184 | Val. Loss: 0.24098297953605652\n",
            "Epoch: 284 | Loss: 0.2581528127193451 | Val. Loss: 0.24187929928302765\n",
            "Epoch: 285 | Loss: 0.17185069620609283 | Val. Loss: 0.24059204757213593\n",
            "Epoch: 286 | Loss: 0.22176922857761383 | Val. Loss: 0.241156667470932\n",
            "Epoch: 287 | Loss: 0.1572238802909851 | Val. Loss: 0.24047987163066864\n",
            "Epoch: 288 | Loss: 0.150129497051239 | Val. Loss: 0.24213333427906036\n",
            "Epoch: 289 | Loss: 0.16840484738349915 | Val. Loss: 0.24006354808807373\n",
            "Epoch: 290 | Loss: 0.3164311647415161 | Val. Loss: 0.2398243397474289\n",
            "Epoch: 291 | Loss: 0.21102498471736908 | Val. Loss: 0.24101033806800842\n",
            "Epoch: 292 | Loss: 0.20002536475658417 | Val. Loss: 0.23965267837047577\n",
            "Epoch: 293 | Loss: 0.24497458338737488 | Val. Loss: 0.2409227192401886\n",
            "Epoch: 294 | Loss: 0.14152920246124268 | Val. Loss: 0.24011531472206116\n",
            "Epoch: 295 | Loss: 0.20502837002277374 | Val. Loss: 0.2398742139339447\n",
            "Epoch: 296 | Loss: 0.19023707509040833 | Val. Loss: 0.239942267537117\n",
            "Epoch: 297 | Loss: 0.20134484767913818 | Val. Loss: 0.2399294376373291\n",
            "Epoch: 298 | Loss: 0.24015948176383972 | Val. Loss: 0.23906472325325012\n",
            "Epoch: 299 | Loss: 0.23793575167655945 | Val. Loss: 0.23987111449241638\n",
            "Epoch: 300 | Loss: 0.13124647736549377 | Val. Loss: 0.24006251990795135\n",
            "Epoch: 301 | Loss: 0.19404567778110504 | Val. Loss: 0.24029915034770966\n",
            "Epoch: 302 | Loss: 0.1410951018333435 | Val. Loss: 0.23909911513328552\n",
            "Epoch: 303 | Loss: 0.18483372032642365 | Val. Loss: 0.23909687995910645\n",
            "Epoch: 304 | Loss: 0.14406707882881165 | Val. Loss: 0.23855534195899963\n",
            "Epoch: 305 | Loss: 0.1347772777080536 | Val. Loss: 0.23928788304328918\n",
            "Epoch: 306 | Loss: 0.26355409622192383 | Val. Loss: 0.23951809108257294\n",
            "Epoch: 307 | Loss: 0.1866929829120636 | Val. Loss: 0.2386999875307083\n",
            "Epoch: 308 | Loss: 0.2257760614156723 | Val. Loss: 0.23890456557273865\n",
            "Epoch: 309 | Loss: 0.3214000463485718 | Val. Loss: 0.23854529857635498\n",
            "Epoch: 310 | Loss: 0.24211466312408447 | Val. Loss: 0.2383500039577484\n",
            "Epoch: 311 | Loss: 0.3085019290447235 | Val. Loss: 0.23770348727703094\n",
            "Epoch: 312 | Loss: 0.27313169836997986 | Val. Loss: 0.23920121788978577\n",
            "Epoch: 313 | Loss: 0.14022143185138702 | Val. Loss: 0.23834434151649475\n",
            "Epoch: 314 | Loss: 0.1425315886735916 | Val. Loss: 0.2380957305431366\n",
            "Epoch: 315 | Loss: 0.20682448148727417 | Val. Loss: 0.2377462387084961\n",
            "Epoch: 316 | Loss: 0.2104247659444809 | Val. Loss: 0.23650972545146942\n",
            "Epoch: 317 | Loss: 0.25114139914512634 | Val. Loss: 0.23645088076591492\n",
            "Epoch: 318 | Loss: 0.16254067420959473 | Val. Loss: 0.23719143867492676\n",
            "Epoch: 319 | Loss: 0.13903966546058655 | Val. Loss: 0.23705288767814636\n",
            "Epoch: 320 | Loss: 0.15436944365501404 | Val. Loss: 0.235108882188797\n",
            "Epoch: 321 | Loss: 0.30323994159698486 | Val. Loss: 0.23507389426231384\n",
            "Epoch: 322 | Loss: 0.21737521886825562 | Val. Loss: 0.23533454537391663\n",
            "Epoch: 323 | Loss: 0.25193026661872864 | Val. Loss: 0.23548170924186707\n",
            "Epoch: 324 | Loss: 0.17676150798797607 | Val. Loss: 0.2350085973739624\n",
            "Epoch: 325 | Loss: 0.32102200388908386 | Val. Loss: 0.23597972095012665\n",
            "Epoch: 326 | Loss: 0.2439735233783722 | Val. Loss: 0.23464329540729523\n",
            "Epoch: 327 | Loss: 0.21803513169288635 | Val. Loss: 0.23476508259773254\n",
            "Epoch: 328 | Loss: 0.23365502059459686 | Val. Loss: 0.2352011650800705\n",
            "Epoch: 329 | Loss: 0.1473899632692337 | Val. Loss: 0.23460109531879425\n",
            "Epoch: 330 | Loss: 0.28492820262908936 | Val. Loss: 0.23453228175640106\n",
            "Epoch: 331 | Loss: 0.18176597356796265 | Val. Loss: 0.2337108552455902\n",
            "Epoch: 332 | Loss: 0.17536549270153046 | Val. Loss: 0.23472245037555695\n",
            "Epoch: 333 | Loss: 0.12875665724277496 | Val. Loss: 0.23494359850883484\n",
            "Epoch: 334 | Loss: 0.12491103261709213 | Val. Loss: 0.2330012023448944\n",
            "Epoch: 335 | Loss: 0.2375570833683014 | Val. Loss: 0.23363420367240906\n",
            "Epoch: 336 | Loss: 0.17073360085487366 | Val. Loss: 0.23513607680797577\n",
            "Epoch: 337 | Loss: 0.21267782151699066 | Val. Loss: 0.234427809715271\n",
            "Epoch: 338 | Loss: 0.15768307447433472 | Val. Loss: 0.23357701301574707\n",
            "Epoch: 339 | Loss: 0.16104073822498322 | Val. Loss: 0.2331712245941162\n",
            "Epoch: 340 | Loss: 0.20107969641685486 | Val. Loss: 0.23350465297698975\n",
            "Epoch: 341 | Loss: 0.21034222841262817 | Val. Loss: 0.23222604393959045\n",
            "Epoch: 342 | Loss: 0.16080503165721893 | Val. Loss: 0.23333367705345154\n",
            "Epoch: 343 | Loss: 0.14485636353492737 | Val. Loss: 0.233372300863266\n",
            "Epoch: 344 | Loss: 0.22025437653064728 | Val. Loss: 0.2342674285173416\n",
            "Epoch: 345 | Loss: 0.25014686584472656 | Val. Loss: 0.23302677273750305\n",
            "Epoch: 346 | Loss: 0.13393235206604004 | Val. Loss: 0.2329166829586029\n",
            "Epoch: 347 | Loss: 0.12964296340942383 | Val. Loss: 0.23357422649860382\n",
            "Epoch: 348 | Loss: 0.1714024394750595 | Val. Loss: 0.2323349416255951\n",
            "Epoch: 349 | Loss: 0.25569432973861694 | Val. Loss: 0.23299749195575714\n",
            "Epoch: 350 | Loss: 0.18730367720127106 | Val. Loss: 0.23307888209819794\n",
            "Epoch: 351 | Loss: 0.19552545249462128 | Val. Loss: 0.23289351165294647\n",
            "Epoch: 352 | Loss: 0.1869104653596878 | Val. Loss: 0.23200766742229462\n",
            "Epoch: 353 | Loss: 0.2834054231643677 | Val. Loss: 0.23344600200653076\n",
            "Epoch: 354 | Loss: 0.23443825542926788 | Val. Loss: 0.23300786316394806\n",
            "Epoch: 355 | Loss: 0.1496439278125763 | Val. Loss: 0.23282456398010254\n",
            "Epoch: 356 | Loss: 0.1842106431722641 | Val. Loss: 0.23235642910003662\n",
            "Epoch: 357 | Loss: 0.23223340511322021 | Val. Loss: 0.23153090476989746\n",
            "Epoch: 358 | Loss: 0.10640845447778702 | Val. Loss: 0.23214073479175568\n",
            "Epoch: 359 | Loss: 0.2576129138469696 | Val. Loss: 0.23188403248786926\n",
            "Epoch: 360 | Loss: 0.20276865363121033 | Val. Loss: 0.2312586009502411\n",
            "Epoch: 361 | Loss: 0.22841541469097137 | Val. Loss: 0.2310052365064621\n",
            "Epoch: 362 | Loss: 0.11701228469610214 | Val. Loss: 0.2284034937620163\n",
            "Epoch: 363 | Loss: 0.14365248382091522 | Val. Loss: 0.2280941754579544\n",
            "Epoch: 364 | Loss: 0.17607557773590088 | Val. Loss: 0.22843404114246368\n",
            "Epoch: 365 | Loss: 0.16690939664840698 | Val. Loss: 0.22982828319072723\n",
            "Epoch: 366 | Loss: 0.20246750116348267 | Val. Loss: 0.22792311012744904\n",
            "Epoch: 367 | Loss: 0.12651094794273376 | Val. Loss: 0.23015423119068146\n",
            "Epoch: 368 | Loss: 0.19738832116127014 | Val. Loss: 0.2296328991651535\n",
            "Epoch: 369 | Loss: 0.18047821521759033 | Val. Loss: 0.22944669425487518\n",
            "Epoch: 370 | Loss: 0.20972514152526855 | Val. Loss: 0.22880569100379944\n",
            "Epoch: 371 | Loss: 0.13332252204418182 | Val. Loss: 0.2299797534942627\n",
            "Epoch: 372 | Loss: 0.21232686936855316 | Val. Loss: 0.22941112518310547\n",
            "Epoch: 373 | Loss: 0.22136875987052917 | Val. Loss: 0.22904375195503235\n",
            "Epoch: 374 | Loss: 0.1641528308391571 | Val. Loss: 0.22971773147583008\n",
            "Epoch: 375 | Loss: 0.1856691688299179 | Val. Loss: 0.2285492867231369\n",
            "Epoch: 376 | Loss: 0.2358849197626114 | Val. Loss: 0.22807912528514862\n",
            "Epoch: 377 | Loss: 0.18498271703720093 | Val. Loss: 0.2285904437303543\n",
            "Epoch: 378 | Loss: 0.27874282002449036 | Val. Loss: 0.22916631400585175\n",
            "Epoch: 379 | Loss: 0.24510496854782104 | Val. Loss: 0.2291174829006195\n",
            "Epoch: 380 | Loss: 0.2868128716945648 | Val. Loss: 0.22877052426338196\n",
            "Epoch: 381 | Loss: 0.21199040114879608 | Val. Loss: 0.22885413467884064\n",
            "Epoch: 382 | Loss: 0.17339560389518738 | Val. Loss: 0.22880764305591583\n",
            "Epoch: 383 | Loss: 0.15127664804458618 | Val. Loss: 0.227995827794075\n",
            "Epoch: 384 | Loss: 0.24141329526901245 | Val. Loss: 0.2275312840938568\n",
            "Epoch: 385 | Loss: 0.14535009860992432 | Val. Loss: 0.2279023379087448\n",
            "Epoch: 386 | Loss: 0.19057653844356537 | Val. Loss: 0.2280501425266266\n",
            "Epoch: 387 | Loss: 0.19462566077709198 | Val. Loss: 0.22900912165641785\n",
            "Epoch: 388 | Loss: 0.08645487576723099 | Val. Loss: 0.22905881702899933\n",
            "Epoch: 389 | Loss: 0.14681623876094818 | Val. Loss: 0.22822923958301544\n",
            "Epoch: 390 | Loss: 0.24757346510887146 | Val. Loss: 0.22803524136543274\n",
            "Epoch: 391 | Loss: 0.1507691591978073 | Val. Loss: 0.2291618138551712\n",
            "Epoch: 392 | Loss: 0.16168756783008575 | Val. Loss: 0.22878225147724152\n",
            "Epoch: 393 | Loss: 0.20416396856307983 | Val. Loss: 0.22851069271564484\n",
            "Epoch: 394 | Loss: 0.3423071801662445 | Val. Loss: 0.22925159335136414\n",
            "Epoch: 395 | Loss: 0.19956094026565552 | Val. Loss: 0.22882454097270966\n",
            "Epoch: 396 | Loss: 0.1864832192659378 | Val. Loss: 0.22837913036346436\n",
            "Epoch: 397 | Loss: 0.19139601290225983 | Val. Loss: 0.22848351299762726\n",
            "Epoch: 398 | Loss: 0.29009193181991577 | Val. Loss: 0.2281123846769333\n",
            "Epoch: 399 | Loss: 0.08004657179117203 | Val. Loss: 0.22851082682609558\n",
            "Epoch: 400 | Loss: 0.20712830126285553 | Val. Loss: 0.22840702533721924\n",
            "Epoch: 401 | Loss: 0.09245165437459946 | Val. Loss: 0.22855627536773682\n",
            "Epoch: 402 | Loss: 0.12028581649065018 | Val. Loss: 0.22864100337028503\n",
            "Epoch: 403 | Loss: 0.1874874383211136 | Val. Loss: 0.22770142555236816\n",
            "Epoch: 404 | Loss: 0.07162228226661682 | Val. Loss: 0.22891731560230255\n",
            "Epoch: 405 | Loss: 0.24261297285556793 | Val. Loss: 0.22856085002422333\n",
            "Epoch: 406 | Loss: 0.29144901037216187 | Val. Loss: 0.22826340794563293\n",
            "Epoch: 407 | Loss: 0.11392799764871597 | Val. Loss: 0.22799217700958252\n",
            "Epoch: 408 | Loss: 0.1633356809616089 | Val. Loss: 0.22831586003303528\n",
            "Epoch: 409 | Loss: 0.17247769236564636 | Val. Loss: 0.2288908064365387\n",
            "Epoch: 410 | Loss: 0.2456517368555069 | Val. Loss: 0.22866372764110565\n",
            "Epoch: 411 | Loss: 0.09509574621915817 | Val. Loss: 0.22756949067115784\n",
            "Epoch: 412 | Loss: 0.14351218938827515 | Val. Loss: 0.22746959328651428\n",
            "Epoch: 413 | Loss: 0.13859392702579498 | Val. Loss: 0.22794854640960693\n",
            "Epoch: 414 | Loss: 0.18805459141731262 | Val. Loss: 0.2283671796321869\n",
            "Epoch: 415 | Loss: 0.22096329927444458 | Val. Loss: 0.22904792428016663\n",
            "Epoch: 416 | Loss: 0.19221775233745575 | Val. Loss: 0.22820785641670227\n",
            "Epoch: 417 | Loss: 0.12338277697563171 | Val. Loss: 0.22771494090557098\n",
            "Epoch: 418 | Loss: 0.07047568261623383 | Val. Loss: 0.22689679265022278\n",
            "Epoch: 419 | Loss: 0.13260690867900848 | Val. Loss: 0.22622890770435333\n",
            "Epoch: 420 | Loss: 0.2384236454963684 | Val. Loss: 0.22642095386981964\n",
            "Epoch: 421 | Loss: 0.2423069030046463 | Val. Loss: 0.22737927734851837\n",
            "Epoch: 422 | Loss: 0.1286121904850006 | Val. Loss: 0.2264081984758377\n",
            "Epoch: 423 | Loss: 0.1573951542377472 | Val. Loss: 0.22688183188438416\n",
            "Epoch: 424 | Loss: 0.23756229877471924 | Val. Loss: 0.22611919045448303\n",
            "Epoch: 425 | Loss: 0.08343245089054108 | Val. Loss: 0.22651541233062744\n",
            "Epoch: 426 | Loss: 0.1347435712814331 | Val. Loss: 0.22534136474132538\n",
            "Epoch: 427 | Loss: 0.20329375565052032 | Val. Loss: 0.22742895781993866\n",
            "Epoch: 428 | Loss: 0.09143278747797012 | Val. Loss: 0.22623614966869354\n",
            "Epoch: 429 | Loss: 0.20761919021606445 | Val. Loss: 0.2264794558286667\n",
            "Epoch: 430 | Loss: 0.17497709393501282 | Val. Loss: 0.22563707828521729\n",
            "Epoch: 431 | Loss: 0.27006304264068604 | Val. Loss: 0.22541049122810364\n",
            "Epoch: 432 | Loss: 0.15907412767410278 | Val. Loss: 0.22599394619464874\n",
            "Epoch: 433 | Loss: 0.22599472105503082 | Val. Loss: 0.22558382153511047\n",
            "Epoch: 434 | Loss: 0.16620691120624542 | Val. Loss: 0.22428178787231445\n",
            "Epoch: 435 | Loss: 0.18769480288028717 | Val. Loss: 0.225002720952034\n",
            "Epoch: 436 | Loss: 0.1275523155927658 | Val. Loss: 0.2250467836856842\n",
            "Epoch: 437 | Loss: 0.1699645072221756 | Val. Loss: 0.22457543015480042\n",
            "Epoch: 438 | Loss: 0.08141229301691055 | Val. Loss: 0.22412702441215515\n",
            "Epoch: 439 | Loss: 0.17717844247817993 | Val. Loss: 0.22524943947792053\n",
            "Epoch: 440 | Loss: 0.11027316749095917 | Val. Loss: 0.22576852142810822\n",
            "Epoch: 441 | Loss: 0.14069806039333344 | Val. Loss: 0.22392956912517548\n",
            "Epoch: 442 | Loss: 0.23654955625534058 | Val. Loss: 0.22461195290088654\n",
            "Epoch: 443 | Loss: 0.13002760708332062 | Val. Loss: 0.2240825593471527\n",
            "Epoch: 444 | Loss: 0.15515737235546112 | Val. Loss: 0.2243233621120453\n",
            "Epoch: 445 | Loss: 0.21446268260478973 | Val. Loss: 0.2243095189332962\n",
            "Epoch: 446 | Loss: 0.21236683428287506 | Val. Loss: 0.22498436272144318\n",
            "Epoch: 447 | Loss: 0.12026273459196091 | Val. Loss: 0.22425386309623718\n",
            "Epoch: 448 | Loss: 0.13026340305805206 | Val. Loss: 0.2244766652584076\n",
            "Epoch: 449 | Loss: 0.136999249458313 | Val. Loss: 0.2241538017988205\n",
            "Epoch: 450 | Loss: 0.23285257816314697 | Val. Loss: 0.22331157326698303\n",
            "Epoch: 451 | Loss: 0.06277770549058914 | Val. Loss: 0.22333741188049316\n",
            "Epoch: 452 | Loss: 0.15903235971927643 | Val. Loss: 0.22378821671009064\n",
            "Epoch: 453 | Loss: 0.15293745696544647 | Val. Loss: 0.22362938523292542\n",
            "Epoch: 454 | Loss: 0.19048325717449188 | Val. Loss: 0.22375090420246124\n",
            "Epoch: 455 | Loss: 0.08432786911725998 | Val. Loss: 0.2234024554491043\n",
            "Epoch: 456 | Loss: 0.08003944158554077 | Val. Loss: 0.22245745360851288\n",
            "Epoch: 457 | Loss: 0.1154683455824852 | Val. Loss: 0.22398877143859863\n",
            "Epoch: 458 | Loss: 0.15161234140396118 | Val. Loss: 0.22404208779335022\n",
            "Epoch: 459 | Loss: 0.0958302766084671 | Val. Loss: 0.22274765372276306\n",
            "Epoch: 460 | Loss: 0.260915607213974 | Val. Loss: 0.22343961894512177\n",
            "Epoch: 461 | Loss: 0.14040562510490417 | Val. Loss: 0.22416183352470398\n",
            "Epoch: 462 | Loss: 0.18577922880649567 | Val. Loss: 0.2250361144542694\n",
            "Epoch: 463 | Loss: 0.17142386734485626 | Val. Loss: 0.2237299233675003\n",
            "Epoch: 464 | Loss: 0.2059008777141571 | Val. Loss: 0.22256068885326385\n",
            "Epoch: 465 | Loss: 0.1691025048494339 | Val. Loss: 0.2218715250492096\n",
            "Epoch: 466 | Loss: 0.3215530812740326 | Val. Loss: 0.2245846539735794\n",
            "Epoch: 467 | Loss: 0.19678020477294922 | Val. Loss: 0.22297115623950958\n",
            "Epoch: 468 | Loss: 0.24011170864105225 | Val. Loss: 0.22449174523353577\n",
            "Epoch: 469 | Loss: 0.1646728664636612 | Val. Loss: 0.223488450050354\n",
            "Epoch: 470 | Loss: 0.1994057446718216 | Val. Loss: 0.22331321239471436\n",
            "Epoch: 471 | Loss: 0.11546874046325684 | Val. Loss: 0.22169137001037598\n",
            "Epoch: 472 | Loss: 0.09559529274702072 | Val. Loss: 0.22137603163719177\n",
            "Epoch: 473 | Loss: 0.17677918076515198 | Val. Loss: 0.22153310477733612\n",
            "Epoch: 474 | Loss: 0.10132647305727005 | Val. Loss: 0.22335036098957062\n",
            "Epoch: 475 | Loss: 0.18715405464172363 | Val. Loss: 0.22336260974407196\n",
            "Epoch: 476 | Loss: 0.3175038695335388 | Val. Loss: 0.22241608798503876\n",
            "Epoch: 477 | Loss: 0.1652696430683136 | Val. Loss: 0.2223096340894699\n",
            "Epoch: 478 | Loss: 0.09958041459321976 | Val. Loss: 0.22308504581451416\n",
            "Epoch: 479 | Loss: 0.12590406835079193 | Val. Loss: 0.22172704339027405\n",
            "Epoch: 480 | Loss: 0.11579223722219467 | Val. Loss: 0.22069957852363586\n",
            "Epoch: 481 | Loss: 0.1933501958847046 | Val. Loss: 0.2222566157579422\n",
            "Epoch: 482 | Loss: 0.15855802595615387 | Val. Loss: 0.22184410691261292\n",
            "Epoch: 483 | Loss: 0.1765352040529251 | Val. Loss: 0.22189994156360626\n",
            "Epoch: 484 | Loss: 0.08587180078029633 | Val. Loss: 0.22107894718647003\n",
            "Epoch: 485 | Loss: 0.23365581035614014 | Val. Loss: 0.22051408886909485\n",
            "Epoch: 486 | Loss: 0.24788007140159607 | Val. Loss: 0.22231800854206085\n",
            "Epoch: 487 | Loss: 0.1922398805618286 | Val. Loss: 0.2204873263835907\n",
            "Epoch: 488 | Loss: 0.19282908737659454 | Val. Loss: 0.22158701717853546\n",
            "Epoch: 489 | Loss: 0.07088261097669601 | Val. Loss: 0.2215244323015213\n",
            "Epoch: 490 | Loss: 0.1872512847185135 | Val. Loss: 0.2207862138748169\n",
            "Epoch: 491 | Loss: 0.24580207467079163 | Val. Loss: 0.22357411682605743\n",
            "Epoch: 492 | Loss: 0.19464623928070068 | Val. Loss: 0.22421924769878387\n",
            "Epoch: 493 | Loss: 0.1293543130159378 | Val. Loss: 0.22412022948265076\n",
            "Epoch: 494 | Loss: 0.14041399955749512 | Val. Loss: 0.22653329372406006\n",
            "Epoch: 495 | Loss: 0.09178688377141953 | Val. Loss: 0.22637033462524414\n",
            "Epoch: 496 | Loss: 0.14780795574188232 | Val. Loss: 0.2257692515850067\n",
            "Epoch: 497 | Loss: 0.06874195486307144 | Val. Loss: 0.2259780913591385\n",
            "Epoch: 498 | Loss: 0.10123911499977112 | Val. Loss: 0.22572879493236542\n",
            "Epoch: 499 | Loss: 0.07891379296779633 | Val. Loss: 0.22495467960834503\n",
            "Epoch: 500 | Loss: 0.07121338695287704 | Val. Loss: 0.2237720787525177\n",
            "Epoch: 501 | Loss: 0.09424048662185669 | Val. Loss: 0.22447355091571808\n",
            "Epoch: 502 | Loss: 0.15455524623394012 | Val. Loss: 0.22313955426216125\n",
            "Epoch: 503 | Loss: 0.10062364488840103 | Val. Loss: 0.22329263389110565\n",
            "Epoch: 504 | Loss: 0.10060465335845947 | Val. Loss: 0.22267398238182068\n",
            "Epoch: 505 | Loss: 0.21792282164096832 | Val. Loss: 0.22431012988090515\n",
            "Epoch: 506 | Loss: 0.23972772061824799 | Val. Loss: 0.22176416218280792\n",
            "Epoch: 507 | Loss: 0.16892372071743011 | Val. Loss: 0.2206626832485199\n",
            "Epoch: 508 | Loss: 0.23602700233459473 | Val. Loss: 0.22173848748207092\n",
            "Epoch: 509 | Loss: 0.07346799969673157 | Val. Loss: 0.22116999328136444\n",
            "Epoch: 510 | Loss: 0.20396341383457184 | Val. Loss: 0.2210526168346405\n",
            "Epoch: 511 | Loss: 0.12750652432441711 | Val. Loss: 0.2207566797733307\n",
            "Epoch: 512 | Loss: 0.15147310495376587 | Val. Loss: 0.22010359168052673\n",
            "Epoch: 513 | Loss: 0.11664432287216187 | Val. Loss: 0.22002577781677246\n",
            "Epoch: 514 | Loss: 0.118321493268013 | Val. Loss: 0.22086739540100098\n",
            "Epoch: 515 | Loss: 0.17551706731319427 | Val. Loss: 0.2211376428604126\n",
            "Epoch: 516 | Loss: 0.09898781776428223 | Val. Loss: 0.21883758902549744\n",
            "Epoch: 517 | Loss: 0.17134128510951996 | Val. Loss: 0.21786467730998993\n",
            "Epoch: 518 | Loss: 0.15633107721805573 | Val. Loss: 0.2188987284898758\n",
            "Epoch: 519 | Loss: 0.14051532745361328 | Val. Loss: 0.21798816323280334\n",
            "Epoch: 520 | Loss: 0.1098756343126297 | Val. Loss: 0.21821609139442444\n",
            "Epoch: 521 | Loss: 0.059894904494285583 | Val. Loss: 0.21801801025867462\n",
            "Epoch: 522 | Loss: 0.16021203994750977 | Val. Loss: 0.21808572113513947\n",
            "Epoch: 523 | Loss: 0.12536662817001343 | Val. Loss: 0.21765124797821045\n",
            "Epoch: 524 | Loss: 0.1250738650560379 | Val. Loss: 0.21809646487236023\n",
            "Epoch: 525 | Loss: 0.16519509255886078 | Val. Loss: 0.21686199307441711\n",
            "Epoch: 526 | Loss: 0.19284304976463318 | Val. Loss: 0.21608516573905945\n",
            "Epoch: 527 | Loss: 0.048306915909051895 | Val. Loss: 0.21684353053569794\n",
            "Epoch: 528 | Loss: 0.19359974563121796 | Val. Loss: 0.21603448688983917\n",
            "Epoch: 529 | Loss: 0.164035364985466 | Val. Loss: 0.21608766913414001\n",
            "Epoch: 530 | Loss: 0.11956460773944855 | Val. Loss: 0.21516267955303192\n",
            "Epoch: 531 | Loss: 0.14374090731143951 | Val. Loss: 0.21751639246940613\n",
            "Epoch: 532 | Loss: 0.2596927881240845 | Val. Loss: 0.21608158946037292\n",
            "Epoch: 533 | Loss: 0.19039562344551086 | Val. Loss: 0.2185678780078888\n",
            "Epoch: 534 | Loss: 0.1130034402012825 | Val. Loss: 0.2168995440006256\n",
            "Epoch: 535 | Loss: 0.09448069334030151 | Val. Loss: 0.21620996296405792\n",
            "Epoch: 536 | Loss: 0.20471005141735077 | Val. Loss: 0.21821486949920654\n",
            "Epoch: 537 | Loss: 0.1550191342830658 | Val. Loss: 0.21604414284229279\n",
            "Epoch: 538 | Loss: 0.10079140961170197 | Val. Loss: 0.21506814658641815\n",
            "Epoch: 539 | Loss: 0.16431866586208344 | Val. Loss: 0.2147570550441742\n",
            "Epoch: 540 | Loss: 0.14152662456035614 | Val. Loss: 0.2144193947315216\n",
            "Epoch: 541 | Loss: 0.1501174122095108 | Val. Loss: 0.21546503901481628\n",
            "Epoch: 542 | Loss: 0.20374974608421326 | Val. Loss: 0.2160080373287201\n",
            "Epoch: 543 | Loss: 0.11582984775304794 | Val. Loss: 0.2144489735364914\n",
            "Epoch: 544 | Loss: 0.11398778855800629 | Val. Loss: 0.21515992283821106\n",
            "Epoch: 545 | Loss: 0.08300285786390305 | Val. Loss: 0.21328651905059814\n",
            "Epoch: 546 | Loss: 0.18792560696601868 | Val. Loss: 0.2134101390838623\n",
            "Epoch: 547 | Loss: 0.09689398854970932 | Val. Loss: 0.2129945307970047\n",
            "Epoch: 548 | Loss: 0.3214161694049835 | Val. Loss: 0.21382124722003937\n",
            "Epoch: 549 | Loss: 0.12558181583881378 | Val. Loss: 0.21323934197425842\n",
            "Epoch: 550 | Loss: 0.18800029158592224 | Val. Loss: 0.21475303173065186\n",
            "Epoch: 551 | Loss: 0.1752358227968216 | Val. Loss: 0.2140919268131256\n",
            "Epoch: 552 | Loss: 0.07207020372152328 | Val. Loss: 0.213255375623703\n",
            "Epoch: 553 | Loss: 0.08651182055473328 | Val. Loss: 0.21314319968223572\n",
            "Epoch: 554 | Loss: 0.07297568023204803 | Val. Loss: 0.21362102031707764\n",
            "Epoch: 555 | Loss: 0.24399158358573914 | Val. Loss: 0.21459634602069855\n",
            "Epoch: 556 | Loss: 0.11388106644153595 | Val. Loss: 0.2133590281009674\n",
            "Epoch: 557 | Loss: 0.16379186511039734 | Val. Loss: 0.21278634667396545\n",
            "Epoch: 558 | Loss: 0.2177029252052307 | Val. Loss: 0.2127666026353836\n",
            "Epoch: 559 | Loss: 0.22133223712444305 | Val. Loss: 0.2112191915512085\n",
            "Epoch: 560 | Loss: 0.15968948602676392 | Val. Loss: 0.21209052205085754\n",
            "Epoch: 561 | Loss: 0.20912498235702515 | Val. Loss: 0.21250124275684357\n",
            "Epoch: 562 | Loss: 0.10866152495145798 | Val. Loss: 0.21209561824798584\n",
            "Epoch: 563 | Loss: 0.21210554242134094 | Val. Loss: 0.21175238490104675\n",
            "Epoch: 564 | Loss: 0.10833384841680527 | Val. Loss: 0.21070942282676697\n",
            "Epoch: 565 | Loss: 0.19587334990501404 | Val. Loss: 0.21164700388908386\n",
            "Epoch: 566 | Loss: 0.15197937190532684 | Val. Loss: 0.21063673496246338\n",
            "Epoch: 567 | Loss: 0.12828783690929413 | Val. Loss: 0.21011467278003693\n",
            "Epoch: 568 | Loss: 0.10662280023097992 | Val. Loss: 0.2119409590959549\n",
            "Epoch: 569 | Loss: 0.12003504484891891 | Val. Loss: 0.21134579181671143\n",
            "Epoch: 570 | Loss: 0.13701269030570984 | Val. Loss: 0.2116512954235077\n",
            "Epoch: 571 | Loss: 0.07359032332897186 | Val. Loss: 0.21101073920726776\n",
            "Epoch: 572 | Loss: 0.2133621871471405 | Val. Loss: 0.21236935257911682\n",
            "Epoch: 573 | Loss: 0.14568662643432617 | Val. Loss: 0.2105155736207962\n",
            "Epoch: 574 | Loss: 0.055736444890499115 | Val. Loss: 0.21219420433044434\n",
            "Epoch: 575 | Loss: 0.14920677244663239 | Val. Loss: 0.2097204625606537\n",
            "Epoch: 576 | Loss: 0.09438971430063248 | Val. Loss: 0.20850257575511932\n",
            "Epoch: 577 | Loss: 0.3274179995059967 | Val. Loss: 0.21036723256111145\n",
            "Epoch: 578 | Loss: 0.08036907017230988 | Val. Loss: 0.2111743688583374\n",
            "Epoch: 579 | Loss: 0.19139070808887482 | Val. Loss: 0.21142439544200897\n",
            "Epoch: 580 | Loss: 0.15231260657310486 | Val. Loss: 0.21005940437316895\n",
            "Epoch: 581 | Loss: 0.2046332061290741 | Val. Loss: 0.20873527228832245\n",
            "Epoch: 582 | Loss: 0.16981621086597443 | Val. Loss: 0.20921003818511963\n",
            "Epoch: 583 | Loss: 0.1402556598186493 | Val. Loss: 0.20995958149433136\n",
            "Epoch: 584 | Loss: 0.1568908542394638 | Val. Loss: 0.2111799716949463\n",
            "Epoch: 585 | Loss: 0.075849249958992 | Val. Loss: 0.20880670845508575\n",
            "Epoch: 586 | Loss: 0.06060769408941269 | Val. Loss: 0.2087463140487671\n",
            "Epoch: 587 | Loss: 0.20817798376083374 | Val. Loss: 0.20993240177631378\n",
            "Epoch: 588 | Loss: 0.12226314842700958 | Val. Loss: 0.2099742442369461\n",
            "Epoch: 589 | Loss: 0.15490682423114777 | Val. Loss: 0.21050134301185608\n",
            "Epoch: 590 | Loss: 0.12689906358718872 | Val. Loss: 0.2069358080625534\n",
            "Epoch: 591 | Loss: 0.16029126942157745 | Val. Loss: 0.2068643569946289\n",
            "Epoch: 592 | Loss: 0.08721327036619186 | Val. Loss: 0.20803019404411316\n",
            "Epoch: 593 | Loss: 0.09945018589496613 | Val. Loss: 0.2066478282213211\n",
            "Epoch: 594 | Loss: 0.06636474281549454 | Val. Loss: 0.20750518143177032\n",
            "Epoch: 595 | Loss: 0.13576871156692505 | Val. Loss: 0.2082560509443283\n",
            "Epoch: 596 | Loss: 0.20435571670532227 | Val. Loss: 0.2074735462665558\n",
            "Epoch: 597 | Loss: 0.23005251586437225 | Val. Loss: 0.2049664556980133\n",
            "Epoch: 598 | Loss: 0.18184898793697357 | Val. Loss: 0.20583823323249817\n",
            "Epoch: 599 | Loss: 0.1755383461713791 | Val. Loss: 0.20697030425071716\n",
            "Epoch: 600 | Loss: 0.13778138160705566 | Val. Loss: 0.206328347325325\n",
            "Epoch: 601 | Loss: 0.11110588908195496 | Val. Loss: 0.20572039484977722\n",
            "Epoch: 602 | Loss: 0.1622646600008011 | Val. Loss: 0.20336441695690155\n",
            "Epoch: 603 | Loss: 0.10876020044088364 | Val. Loss: 0.20354604721069336\n",
            "Epoch: 604 | Loss: 0.20512211322784424 | Val. Loss: 0.20532825589179993\n",
            "Epoch: 605 | Loss: 0.14909066259860992 | Val. Loss: 0.20556330680847168\n",
            "Epoch: 606 | Loss: 0.12549519538879395 | Val. Loss: 0.2042558640241623\n",
            "Epoch: 607 | Loss: 0.20142145454883575 | Val. Loss: 0.20545606315135956\n",
            "Epoch: 608 | Loss: 0.23029038310050964 | Val. Loss: 0.20436573028564453\n",
            "Epoch: 609 | Loss: 0.17660023272037506 | Val. Loss: 0.20461943745613098\n",
            "Epoch: 610 | Loss: 0.18259702622890472 | Val. Loss: 0.20305399596691132\n",
            "Epoch: 611 | Loss: 0.17178651690483093 | Val. Loss: 0.20293211936950684\n",
            "Epoch: 612 | Loss: 0.08652995526790619 | Val. Loss: 0.2024955004453659\n",
            "Epoch: 613 | Loss: 0.18468187749385834 | Val. Loss: 0.20313839614391327\n",
            "Epoch: 614 | Loss: 0.10600777715444565 | Val. Loss: 0.20296534895896912\n",
            "Epoch: 615 | Loss: 0.16072851419448853 | Val. Loss: 0.20167812705039978\n",
            "Epoch: 616 | Loss: 0.19429734349250793 | Val. Loss: 0.20085720717906952\n",
            "Epoch: 617 | Loss: 0.10897450149059296 | Val. Loss: 0.1991978883743286\n",
            "Epoch: 618 | Loss: 0.23904505372047424 | Val. Loss: 0.20120272040367126\n",
            "Epoch: 619 | Loss: 0.10009687393903732 | Val. Loss: 0.20119738578796387\n",
            "Epoch: 620 | Loss: 0.15326817333698273 | Val. Loss: 0.20216092467308044\n",
            "Epoch: 621 | Loss: 0.0844908133149147 | Val. Loss: 0.2014915645122528\n",
            "Epoch: 622 | Loss: 0.0863889530301094 | Val. Loss: 0.20006021857261658\n",
            "Epoch: 623 | Loss: 0.07234733551740646 | Val. Loss: 0.20043177902698517\n",
            "Epoch: 624 | Loss: 0.14211054146289825 | Val. Loss: 0.20040598511695862\n",
            "Epoch: 625 | Loss: 0.07413458824157715 | Val. Loss: 0.2006310224533081\n",
            "Epoch: 626 | Loss: 0.0916358232498169 | Val. Loss: 0.19954437017440796\n",
            "Epoch: 627 | Loss: 0.23043063282966614 | Val. Loss: 0.20155449211597443\n",
            "Epoch: 628 | Loss: 0.10082852840423584 | Val. Loss: 0.20093314349651337\n",
            "Epoch: 629 | Loss: 0.2255244255065918 | Val. Loss: 0.20074239373207092\n",
            "Epoch: 630 | Loss: 0.25920921564102173 | Val. Loss: 0.19950255751609802\n",
            "Epoch: 631 | Loss: 0.1789645552635193 | Val. Loss: 0.19766245782375336\n",
            "Epoch: 632 | Loss: 0.1816709190607071 | Val. Loss: 0.19773194193840027\n",
            "Epoch: 633 | Loss: 0.11948487907648087 | Val. Loss: 0.19782032072544098\n",
            "Epoch: 634 | Loss: 0.1706433892250061 | Val. Loss: 0.1997479796409607\n",
            "Epoch: 635 | Loss: 0.08806762844324112 | Val. Loss: 0.1964043825864792\n",
            "Epoch: 636 | Loss: 0.06508893519639969 | Val. Loss: 0.19699449837207794\n",
            "Epoch: 637 | Loss: 0.11647604405879974 | Val. Loss: 0.19627411663532257\n",
            "Epoch: 638 | Loss: 0.10028442740440369 | Val. Loss: 0.19618801772594452\n",
            "Epoch: 639 | Loss: 0.18176642060279846 | Val. Loss: 0.1965186893939972\n",
            "Epoch: 640 | Loss: 0.13704080879688263 | Val. Loss: 0.19754454493522644\n",
            "Epoch: 641 | Loss: 0.0852448120713234 | Val. Loss: 0.19648997485637665\n",
            "Epoch: 642 | Loss: 0.054539840668439865 | Val. Loss: 0.1958620250225067\n",
            "Epoch: 643 | Loss: 0.19209277629852295 | Val. Loss: 0.19444581866264343\n",
            "Epoch: 644 | Loss: 0.13753856718540192 | Val. Loss: 0.19521841406822205\n",
            "Epoch: 645 | Loss: 0.13614921271800995 | Val. Loss: 0.19623848795890808\n",
            "Epoch: 646 | Loss: 0.12266437709331512 | Val. Loss: 0.19670428335666656\n",
            "Epoch: 647 | Loss: 0.19175930321216583 | Val. Loss: 0.196369469165802\n",
            "Epoch: 648 | Loss: 0.08423656225204468 | Val. Loss: 0.19529414176940918\n",
            "Epoch: 649 | Loss: 0.0630396381020546 | Val. Loss: 0.1946669965982437\n",
            "Epoch: 650 | Loss: 0.0600370392203331 | Val. Loss: 0.1945243775844574\n",
            "Epoch: 651 | Loss: 0.1580096334218979 | Val. Loss: 0.1937870979309082\n",
            "Epoch: 652 | Loss: 0.10035138577222824 | Val. Loss: 0.19407866895198822\n",
            "Epoch: 653 | Loss: 0.13252413272857666 | Val. Loss: 0.19285418093204498\n",
            "Epoch: 654 | Loss: 0.11221393942832947 | Val. Loss: 0.19342045485973358\n",
            "Epoch: 655 | Loss: 0.13139311969280243 | Val. Loss: 0.19264480471611023\n",
            "Epoch: 656 | Loss: 0.13521519303321838 | Val. Loss: 0.1920831799507141\n",
            "Epoch: 657 | Loss: 0.211094930768013 | Val. Loss: 0.1910984367132187\n",
            "Epoch: 658 | Loss: 0.15536077320575714 | Val. Loss: 0.19208526611328125\n",
            "Epoch: 659 | Loss: 0.08133167028427124 | Val. Loss: 0.19310420751571655\n",
            "Epoch: 660 | Loss: 0.11988307535648346 | Val. Loss: 0.19198450446128845\n",
            "Epoch: 661 | Loss: 0.17992214858531952 | Val. Loss: 0.19246216118335724\n",
            "Epoch: 662 | Loss: 0.11382781714200974 | Val. Loss: 0.1919061839580536\n",
            "Epoch: 663 | Loss: 0.1435374617576599 | Val. Loss: 0.19196924567222595\n",
            "Epoch: 664 | Loss: 0.18982020020484924 | Val. Loss: 0.19320876896381378\n",
            "Epoch: 665 | Loss: 0.14353692531585693 | Val. Loss: 0.1926000565290451\n",
            "Epoch: 666 | Loss: 0.2719268798828125 | Val. Loss: 0.1902013123035431\n",
            "Epoch: 667 | Loss: 0.2703113257884979 | Val. Loss: 0.18995459377765656\n",
            "Epoch: 668 | Loss: 0.07812894880771637 | Val. Loss: 0.19077539443969727\n",
            "Epoch: 669 | Loss: 0.06745223701000214 | Val. Loss: 0.18959221243858337\n",
            "Epoch: 670 | Loss: 0.05375225096940994 | Val. Loss: 0.18862594664096832\n",
            "Epoch: 671 | Loss: 0.12151633203029633 | Val. Loss: 0.18839333951473236\n",
            "Epoch: 672 | Loss: 0.056582625955343246 | Val. Loss: 0.18904539942741394\n",
            "Epoch: 673 | Loss: 0.057620156556367874 | Val. Loss: 0.18820475041866302\n",
            "Epoch: 674 | Loss: 0.22057129442691803 | Val. Loss: 0.18664215505123138\n",
            "Epoch: 675 | Loss: 0.14262141287326813 | Val. Loss: 0.18669739365577698\n",
            "Epoch: 676 | Loss: 0.19745899736881256 | Val. Loss: 0.18895544111728668\n",
            "Epoch: 677 | Loss: 0.19105154275894165 | Val. Loss: 0.18911108374595642\n",
            "Epoch: 678 | Loss: 0.15219883620738983 | Val. Loss: 0.1872592717409134\n",
            "Epoch: 679 | Loss: 0.09920307993888855 | Val. Loss: 0.1879618614912033\n",
            "Epoch: 680 | Loss: 0.09624132513999939 | Val. Loss: 0.18604706227779388\n",
            "Epoch: 681 | Loss: 0.16250020265579224 | Val. Loss: 0.18751445412635803\n",
            "Epoch: 682 | Loss: 0.1548265963792801 | Val. Loss: 0.18684956431388855\n",
            "Epoch: 683 | Loss: 0.10978230088949203 | Val. Loss: 0.18664692342281342\n",
            "Epoch: 684 | Loss: 0.128794863820076 | Val. Loss: 0.18616743385791779\n",
            "Epoch: 685 | Loss: 0.16008859872817993 | Val. Loss: 0.18614457547664642\n",
            "Epoch: 686 | Loss: 0.08199720829725266 | Val. Loss: 0.1856519877910614\n",
            "Epoch: 687 | Loss: 0.17654964327812195 | Val. Loss: 0.18506239354610443\n",
            "Epoch: 688 | Loss: 0.08348838984966278 | Val. Loss: 0.18441136181354523\n",
            "Epoch: 689 | Loss: 0.16714386641979218 | Val. Loss: 0.1849389374256134\n",
            "Epoch: 690 | Loss: 0.15292100608348846 | Val. Loss: 0.18445566296577454\n",
            "Epoch: 691 | Loss: 0.20119984447956085 | Val. Loss: 0.1826939880847931\n",
            "Epoch: 692 | Loss: 0.09781239181756973 | Val. Loss: 0.18243584036827087\n",
            "Epoch: 693 | Loss: 0.04538997262716293 | Val. Loss: 0.18256394565105438\n",
            "Epoch: 694 | Loss: 0.09438583999872208 | Val. Loss: 0.1810659021139145\n",
            "Epoch: 695 | Loss: 0.18919797241687775 | Val. Loss: 0.17989230155944824\n",
            "Epoch: 696 | Loss: 0.20638716220855713 | Val. Loss: 0.18466082215309143\n",
            "Epoch: 697 | Loss: 0.058239951729774475 | Val. Loss: 0.18321503698825836\n",
            "Epoch: 698 | Loss: 0.12937788665294647 | Val. Loss: 0.18240134418010712\n",
            "Epoch: 699 | Loss: 0.1616145670413971 | Val. Loss: 0.18360301852226257\n",
            "Epoch: 700 | Loss: 0.13141027092933655 | Val. Loss: 0.1821424961090088\n",
            "Epoch: 701 | Loss: 0.09219037741422653 | Val. Loss: 0.181192547082901\n",
            "Epoch: 702 | Loss: 0.10224583745002747 | Val. Loss: 0.18018902838230133\n",
            "Epoch: 703 | Loss: 0.22162573039531708 | Val. Loss: 0.18174996972084045\n",
            "Epoch: 704 | Loss: 0.096957266330719 | Val. Loss: 0.1808728128671646\n",
            "Epoch: 705 | Loss: 0.2562992572784424 | Val. Loss: 0.1795680820941925\n",
            "Epoch: 706 | Loss: 0.07562486082315445 | Val. Loss: 0.1786482036113739\n",
            "Epoch: 707 | Loss: 0.10094199329614639 | Val. Loss: 0.17825022339820862\n",
            "Epoch: 708 | Loss: 0.19909119606018066 | Val. Loss: 0.17717406153678894\n",
            "Epoch: 709 | Loss: 0.18154914677143097 | Val. Loss: 0.17716647684574127\n",
            "Epoch: 710 | Loss: 0.1763361245393753 | Val. Loss: 0.177220419049263\n",
            "Epoch: 711 | Loss: 0.15525637567043304 | Val. Loss: 0.17800715565681458\n",
            "Epoch: 712 | Loss: 0.05788399651646614 | Val. Loss: 0.17673873901367188\n",
            "Epoch: 713 | Loss: 0.14060509204864502 | Val. Loss: 0.17769715189933777\n",
            "Epoch: 714 | Loss: 0.1452130824327469 | Val. Loss: 0.17571178078651428\n",
            "Epoch: 715 | Loss: 0.19448232650756836 | Val. Loss: 0.17509666085243225\n",
            "Epoch: 716 | Loss: 0.14847494661808014 | Val. Loss: 0.1775428056716919\n",
            "Epoch: 717 | Loss: 0.12114857137203217 | Val. Loss: 0.1749095469713211\n",
            "Epoch: 718 | Loss: 0.15706181526184082 | Val. Loss: 0.1751909703016281\n",
            "Epoch: 719 | Loss: 0.276824951171875 | Val. Loss: 0.1737360805273056\n",
            "Epoch: 720 | Loss: 0.15812411904335022 | Val. Loss: 0.17489835619926453\n",
            "Epoch: 721 | Loss: 0.09925427287817001 | Val. Loss: 0.1737775206565857\n",
            "Epoch: 722 | Loss: 0.1470668911933899 | Val. Loss: 0.17329959571361542\n",
            "Epoch: 723 | Loss: 0.05252070724964142 | Val. Loss: 0.17477813363075256\n",
            "Epoch: 724 | Loss: 0.12675030529499054 | Val. Loss: 0.16989001631736755\n",
            "Epoch: 725 | Loss: 0.09279723465442657 | Val. Loss: 0.17272168397903442\n",
            "Epoch: 726 | Loss: 0.0847490057349205 | Val. Loss: 0.17173348367214203\n",
            "Epoch: 727 | Loss: 0.14162001013755798 | Val. Loss: 0.17328155040740967\n",
            "Epoch: 728 | Loss: 0.18932946026325226 | Val. Loss: 0.171963632106781\n",
            "Epoch: 729 | Loss: 0.045835480093955994 | Val. Loss: 0.17266221344470978\n",
            "Epoch: 730 | Loss: 0.20769460499286652 | Val. Loss: 0.17143014073371887\n",
            "Epoch: 731 | Loss: 0.09527812898159027 | Val. Loss: 0.17312322556972504\n",
            "Epoch: 732 | Loss: 0.1933583915233612 | Val. Loss: 0.1721143126487732\n",
            "Epoch: 733 | Loss: 0.07400200515985489 | Val. Loss: 0.1691538542509079\n",
            "Epoch: 734 | Loss: 0.13712382316589355 | Val. Loss: 0.16911622881889343\n",
            "Epoch: 735 | Loss: 0.20109760761260986 | Val. Loss: 0.17080631852149963\n",
            "Epoch: 736 | Loss: 0.20588535070419312 | Val. Loss: 0.17026285827159882\n",
            "Epoch: 737 | Loss: 0.04525553062558174 | Val. Loss: 0.16663846373558044\n",
            "Epoch: 738 | Loss: 0.2345472127199173 | Val. Loss: 0.16934193670749664\n",
            "Epoch: 739 | Loss: 0.0629597082734108 | Val. Loss: 0.16651791334152222\n",
            "Epoch: 740 | Loss: 0.13736377656459808 | Val. Loss: 0.16687482595443726\n",
            "Epoch: 741 | Loss: 0.15195074677467346 | Val. Loss: 0.16593106091022491\n",
            "Epoch: 742 | Loss: 0.16313524544239044 | Val. Loss: 0.1639757603406906\n",
            "Epoch: 743 | Loss: 0.1291833370923996 | Val. Loss: 0.16418491303920746\n",
            "Epoch: 744 | Loss: 0.2417420595884323 | Val. Loss: 0.1663610339164734\n",
            "Epoch: 745 | Loss: 0.1191008985042572 | Val. Loss: 0.1659502387046814\n",
            "Epoch: 746 | Loss: 0.10676686465740204 | Val. Loss: 0.1624312698841095\n",
            "Epoch: 747 | Loss: 0.10024631768465042 | Val. Loss: 0.1618616133928299\n",
            "Epoch: 748 | Loss: 0.1157204657793045 | Val. Loss: 0.1633877009153366\n",
            "Epoch: 749 | Loss: 0.11928676813840866 | Val. Loss: 0.16431477665901184\n",
            "Epoch: 750 | Loss: 0.09557563066482544 | Val. Loss: 0.1619865894317627\n",
            "Epoch: 751 | Loss: 0.2586708068847656 | Val. Loss: 0.16267696022987366\n",
            "Epoch: 752 | Loss: 0.08632204681634903 | Val. Loss: 0.15865251421928406\n",
            "Epoch: 753 | Loss: 0.09772130846977234 | Val. Loss: 0.15840233862400055\n",
            "Epoch: 754 | Loss: 0.09055740386247635 | Val. Loss: 0.16067712008953094\n",
            "Epoch: 755 | Loss: 0.10493259876966476 | Val. Loss: 0.16092121601104736\n",
            "Epoch: 756 | Loss: 0.04312628135085106 | Val. Loss: 0.15928015112876892\n",
            "Epoch: 757 | Loss: 0.04701527953147888 | Val. Loss: 0.1598241925239563\n",
            "Epoch: 758 | Loss: 0.17279449105262756 | Val. Loss: 0.15865649282932281\n",
            "Epoch: 759 | Loss: 0.23096823692321777 | Val. Loss: 0.15749847888946533\n",
            "Epoch: 760 | Loss: 0.044625043869018555 | Val. Loss: 0.15950451791286469\n",
            "Epoch: 761 | Loss: 0.1683826446533203 | Val. Loss: 0.15885433554649353\n",
            "Epoch: 762 | Loss: 0.10820131003856659 | Val. Loss: 0.1586945503950119\n",
            "Epoch: 763 | Loss: 0.13540877401828766 | Val. Loss: 0.1569756716489792\n",
            "Epoch: 764 | Loss: 0.14081519842147827 | Val. Loss: 0.15544869005680084\n",
            "Epoch: 765 | Loss: 0.08390804380178452 | Val. Loss: 0.15776070952415466\n",
            "Epoch: 766 | Loss: 0.1442182958126068 | Val. Loss: 0.15476804971694946\n",
            "Epoch: 767 | Loss: 0.09091035276651382 | Val. Loss: 0.15449965000152588\n",
            "Epoch: 768 | Loss: 0.1398199498653412 | Val. Loss: 0.1540439873933792\n",
            "Epoch: 769 | Loss: 0.32255494594573975 | Val. Loss: 0.15293432772159576\n",
            "Epoch: 770 | Loss: 0.2719068229198456 | Val. Loss: 0.15686453878879547\n",
            "Epoch: 771 | Loss: 0.04773928225040436 | Val. Loss: 0.15330523252487183\n",
            "Epoch: 772 | Loss: 0.18053652346134186 | Val. Loss: 0.15443089604377747\n",
            "Epoch: 773 | Loss: 0.1739858090877533 | Val. Loss: 0.15422913432121277\n",
            "Epoch: 774 | Loss: 0.19610436260700226 | Val. Loss: 0.1510540395975113\n",
            "Epoch: 775 | Loss: 0.09333942085504532 | Val. Loss: 0.1515885889530182\n",
            "Epoch: 776 | Loss: 0.07717404514551163 | Val. Loss: 0.14937636256217957\n",
            "Epoch: 777 | Loss: 0.19266819953918457 | Val. Loss: 0.15026788413524628\n",
            "Epoch: 778 | Loss: 0.22075727581977844 | Val. Loss: 0.1528037041425705\n",
            "Epoch: 779 | Loss: 0.13762453198432922 | Val. Loss: 0.1510593295097351\n",
            "Epoch: 780 | Loss: 0.10116539150476456 | Val. Loss: 0.1489643007516861\n",
            "Epoch: 781 | Loss: 0.13174974918365479 | Val. Loss: 0.14685587584972382\n",
            "Epoch: 782 | Loss: 0.09170178323984146 | Val. Loss: 0.14862431585788727\n",
            "Epoch: 783 | Loss: 0.03790244087576866 | Val. Loss: 0.14747686684131622\n",
            "Epoch: 784 | Loss: 0.04903523996472359 | Val. Loss: 0.14849123358726501\n",
            "Epoch: 785 | Loss: 0.12943130731582642 | Val. Loss: 0.14946646988391876\n",
            "Epoch: 786 | Loss: 0.17244815826416016 | Val. Loss: 0.14835593104362488\n",
            "Epoch: 787 | Loss: 0.07985711097717285 | Val. Loss: 0.1468031406402588\n",
            "Epoch: 788 | Loss: 0.041998930275440216 | Val. Loss: 0.14587640762329102\n",
            "Epoch: 789 | Loss: 0.07983797788619995 | Val. Loss: 0.14566929638385773\n",
            "Epoch: 790 | Loss: 0.10236594825983047 | Val. Loss: 0.14590752124786377\n",
            "Epoch: 791 | Loss: 0.05286581441760063 | Val. Loss: 0.14687588810920715\n",
            "Epoch: 792 | Loss: 0.08019496500492096 | Val. Loss: 0.14502951502799988\n",
            "Epoch: 793 | Loss: 0.23448336124420166 | Val. Loss: 0.14626792073249817\n",
            "Epoch: 794 | Loss: 0.14028091728687286 | Val. Loss: 0.14446619153022766\n",
            "Epoch: 795 | Loss: 0.13152438402175903 | Val. Loss: 0.14449948072433472\n",
            "Epoch: 796 | Loss: 0.14596101641654968 | Val. Loss: 0.14318326115608215\n",
            "Epoch: 797 | Loss: 0.14889861643314362 | Val. Loss: 0.1451270878314972\n",
            "Epoch: 798 | Loss: 0.14881449937820435 | Val. Loss: 0.1424327790737152\n",
            "Epoch: 799 | Loss: 0.1807572841644287 | Val. Loss: 0.14227195084095\n",
            "Epoch: 800 | Loss: 0.1493842601776123 | Val. Loss: 0.1427043229341507\n",
            "Epoch: 801 | Loss: 0.21483874320983887 | Val. Loss: 0.14184437692165375\n",
            "Epoch: 802 | Loss: 0.11154008656740189 | Val. Loss: 0.14123503863811493\n",
            "Epoch: 803 | Loss: 0.20635125041007996 | Val. Loss: 0.1410653293132782\n",
            "Epoch: 804 | Loss: 0.0887652263045311 | Val. Loss: 0.14062926173210144\n",
            "Epoch: 805 | Loss: 0.17549897730350494 | Val. Loss: 0.14106617867946625\n",
            "Epoch: 806 | Loss: 0.07952546328306198 | Val. Loss: 0.14124509692192078\n",
            "Epoch: 807 | Loss: 0.04030787944793701 | Val. Loss: 0.1396743357181549\n",
            "Epoch: 808 | Loss: 0.22475853562355042 | Val. Loss: 0.1379339098930359\n",
            "Epoch: 809 | Loss: 0.1471860557794571 | Val. Loss: 0.13982464373111725\n",
            "Epoch: 810 | Loss: 0.02779596671462059 | Val. Loss: 0.14043591916561127\n",
            "Epoch: 811 | Loss: 0.07474809885025024 | Val. Loss: 0.13927453756332397\n",
            "Epoch: 812 | Loss: 0.09356005489826202 | Val. Loss: 0.1387874186038971\n",
            "Epoch: 813 | Loss: 0.09341325610876083 | Val. Loss: 0.13734358549118042\n",
            "Epoch: 814 | Loss: 0.0818646252155304 | Val. Loss: 0.1368241012096405\n",
            "Epoch: 815 | Loss: 0.15056146681308746 | Val. Loss: 0.13949289917945862\n",
            "Epoch: 816 | Loss: 0.03650198504328728 | Val. Loss: 0.13728386163711548\n",
            "Epoch: 817 | Loss: 0.2198316752910614 | Val. Loss: 0.13605451583862305\n",
            "Epoch: 818 | Loss: 0.1303422600030899 | Val. Loss: 0.13544529676437378\n",
            "Epoch: 819 | Loss: 0.12402539700269699 | Val. Loss: 0.1361023336648941\n",
            "Epoch: 820 | Loss: 0.048526473343372345 | Val. Loss: 0.13515932857990265\n",
            "Epoch: 821 | Loss: 0.09702546894550323 | Val. Loss: 0.13558556139469147\n",
            "Epoch: 822 | Loss: 0.14371298253536224 | Val. Loss: 0.13618600368499756\n",
            "Epoch: 823 | Loss: 0.05740811675786972 | Val. Loss: 0.1362927258014679\n",
            "Epoch: 824 | Loss: 0.08595290780067444 | Val. Loss: 0.1361987292766571\n",
            "Epoch: 825 | Loss: 0.06571413576602936 | Val. Loss: 0.1366787850856781\n",
            "Epoch: 826 | Loss: 0.17702236771583557 | Val. Loss: 0.13556839525699615\n",
            "Epoch: 827 | Loss: 0.08030983060598373 | Val. Loss: 0.13495534658432007\n",
            "Epoch: 828 | Loss: 0.20780858397483826 | Val. Loss: 0.13333143293857574\n",
            "Epoch: 829 | Loss: 0.1647956818342209 | Val. Loss: 0.13298366963863373\n",
            "Epoch: 830 | Loss: 0.03330153599381447 | Val. Loss: 0.134415403008461\n",
            "Epoch: 831 | Loss: 0.06146799027919769 | Val. Loss: 0.13206800818443298\n",
            "Epoch: 832 | Loss: 0.1582227647304535 | Val. Loss: 0.13264642655849457\n",
            "Epoch: 833 | Loss: 0.048360586166381836 | Val. Loss: 0.1319352388381958\n",
            "Epoch: 834 | Loss: 0.12688010931015015 | Val. Loss: 0.13349421322345734\n",
            "Epoch: 835 | Loss: 0.09341040253639221 | Val. Loss: 0.13351556658744812\n",
            "Epoch: 836 | Loss: 0.18626534938812256 | Val. Loss: 0.13333311676979065\n",
            "Epoch: 837 | Loss: 0.06667990982532501 | Val. Loss: 0.13127121329307556\n",
            "Epoch: 838 | Loss: 0.045518167316913605 | Val. Loss: 0.13177905976772308\n",
            "Epoch: 839 | Loss: 0.04614602029323578 | Val. Loss: 0.13140812516212463\n",
            "Epoch: 840 | Loss: 0.1153125911951065 | Val. Loss: 0.13085028529167175\n",
            "Epoch: 841 | Loss: 0.15861576795578003 | Val. Loss: 0.12997455894947052\n",
            "Epoch: 842 | Loss: 0.053376663476228714 | Val. Loss: 0.1312067210674286\n",
            "Epoch: 843 | Loss: 0.12202239781618118 | Val. Loss: 0.13076429069042206\n",
            "Epoch: 844 | Loss: 0.07685031741857529 | Val. Loss: 0.13130870461463928\n",
            "Epoch: 845 | Loss: 0.1414625644683838 | Val. Loss: 0.12978914380073547\n",
            "Epoch: 846 | Loss: 0.1789693683385849 | Val. Loss: 0.1308516561985016\n",
            "Epoch: 847 | Loss: 0.1237451359629631 | Val. Loss: 0.1293308585882187\n",
            "Epoch: 848 | Loss: 0.0947936400771141 | Val. Loss: 0.12848396599292755\n",
            "Epoch: 849 | Loss: 0.12334410846233368 | Val. Loss: 0.12913452088832855\n",
            "Epoch: 850 | Loss: 0.04601842537522316 | Val. Loss: 0.1294378638267517\n",
            "Epoch: 851 | Loss: 0.04192439466714859 | Val. Loss: 0.12882885336875916\n",
            "Epoch: 852 | Loss: 0.05106038972735405 | Val. Loss: 0.12852641940116882\n",
            "Epoch: 853 | Loss: 0.08257550001144409 | Val. Loss: 0.12885326147079468\n",
            "Epoch: 854 | Loss: 0.23969793319702148 | Val. Loss: 0.12892964482307434\n",
            "Epoch: 855 | Loss: 0.13317632675170898 | Val. Loss: 0.12821920216083527\n",
            "Epoch: 856 | Loss: 0.09298009425401688 | Val. Loss: 0.12766313552856445\n",
            "Epoch: 857 | Loss: 0.16290152072906494 | Val. Loss: 0.12735040485858917\n",
            "Epoch: 858 | Loss: 0.14030689001083374 | Val. Loss: 0.12860217690467834\n",
            "Epoch: 859 | Loss: 0.08271156996488571 | Val. Loss: 0.1268928349018097\n",
            "Epoch: 860 | Loss: 0.13942328095436096 | Val. Loss: 0.12766176462173462\n",
            "Epoch: 861 | Loss: 0.10073339194059372 | Val. Loss: 0.12713059782981873\n",
            "Epoch: 862 | Loss: 0.02857096493244171 | Val. Loss: 0.1272028535604477\n",
            "Epoch: 863 | Loss: 0.13071554899215698 | Val. Loss: 0.12805159389972687\n",
            "Epoch: 864 | Loss: 0.09437532722949982 | Val. Loss: 0.12705712020397186\n",
            "Epoch: 865 | Loss: 0.12976622581481934 | Val. Loss: 0.1262216717004776\n",
            "Epoch: 866 | Loss: 0.04097878932952881 | Val. Loss: 0.1261432021856308\n",
            "Epoch: 867 | Loss: 0.11917386204004288 | Val. Loss: 0.12776248157024384\n",
            "Epoch: 868 | Loss: 0.13711850345134735 | Val. Loss: 0.1259990632534027\n",
            "Epoch: 869 | Loss: 0.13703586161136627 | Val. Loss: 0.12488754093647003\n",
            "Epoch: 870 | Loss: 0.09660450369119644 | Val. Loss: 0.1256379336118698\n",
            "Epoch: 871 | Loss: 0.08787429332733154 | Val. Loss: 0.12587617337703705\n",
            "Epoch: 872 | Loss: 0.03763012960553169 | Val. Loss: 0.12626032531261444\n",
            "Epoch: 873 | Loss: 0.04966212064027786 | Val. Loss: 0.1245218962430954\n",
            "Epoch: 874 | Loss: 0.07738907635211945 | Val. Loss: 0.1251356154680252\n",
            "Epoch: 875 | Loss: 0.12632183730602264 | Val. Loss: 0.1237376481294632\n",
            "Epoch: 876 | Loss: 0.12815862894058228 | Val. Loss: 0.1247047558426857\n",
            "Epoch: 877 | Loss: 0.13227710127830505 | Val. Loss: 0.12321758270263672\n",
            "Epoch: 878 | Loss: 0.08892673254013062 | Val. Loss: 0.12431784719228745\n",
            "Epoch: 879 | Loss: 0.04433523118495941 | Val. Loss: 0.12362625449895859\n",
            "Epoch: 880 | Loss: 0.046716347336769104 | Val. Loss: 0.12432970106601715\n",
            "Epoch: 881 | Loss: 0.22456485033035278 | Val. Loss: 0.12348239123821259\n",
            "Epoch: 882 | Loss: 0.1305195838212967 | Val. Loss: 0.12454278767108917\n",
            "Epoch: 883 | Loss: 0.08248986303806305 | Val. Loss: 0.12352265417575836\n",
            "Epoch: 884 | Loss: 0.12435321509838104 | Val. Loss: 0.12340652942657471\n",
            "Epoch: 885 | Loss: 0.05374059081077576 | Val. Loss: 0.12358318269252777\n",
            "Epoch: 886 | Loss: 0.1402343213558197 | Val. Loss: 0.1245093122124672\n",
            "Epoch: 887 | Loss: 0.07561257481575012 | Val. Loss: 0.1228049248456955\n",
            "Epoch: 888 | Loss: 0.07987725734710693 | Val. Loss: 0.12248142063617706\n",
            "Epoch: 889 | Loss: 0.03760173171758652 | Val. Loss: 0.1225367933511734\n",
            "Epoch: 890 | Loss: 0.22065065801143646 | Val. Loss: 0.12236771732568741\n",
            "Epoch: 891 | Loss: 0.052234895527362823 | Val. Loss: 0.12342605739831924\n",
            "Epoch: 892 | Loss: 0.134247824549675 | Val. Loss: 0.12215016037225723\n",
            "Epoch: 893 | Loss: 0.09506496042013168 | Val. Loss: 0.12159545719623566\n",
            "Epoch: 894 | Loss: 0.14771199226379395 | Val. Loss: 0.12124107033014297\n",
            "Epoch: 895 | Loss: 0.03385722637176514 | Val. Loss: 0.12081823498010635\n",
            "Epoch: 896 | Loss: 0.04478760063648224 | Val. Loss: 0.12194521725177765\n",
            "Epoch: 897 | Loss: 0.07114621996879578 | Val. Loss: 0.12244945764541626\n",
            "Epoch: 898 | Loss: 0.13477544486522675 | Val. Loss: 0.12126386165618896\n",
            "Epoch: 899 | Loss: 0.07514964044094086 | Val. Loss: 0.12032272666692734\n",
            "Epoch: 900 | Loss: 0.16693376004695892 | Val. Loss: 0.11991820484399796\n",
            "Epoch: 901 | Loss: 0.11136118322610855 | Val. Loss: 0.12024451792240143\n",
            "Epoch: 902 | Loss: 0.08753407746553421 | Val. Loss: 0.12128434330224991\n",
            "Epoch: 903 | Loss: 0.12555234134197235 | Val. Loss: 0.12155846506357193\n",
            "Epoch: 904 | Loss: 0.08071774989366531 | Val. Loss: 0.11948923021554947\n",
            "Epoch: 905 | Loss: 0.3308423161506653 | Val. Loss: 0.11892278492450714\n",
            "Epoch: 906 | Loss: 0.056187987327575684 | Val. Loss: 0.12011895328760147\n",
            "Epoch: 907 | Loss: 0.07886968553066254 | Val. Loss: 0.12026301771402359\n",
            "Epoch: 908 | Loss: 0.051878754049539566 | Val. Loss: 0.12057004868984222\n",
            "Epoch: 909 | Loss: 0.07414603978395462 | Val. Loss: 0.11968322843313217\n",
            "Epoch: 910 | Loss: 0.03556837886571884 | Val. Loss: 0.11846154928207397\n",
            "Epoch: 911 | Loss: 0.1903984397649765 | Val. Loss: 0.1188637986779213\n",
            "Epoch: 912 | Loss: 0.0822393000125885 | Val. Loss: 0.11907152831554413\n",
            "Epoch: 913 | Loss: 0.18546755611896515 | Val. Loss: 0.11970297247171402\n",
            "Epoch: 914 | Loss: 0.1233060359954834 | Val. Loss: 0.11765234172344208\n",
            "Epoch: 915 | Loss: 0.07420162856578827 | Val. Loss: 0.11805522441864014\n",
            "Epoch: 916 | Loss: 0.07835949957370758 | Val. Loss: 0.11696077883243561\n",
            "Epoch: 917 | Loss: 0.23115095496177673 | Val. Loss: 0.11725467443466187\n",
            "Epoch: 918 | Loss: 0.2690573036670685 | Val. Loss: 0.11797533184289932\n",
            "Epoch: 919 | Loss: 0.12454589456319809 | Val. Loss: 0.11917994171380997\n",
            "Epoch: 920 | Loss: 0.07764410227537155 | Val. Loss: 0.11787575483322144\n",
            "Epoch: 921 | Loss: 0.08471385389566422 | Val. Loss: 0.11663369089365005\n",
            "Epoch: 922 | Loss: 0.18545371294021606 | Val. Loss: 0.11691764742136002\n",
            "Epoch: 923 | Loss: 0.18576467037200928 | Val. Loss: 0.11802355200052261\n",
            "Epoch: 924 | Loss: 0.09260863065719604 | Val. Loss: 0.1166127547621727\n",
            "Epoch: 925 | Loss: 0.07518788427114487 | Val. Loss: 0.11650878190994263\n",
            "Epoch: 926 | Loss: 0.128285214304924 | Val. Loss: 0.11765594780445099\n",
            "Epoch: 927 | Loss: 0.19155001640319824 | Val. Loss: 0.11610265076160431\n",
            "Epoch: 928 | Loss: 0.11938419938087463 | Val. Loss: 0.11624468863010406\n",
            "Epoch: 929 | Loss: 0.22430863976478577 | Val. Loss: 0.11637234687805176\n",
            "Epoch: 930 | Loss: 0.04480177164077759 | Val. Loss: 0.11590484529733658\n",
            "Epoch: 931 | Loss: 0.13804909586906433 | Val. Loss: 0.11696527898311615\n",
            "Epoch: 932 | Loss: 0.0409151166677475 | Val. Loss: 0.11516473442316055\n",
            "Epoch: 933 | Loss: 0.12863120436668396 | Val. Loss: 0.11496792733669281\n",
            "Epoch: 934 | Loss: 0.08078157156705856 | Val. Loss: 0.11513300985097885\n",
            "Epoch: 935 | Loss: 0.13136009871959686 | Val. Loss: 0.11515931040048599\n",
            "Epoch: 936 | Loss: 0.07068893313407898 | Val. Loss: 0.11533007770776749\n",
            "Epoch: 937 | Loss: 0.09119770675897598 | Val. Loss: 0.11578182131052017\n",
            "Epoch: 938 | Loss: 0.08252783864736557 | Val. Loss: 0.11513564735651016\n",
            "Epoch: 939 | Loss: 0.02701534517109394 | Val. Loss: 0.11469481140375137\n",
            "Epoch: 940 | Loss: 0.11683548241853714 | Val. Loss: 0.11543896049261093\n",
            "Epoch: 941 | Loss: 0.09260354191064835 | Val. Loss: 0.11536145210266113\n",
            "Epoch: 942 | Loss: 0.046438269317150116 | Val. Loss: 0.11447115242481232\n",
            "Epoch: 943 | Loss: 0.31962892413139343 | Val. Loss: 0.11477222293615341\n",
            "Epoch: 944 | Loss: 0.08197299391031265 | Val. Loss: 0.11497696489095688\n",
            "Epoch: 945 | Loss: 0.13525789976119995 | Val. Loss: 0.11329078674316406\n",
            "Epoch: 946 | Loss: 0.034100912511348724 | Val. Loss: 0.11387870460748672\n",
            "Epoch: 947 | Loss: 0.077904112637043 | Val. Loss: 0.11348666995763779\n",
            "Epoch: 948 | Loss: 0.22638629376888275 | Val. Loss: 0.11446501314640045\n",
            "Epoch: 949 | Loss: 0.03840137645602226 | Val. Loss: 0.11395704746246338\n",
            "Epoch: 950 | Loss: 0.08238061517477036 | Val. Loss: 0.11285271495580673\n",
            "Epoch: 951 | Loss: 0.03296590968966484 | Val. Loss: 0.11308526992797852\n",
            "Epoch: 952 | Loss: 0.035306017845869064 | Val. Loss: 0.11320213228464127\n",
            "Epoch: 953 | Loss: 0.17835180461406708 | Val. Loss: 0.11336048692464828\n",
            "Epoch: 954 | Loss: 0.04364500194787979 | Val. Loss: 0.11434489488601685\n",
            "Epoch: 955 | Loss: 0.13060130178928375 | Val. Loss: 0.1133401170372963\n",
            "Epoch: 956 | Loss: 0.08319553732872009 | Val. Loss: 0.11350198835134506\n",
            "Epoch: 957 | Loss: 0.029730189591646194 | Val. Loss: 0.1133308857679367\n",
            "Epoch: 958 | Loss: 0.21802785992622375 | Val. Loss: 0.11311783641576767\n",
            "Epoch: 959 | Loss: 0.08410680294036865 | Val. Loss: 0.11164513975381851\n",
            "Epoch: 960 | Loss: 0.030641429126262665 | Val. Loss: 0.11194157600402832\n",
            "Epoch: 961 | Loss: 0.17206701636314392 | Val. Loss: 0.11188219487667084\n",
            "Epoch: 962 | Loss: 0.0747881680727005 | Val. Loss: 0.11247758567333221\n",
            "Epoch: 963 | Loss: 0.07842228561639786 | Val. Loss: 0.11201951652765274\n",
            "Epoch: 964 | Loss: 0.04344817250967026 | Val. Loss: 0.11288833618164062\n",
            "Epoch: 965 | Loss: 0.1258288025856018 | Val. Loss: 0.1117342934012413\n",
            "Epoch: 966 | Loss: 0.038058213889598846 | Val. Loss: 0.11257650703191757\n",
            "Epoch: 967 | Loss: 0.12887221574783325 | Val. Loss: 0.11125866323709488\n",
            "Epoch: 968 | Loss: 0.07684407383203506 | Val. Loss: 0.11177311837673187\n",
            "Epoch: 969 | Loss: 0.03396983817219734 | Val. Loss: 0.11199413239955902\n",
            "Epoch: 970 | Loss: 0.08252682536840439 | Val. Loss: 0.11175304651260376\n",
            "Epoch: 971 | Loss: 0.07937024533748627 | Val. Loss: 0.1112532839179039\n",
            "Epoch: 972 | Loss: 0.07135093212127686 | Val. Loss: 0.11074431985616684\n",
            "Epoch: 973 | Loss: 0.027002274990081787 | Val. Loss: 0.11102895438671112\n",
            "Epoch: 974 | Loss: 0.07872200757265091 | Val. Loss: 0.11050905287265778\n",
            "Epoch: 975 | Loss: 0.12344175577163696 | Val. Loss: 0.11197453737258911\n",
            "Epoch: 976 | Loss: 0.0738438218832016 | Val. Loss: 0.11151395738124847\n",
            "Epoch: 977 | Loss: 0.07719463109970093 | Val. Loss: 0.11130477488040924\n",
            "Epoch: 978 | Loss: 0.2680792808532715 | Val. Loss: 0.11225195974111557\n",
            "Epoch: 979 | Loss: 0.02721482142806053 | Val. Loss: 0.11107321083545685\n",
            "Epoch: 980 | Loss: 0.029269885271787643 | Val. Loss: 0.11147632449865341\n",
            "Epoch: 981 | Loss: 0.07802346348762512 | Val. Loss: 0.11100538820028305\n",
            "Epoch: 982 | Loss: 0.12820357084274292 | Val. Loss: 0.11144783347845078\n",
            "Epoch: 983 | Loss: 0.0841134563088417 | Val. Loss: 0.11053663492202759\n",
            "Epoch: 984 | Loss: 0.08529220521450043 | Val. Loss: 0.11025907844305038\n",
            "Epoch: 985 | Loss: 0.08078345656394958 | Val. Loss: 0.11017917096614838\n",
            "Epoch: 986 | Loss: 0.1638607382774353 | Val. Loss: 0.11039459705352783\n",
            "Epoch: 987 | Loss: 0.08715948462486267 | Val. Loss: 0.11029714345932007\n",
            "Epoch: 988 | Loss: 0.1755307912826538 | Val. Loss: 0.10959278047084808\n",
            "Epoch: 989 | Loss: 0.03209255635738373 | Val. Loss: 0.10937072336673737\n",
            "Epoch: 990 | Loss: 0.0896497368812561 | Val. Loss: 0.11017926782369614\n",
            "Epoch: 991 | Loss: 0.12691226601600647 | Val. Loss: 0.10911659151315689\n",
            "Epoch: 992 | Loss: 0.12141645699739456 | Val. Loss: 0.10914027690887451\n",
            "Epoch: 993 | Loss: 0.11651267111301422 | Val. Loss: 0.10932186990976334\n",
            "Epoch: 994 | Loss: 0.13195116817951202 | Val. Loss: 0.1091047152876854\n",
            "Epoch: 995 | Loss: 0.03600839897990227 | Val. Loss: 0.10974685102701187\n",
            "Epoch: 996 | Loss: 0.03344602882862091 | Val. Loss: 0.10903358459472656\n",
            "Epoch: 997 | Loss: 0.07028158754110336 | Val. Loss: 0.10884271562099457\n",
            "Epoch: 998 | Loss: 0.07461216300725937 | Val. Loss: 0.10853967815637589\n",
            "Epoch: 999 | Loss: 0.07560161501169205 | Val. Loss: 0.10869435966014862\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Loss')"
            ]
          },
          "metadata": {},
          "execution_count": 51
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testando o modelo\n",
        "\n",
        "# Converte os dados de teste em arrays numpy\n",
        "# x_teste_np = data_test.iloc[:, :-1].to_numpy()\n",
        "# d_teste_np = data_test.iloc[:, -1].to_numpy()\n",
        "\n",
        "# Aplica a normalização no conjunto de teste\n",
        "# x_teste_np_norm = (x_teste_np - media_treino) / desvio_padrao_treino\n",
        "\n",
        "# Projeta o teste\n",
        "x_teste_pca = (U @ x_teste_np_norm.T)\n",
        "x_teste_pca = x_teste_pca.T\n",
        "\n",
        "# Converte os arrays numpy em tensores PyTorch\n",
        "x_teste_pca = torch.tensor(x_teste_pca, dtype=torch.float32).to(device=device)\n",
        "d_teste_tensor = torch.tensor(d_teste_np, dtype=torch.long).to(device=device)\n",
        "\n",
        "# Testa o modelo com os dados de teste\n",
        "y_teste_tensor_pca = model_pca(x_teste_pca)\n",
        "y_teste_np = y_teste_tensor_pca.cpu().detach().numpy()\n",
        "\n",
        "predicoes_pca = np.argmax(y_teste_np, axis=1)\n",
        "acuracia_pca = np.mean(predicoes_pca == d_teste_np)\n",
        "\n",
        "# Taxa de erros\n",
        "\n",
        "Taxa_de_erro_pca = (1 - acuracia_pca) * 100\n",
        "\n",
        "print(f\"Acurácia: {acuracia_pca*100:.2f}%\")\n",
        "print(f\"Taxa de erro: {Taxa_de_erro_pca:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoV-OR1K4b7R",
        "outputId": "c09c48a7-9863-4aaa-c5a4-a9d7bd9885f7"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia: 95.11%\n",
            "Taxa de erro: 4.89%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQbeQopQ3K_j"
      },
      "source": [
        "# Exercício 4\n",
        "\n",
        "Repita os exercícios 1, 2 e 3, considerando a transformação dos dados usando o LDA no lugar do PCA. Use como referência o exemplo mostrado [neste Jupyter Notebook](./LDA_IRIS.ipynb)\n",
        "\n",
        "## Resolução"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbWImeSV3K_j"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}